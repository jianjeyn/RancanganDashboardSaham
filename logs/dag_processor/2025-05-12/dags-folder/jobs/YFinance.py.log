{"timestamp":"2025-05-12T00:45:12.620061","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T00:45:42.898767","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T00:45:51.872483Z","level":"critical","event":"Process terminated by signal. For more information, see https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#TaskRunner-killed","signal":-9,"signal_name":"SIGKILL","logger":"processor"}
{"timestamp":"2025-05-12T00:48:38.578575","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T00:48:59.361153Z","level":"info","event":":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:48:59.687371Z","level":"error","event":"Ivy Default Cache set to: /home/airflow/.ivy2/cache","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:48:59.688389Z","level":"error","event":"The jars for the packages stored in: /home/airflow/.ivy2/jars","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:48:59.706009Z","level":"error","event":"org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:48:59.723417Z","level":"error","event":":: resolving dependencies :: org.apache.spark#spark-submit-parent-4e15a70c-b158-4b7f-8c5f-86ca7ca99444;1.0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:48:59.724483Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:02.186036Z","level":"error","event":"\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:02.659175Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:03.146640Z","level":"error","event":"\tfound org.mongodb#bson;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:03.733578Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-core;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:03.983345Z","level":"error","event":"downloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/3.0.1/mongo-spark-connector_2.12-3.0.1.jar ...","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:04.936587Z","level":"error","event":"\t[SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1!mongo-spark-connector_2.12.jar (1138ms)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:05.163455Z","level":"error","event":"downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/4.0.5/mongodb-driver-sync-4.0.5.jar ...","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:05.424234Z","level":"error","event":"\t[SUCCESSFUL ] org.mongodb#mongodb-driver-sync;4.0.5!mongodb-driver-sync.jar (458ms)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:05.647206Z","level":"error","event":"downloading https://repo1.maven.org/maven2/org/mongodb/bson/4.0.5/bson-4.0.5.jar ...","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:06.071282Z","level":"error","event":"\t[SUCCESSFUL ] org.mongodb#bson;4.0.5!bson.jar (633ms)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:06.301789Z","level":"error","event":"downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/4.0.5/mongodb-driver-core-4.0.5.jar ...","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:07.080164Z","level":"error","event":"\t[SUCCESSFUL ] org.mongodb#mongodb-driver-core;4.0.5!mongodb-driver-core.jar (967ms)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:07.083915Z","level":"error","event":":: resolution report :: resolve 4081ms :: artifacts dl 3288ms","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:07.092637Z","level":"error","event":"\t:: modules in use:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:07.095925Z","level":"error","event":"\torg.mongodb#bson;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:07.096842Z","level":"error","event":"\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:07.104601Z","level":"error","event":"\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:07.105363Z","level":"error","event":"\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:07.105975Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:07.106582Z","level":"error","event":"\t|                  |            modules            ||   artifacts   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:07.107149Z","level":"error","event":"\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:07.107643Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:07.153129Z","level":"error","event":"\t|      default     |   4   |   4   |   4   |   0   ||   4   |   4   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:07.153884Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:07.218573Z","level":"error","event":":: retrieving :: org.apache.spark#spark-submit-parent-4e15a70c-b158-4b7f-8c5f-86ca7ca99444","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:07.220331Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:07.284516Z","level":"error","event":"\t4 artifacts copied, 0 already retrieved (2728kB/130ms)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:09.706778Z","level":"error","event":"25/05/12 00:49:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:10.640458","level":"error","event":"Process timed out, PID: 39","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-05-12T00:49:10.642313","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/jobs/YFinance.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 39","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":22,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":497,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":515,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":201,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":436,"name":"_ensure_initialized"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/java_gateway.py","lineno":104,"name":"launch_gateway"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-05-12T00:49:10.870435Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:10.872459Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:10.972025Z","level":"error","event":"Exception in thread \"main\" java.nio.file.NoSuchFileException: /tmp/tmpryh9_ulg/connection9653152345745644372.info","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:10.972807Z","level":"error","event":"\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:10.973497Z","level":"error","event":"\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:10.979249Z","level":"error","event":"\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:10.981135Z","level":"error","event":"\tat java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:218)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:10.981904Z","level":"error","event":"\tat java.base/java.nio.file.Files.newByteChannel(Files.java:380)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:10.982591Z","level":"error","event":"\tat java.base/java.nio.file.Files.createFile(Files.java:658)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:10.983332Z","level":"error","event":"\tat java.base/java.nio.file.TempFileHelper.create(TempFileHelper.java:136)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:10.985220Z","level":"error","event":"\tat java.base/java.nio.file.TempFileHelper.createTempFile(TempFileHelper.java:159)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:10.986077Z","level":"error","event":"\tat java.base/java.nio.file.Files.createTempFile(Files.java:878)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:10.987275Z","level":"error","event":"\tat org.apache.spark.api.python.PythonGatewayServer$.main(PythonGatewayServer.scala:54)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:10.989100Z","level":"error","event":"\tat org.apache.spark.api.python.PythonGatewayServer.main(PythonGatewayServer.scala)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:10.992215Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:10.997492Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:10.998210Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:10.998918Z","level":"error","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:10.999733Z","level":"error","event":"\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:11.000485Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1034)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:11.001654Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:11.002607Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:11.003407Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:11.013112Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:11.015983Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:11.016968Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:49:59.160180","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T00:50:04.369830Z","level":"info","event":":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:50:04.488285Z","level":"error","event":"Ivy Default Cache set to: /home/airflow/.ivy2/cache","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:50:04.489071Z","level":"error","event":"The jars for the packages stored in: /home/airflow/.ivy2/jars","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:50:04.504111Z","level":"error","event":"org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:50:04.505096Z","level":"error","event":":: resolving dependencies :: org.apache.spark#spark-submit-parent-4e4a6386-fbae-4517-ac38-ac4a4400e410;1.0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:50:04.519705Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:50:04.703543Z","level":"error","event":"\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:50:04.749756Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:50:04.795825Z","level":"error","event":"\tfound org.mongodb#bson;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:50:04.843631Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-core;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:50:04.896659Z","level":"error","event":":: resolution report :: resolve 373ms :: artifacts dl 20ms","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:50:04.911994Z","level":"error","event":"\t:: modules in use:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:50:04.912694Z","level":"error","event":"\torg.mongodb#bson;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:50:04.913396Z","level":"error","event":"\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:50:04.914089Z","level":"error","event":"\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:50:04.914793Z","level":"error","event":"\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:50:04.915705Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:50:04.916550Z","level":"error","event":"\t|                  |            modules            ||   artifacts   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:50:04.917191Z","level":"error","event":"\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:50:04.917844Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:50:04.918491Z","level":"error","event":"\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:50:04.919250Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:50:04.922172Z","level":"error","event":":: retrieving :: org.apache.spark#spark-submit-parent-4e4a6386-fbae-4517-ac38-ac4a4400e410","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:50:04.922996Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:50:04.939200Z","level":"error","event":"\t0 artifacts copied, 4 already retrieved (0kB/13ms)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:50:05.456997Z","level":"error","event":"25/05/12 00:50:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:50:05.891095Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:50:05.907823Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:50:09.226026Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:50:10.377069Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:50:11.377188Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:50:11.603695Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:50:12.603964Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:50:12.728142Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:50:13.728406Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:50:13.902737Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:50:14.903039Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:50:15.075667Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:50:16.075845Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:50:24.635806","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"UnsupportedOperationException","exc_value":"MongoCollection already exists","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":101,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":1461,"name":"save"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":185,"name":"deco"}]}]}
{"timestamp":"2025-05-12T00:50:55.929054","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T00:51:01.249105Z","level":"info","event":":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:51:01.337183Z","level":"error","event":"Ivy Default Cache set to: /home/airflow/.ivy2/cache","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:01.337716Z","level":"error","event":"The jars for the packages stored in: /home/airflow/.ivy2/jars","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:01.347753Z","level":"error","event":"org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:01.357964Z","level":"error","event":":: resolving dependencies :: org.apache.spark#spark-submit-parent-d2604064-e039-4cca-9282-c7c104a0815d;1.0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:01.358548Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:01.519013Z","level":"error","event":"\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:01.600926Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:01.719703Z","level":"error","event":"\tfound org.mongodb#bson;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:01.779343Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-core;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:01.857342Z","level":"error","event":":: resolution report :: resolve 474ms :: artifacts dl 32ms","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:01.858369Z","level":"error","event":"\t:: modules in use:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:01.859241Z","level":"error","event":"\torg.mongodb#bson;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:01.860319Z","level":"error","event":"\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:01.861546Z","level":"error","event":"\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:01.865240Z","level":"error","event":"\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:01.866664Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:01.867918Z","level":"error","event":"\t|                  |            modules            ||   artifacts   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:01.868958Z","level":"error","event":"\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:01.870112Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:01.871130Z","level":"error","event":"\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:01.871963Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:01.894882Z","level":"error","event":":: retrieving :: org.apache.spark#spark-submit-parent-d2604064-e039-4cca-9282-c7c104a0815d","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:01.896382Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:01.897941Z","level":"error","event":"\t0 artifacts copied, 4 already retrieved (0kB/17ms)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:02.397425Z","level":"error","event":"25/05/12 00:51:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:02.829965Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:02.843752Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:05.860645Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:51:06.601378Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:51:07.601589Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:51:07.711893Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:51:08.712216Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:51:08.802772Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:51:09.802928Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:51:09.917904Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:51:10.918120Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:51:11.025777Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:51:12.025960Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:51:18.716216","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"UnsupportedOperationException","exc_value":"MongoCollection already exists","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":101,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":1461,"name":"save"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":185,"name":"deco"}]}]}
{"timestamp":"2025-05-12T00:51:49.845261","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T00:51:54.692884Z","level":"info","event":":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:51:54.805026Z","level":"error","event":"Ivy Default Cache set to: /home/airflow/.ivy2/cache","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:54.805937Z","level":"error","event":"The jars for the packages stored in: /home/airflow/.ivy2/jars","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:54.820825Z","level":"error","event":"org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:54.821372Z","level":"error","event":":: resolving dependencies :: org.apache.spark#spark-submit-parent-24ce2b52-781c-4c17-b6c0-407ccc47cee9;1.0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:54.821789Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:54.967602Z","level":"error","event":"\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:55.011481Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:55.044792Z","level":"error","event":"\tfound org.mongodb#bson;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:55.083028Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-core;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:55.117535Z","level":"error","event":":: resolution report :: resolve 286ms :: artifacts dl 13ms","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:55.130130Z","level":"error","event":"\t:: modules in use:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:55.130616Z","level":"error","event":"\torg.mongodb#bson;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:55.131032Z","level":"error","event":"\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:55.131473Z","level":"error","event":"\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:55.131893Z","level":"error","event":"\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:55.132349Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:55.132783Z","level":"error","event":"\t|                  |            modules            ||   artifacts   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:55.133269Z","level":"error","event":"\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:55.133749Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:55.134142Z","level":"error","event":"\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:55.134579Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:55.135059Z","level":"error","event":":: retrieving :: org.apache.spark#spark-submit-parent-24ce2b52-781c-4c17-b6c0-407ccc47cee9","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:55.135489Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:55.144916Z","level":"error","event":"\t0 artifacts copied, 4 already retrieved (0kB/10ms)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:55.346261Z","level":"error","event":"25/05/12 00:51:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:55.606701Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:55.607255Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:51:59.543562Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:52:00.253458Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:52:01.253580Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:52:01.365083Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:52:02.365384Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:52:02.445010Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:52:03.445049Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:52:03.550462Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:52:04.550797Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:52:04.640968Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:52:05.641158Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:52:10.812952","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"UnsupportedOperationException","exc_value":"MongoCollection already exists","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":101,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":1461,"name":"save"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":185,"name":"deco"}]}]}
{"timestamp":"2025-05-12T00:52:42.063438","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T00:52:46.295085Z","level":"info","event":":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:52:46.388306Z","level":"error","event":"Ivy Default Cache set to: /home/airflow/.ivy2/cache","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:52:46.388856Z","level":"error","event":"The jars for the packages stored in: /home/airflow/.ivy2/jars","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:52:46.400112Z","level":"error","event":"org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:52:46.400767Z","level":"error","event":":: resolving dependencies :: org.apache.spark#spark-submit-parent-f28e31fd-b6e5-4ebb-8bd7-630f6fd117b1;1.0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:52:46.401236Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:52:46.558508Z","level":"error","event":"\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:52:46.607319Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:52:46.677009Z","level":"error","event":"\tfound org.mongodb#bson;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:52:46.722586Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-core;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:52:46.780177Z","level":"error","event":":: resolution report :: resolve 357ms :: artifacts dl 24ms","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:52:46.793920Z","level":"error","event":"\t:: modules in use:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:52:46.794849Z","level":"error","event":"\torg.mongodb#bson;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:52:46.795566Z","level":"error","event":"\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:52:46.796911Z","level":"error","event":"\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:52:46.798634Z","level":"error","event":"\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:52:46.799489Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:52:46.800169Z","level":"error","event":"\t|                  |            modules            ||   artifacts   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:52:46.800942Z","level":"error","event":"\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:52:46.802031Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:52:46.802670Z","level":"error","event":"\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:52:46.803711Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:52:46.804345Z","level":"error","event":":: retrieving :: org.apache.spark#spark-submit-parent-f28e31fd-b6e5-4ebb-8bd7-630f6fd117b1","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:52:46.804914Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:52:46.825462Z","level":"error","event":"\t0 artifacts copied, 4 already retrieved (0kB/16ms)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:52:47.086458Z","level":"error","event":"25/05/12 00:52:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:52:47.399726Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:52:47.415710Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:52:50.326166Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:52:50.927129Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:52:51.927307Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:52:52.025176Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:52:53.025733Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:52:53.089820Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:52:54.089969Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:52:54.174802Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:52:55.175264Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:52:55.251239Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:52:56.251378Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:53:03.675286","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"UnsupportedOperationException","exc_value":"MongoCollection already exists","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":101,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":1461,"name":"save"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":185,"name":"deco"}]}]}
{"timestamp":"2025-05-12T00:53:35.634443","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T00:53:38.763054Z","level":"info","event":":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:53:38.857228Z","level":"error","event":"Ivy Default Cache set to: /home/airflow/.ivy2/cache","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:53:38.973670Z","level":"error","event":"The jars for the packages stored in: /home/airflow/.ivy2/jars","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:53:38.974311Z","level":"error","event":"org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:53:38.974742Z","level":"error","event":":: resolving dependencies :: org.apache.spark#spark-submit-parent-1ccc2824-99db-4ead-bb2f-19f45b8f538c;1.0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:53:38.975101Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:53:38.996994Z","level":"error","event":"\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:53:39.031494Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:53:39.060774Z","level":"error","event":"\tfound org.mongodb#bson;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:53:39.091139Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-core;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:53:39.131710Z","level":"error","event":":: resolution report :: resolve 246ms :: artifacts dl 16ms","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:53:39.145516Z","level":"error","event":"\t:: modules in use:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:53:39.146302Z","level":"error","event":"\torg.mongodb#bson;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:53:39.147111Z","level":"error","event":"\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:53:39.147782Z","level":"error","event":"\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:53:39.148647Z","level":"error","event":"\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:53:39.149287Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:53:39.150009Z","level":"error","event":"\t|                  |            modules            ||   artifacts   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:53:39.150636Z","level":"error","event":"\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:53:39.151238Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:53:39.151801Z","level":"error","event":"\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:53:39.152581Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:53:39.153227Z","level":"error","event":":: retrieving :: org.apache.spark#spark-submit-parent-1ccc2824-99db-4ead-bb2f-19f45b8f538c","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:53:39.153950Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:53:39.166479Z","level":"error","event":"\t0 artifacts copied, 4 already retrieved (0kB/11ms)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:53:39.379578Z","level":"error","event":"25/05/12 00:53:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:53:39.626624Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:53:39.644063Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:53:41.355406Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:53:42.094381Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:53:43.095030Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:53:43.235470Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:53:44.235704Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:53:44.310539Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:53:45.310691Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:53:45.405615Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:53:46.405869Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:53:46.515418Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:53:47.515581Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:53:54.594971","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"UnsupportedOperationException","exc_value":"MongoCollection already exists","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":101,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":1461,"name":"save"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":185,"name":"deco"}]}]}
{"timestamp":"2025-05-12T00:54:26.291370","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T00:54:29.754745Z","level":"info","event":":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:54:29.833903Z","level":"error","event":"Ivy Default Cache set to: /home/airflow/.ivy2/cache","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:54:29.834634Z","level":"error","event":"The jars for the packages stored in: /home/airflow/.ivy2/jars","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:54:29.846837Z","level":"error","event":"org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:54:29.847522Z","level":"error","event":":: resolving dependencies :: org.apache.spark#spark-submit-parent-68e05adb-2ccb-4d5f-a13f-a8fa891e814c;1.0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:54:29.848071Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:54:29.968478Z","level":"error","event":"\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:54:30.010815Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:54:30.051844Z","level":"error","event":"\tfound org.mongodb#bson;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:54:30.090187Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-core;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:54:30.126576Z","level":"error","event":":: resolution report :: resolve 272ms :: artifacts dl 11ms","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:54:30.138217Z","level":"error","event":"\t:: modules in use:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:54:30.138725Z","level":"error","event":"\torg.mongodb#bson;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:54:30.139244Z","level":"error","event":"\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:54:30.139711Z","level":"error","event":"\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:54:30.140176Z","level":"error","event":"\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:54:30.140630Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:54:30.141249Z","level":"error","event":"\t|                  |            modules            ||   artifacts   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:54:30.141800Z","level":"error","event":"\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:54:30.142328Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:54:30.142842Z","level":"error","event":"\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:54:30.143341Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:54:30.143789Z","level":"error","event":":: retrieving :: org.apache.spark#spark-submit-parent-68e05adb-2ccb-4d5f-a13f-a8fa891e814c","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:54:30.144259Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:54:30.155348Z","level":"error","event":"\t0 artifacts copied, 4 already retrieved (0kB/10ms)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:54:30.413745Z","level":"error","event":"25/05/12 00:54:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:54:30.671214Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:54:30.682247Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:54:32.677012Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:54:33.301488Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:54:34.302333Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:54:34.418468Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:54:35.418766Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:54:35.518915Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:54:38.532615Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:54:38.642472Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:54:39.628193Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:54:39.733174Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:54:40.733383Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:54:45.594862","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"UnsupportedOperationException","exc_value":"MongoCollection already exists","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":101,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":1461,"name":"save"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":185,"name":"deco"}]}]}
{"timestamp":"2025-05-12T00:57:47.612320","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T00:58:04.476218Z","level":"info","event":":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:58:04.785085Z","level":"error","event":"Ivy Default Cache set to: /home/airflow/.ivy2/cache","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:04.785742Z","level":"error","event":"The jars for the packages stored in: /home/airflow/.ivy2/jars","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:04.814768Z","level":"error","event":"org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:04.815450Z","level":"error","event":":: resolving dependencies :: org.apache.spark#spark-submit-parent-18756a88-ab13-452f-87ad-a40c641e51b9;1.0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:04.816365Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:07.032594Z","level":"error","event":"\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:07.521205Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:07.953993Z","level":"error","event":"\tfound org.mongodb#bson;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:08.397188Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-core;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:08.622824Z","level":"error","event":"downloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/3.0.1/mongo-spark-connector_2.12-3.0.1.jar ...","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:09.535220Z","level":"error","event":"\t[SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1!mongo-spark-connector_2.12.jar (1098ms)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:09.732595Z","level":"error","event":"downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/4.0.5/mongodb-driver-sync-4.0.5.jar ...","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:09.966649Z","level":"error","event":"\t[SUCCESSFUL ] org.mongodb#mongodb-driver-sync;4.0.5!mongodb-driver-sync.jar (417ms)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:10.180989Z","level":"error","event":"downloading https://repo1.maven.org/maven2/org/mongodb/bson/4.0.5/bson-4.0.5.jar ...","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:10.508514Z","level":"error","event":"\t[SUCCESSFUL ] org.mongodb#bson;4.0.5!bson.jar (504ms)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:10.708103Z","level":"error","event":"downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/4.0.5/mongodb-driver-core-4.0.5.jar ...","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:11.511450Z","level":"error","event":"\t[SUCCESSFUL ] org.mongodb#mongodb-driver-core;4.0.5!mongodb-driver-core.jar (984ms)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:11.615251Z","level":"error","event":":: resolution report :: resolve 3617ms :: artifacts dl 3083ms","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:11.615936Z","level":"error","event":"\t:: modules in use:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:11.616516Z","level":"error","event":"\torg.mongodb#bson;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:11.617150Z","level":"error","event":"\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:11.617876Z","level":"error","event":"\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:11.618540Z","level":"error","event":"\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:11.619198Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:11.619818Z","level":"error","event":"\t|                  |            modules            ||   artifacts   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:11.620553Z","level":"error","event":"\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:11.621477Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:11.622146Z","level":"error","event":"\t|      default     |   4   |   4   |   4   |   0   ||   4   |   4   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:11.622906Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:11.623792Z","level":"error","event":":: retrieving :: org.apache.spark#spark-submit-parent-18756a88-ab13-452f-87ad-a40c641e51b9","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:11.624682Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:11.649969Z","level":"error","event":"\t4 artifacts copied, 0 already retrieved (2728kB/77ms)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:12.911107Z","level":"error","event":"25/05/12 00:58:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:13.987496Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:13.988248Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:19.055683Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:58:19.646153","level":"error","event":"Process timed out, PID: 37","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-05-12T00:58:19.664778Z","level":"error","event":"Exception ignored from cffi callback <function buffer_callback at 0x7f9097934900>:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:19.665326Z","level":"error","event":"Traceback (most recent call last):","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:19.665819Z","level":"error","event":"  File \"/home/airflow/.local/lib/python3.12/site-packages/curl_cffi/curl.py\", line 63, in buffer_callback","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:19.666277Z","level":"error","event":"    @ffi.def_extern()","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:19.666773Z","level":"error","event":"","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:19.667353Z","level":"error","event":"  File \"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py\", line 69, in handle_timeout","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:19.668359Z","level":"error","event":"    raise AirflowTaskTimeout(self.error_message)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:19.669488Z","level":"error","event":"airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/jobs/YFinance.py after 30.0s.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:19.670101Z","level":"error","event":"Please take a look at these docs to improve your DAG import time:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:19.670739Z","level":"error","event":"* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:19.671405Z","level":"error","event":"* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 37","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:58:20.920944Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:58:23.135787Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:58:23.343550Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:58:24.343711Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:58:24.478093Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:58:25.478241Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:58:25.649553Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:58:26.649782Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:58:26.810299Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:58:27.810464Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:58:37.152542","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"UnsupportedOperationException","exc_value":"MongoCollection already exists","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":101,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":1461,"name":"save"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":185,"name":"deco"}]}]}
{"timestamp":"2025-05-12T00:58:58.065255","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T00:59:05.410616Z","level":"info","event":":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:59:05.568085Z","level":"error","event":"Ivy Default Cache set to: /home/airflow/.ivy2/cache","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:59:05.569135Z","level":"error","event":"The jars for the packages stored in: /home/airflow/.ivy2/jars","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:59:05.591666Z","level":"error","event":"org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:59:05.592294Z","level":"error","event":":: resolving dependencies :: org.apache.spark#spark-submit-parent-f2d5e5eb-bdea-4452-a637-4cec5208ccd0;1.0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:59:05.592829Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:59:05.812128Z","level":"error","event":"\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:59:05.877159Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:59:05.941680Z","level":"error","event":"\tfound org.mongodb#bson;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:59:05.995632Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-core;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:59:06.060907Z","level":"error","event":":: resolution report :: resolve 448ms :: artifacts dl 25ms","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:59:06.076003Z","level":"error","event":"\t:: modules in use:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:59:06.076705Z","level":"error","event":"\torg.mongodb#bson;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:59:06.077366Z","level":"error","event":"\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:59:06.077938Z","level":"error","event":"\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:59:06.078519Z","level":"error","event":"\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:59:06.079145Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:59:06.079820Z","level":"error","event":"\t|                  |            modules            ||   artifacts   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:59:06.080468Z","level":"error","event":"\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:59:06.081000Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:59:06.081491Z","level":"error","event":"\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:59:06.082089Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:59:06.082736Z","level":"error","event":":: retrieving :: org.apache.spark#spark-submit-parent-f2d5e5eb-bdea-4452-a637-4cec5208ccd0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:59:06.083265Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:59:06.098385Z","level":"error","event":"\t0 artifacts copied, 4 already retrieved (0kB/14ms)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:59:06.490783Z","level":"error","event":"25/05/12 00:59:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:59:06.935384Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:59:06.936078Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T00:59:10.823326Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:59:11.787576Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:59:12.788248Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:59:12.917021Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:59:13.917154Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:59:13.999469Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:59:14.999651Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:59:15.105807Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:59:16.106073Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:59:16.220296Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:59:17.220463Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T00:59:30.110317","level":"error","event":"Process timed out, PID: 31","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-05-12T00:59:30.111024","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/jobs/YFinance.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 31","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/dataframe.py","lineno":5176,"name":"withColumn"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1321,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":511,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-05-12T00:59:30.116876","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-05-12T00:59:31.277991Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 0) / 1]\r\r[Stage 0:>                                                          (0 + 1) / 1]\r\r[Stage 0:===========================================================(1 + 0) / 1]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:02.295438","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:00:07.360682Z","level":"info","event":":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:00:07.488343Z","level":"error","event":"Ivy Default Cache set to: /home/airflow/.ivy2/cache","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:07.489201Z","level":"error","event":"The jars for the packages stored in: /home/airflow/.ivy2/jars","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:07.503002Z","level":"error","event":"org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:07.503568Z","level":"error","event":":: resolving dependencies :: org.apache.spark#spark-submit-parent-26d24092-20ca-45c8-95a6-7a8d50440f48;1.0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:07.504069Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:07.730073Z","level":"error","event":"\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:07.792156Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:07.876113Z","level":"error","event":"\tfound org.mongodb#bson;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:07.976230Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-core;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:08.029813Z","level":"error","event":":: resolution report :: resolve 506ms :: artifacts dl 22ms","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:08.052340Z","level":"error","event":"\t:: modules in use:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:08.053201Z","level":"error","event":"\torg.mongodb#bson;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:08.053877Z","level":"error","event":"\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:08.054517Z","level":"error","event":"\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:08.055167Z","level":"error","event":"\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:08.055778Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:08.056414Z","level":"error","event":"\t|                  |            modules            ||   artifacts   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:08.056986Z","level":"error","event":"\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:08.057541Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:08.060009Z","level":"error","event":"\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:08.063438Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:08.065280Z","level":"error","event":":: retrieving :: org.apache.spark#spark-submit-parent-26d24092-20ca-45c8-95a6-7a8d50440f48","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:08.066019Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:08.080571Z","level":"error","event":"\t0 artifacts copied, 4 already retrieved (0kB/18ms)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:08.509250Z","level":"error","event":"25/05/12 01:00:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:08.923361Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:08.923969Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:00:12.170359Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:00:12.826850Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:00:13.827464Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:00:13.941539Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:00:14.941698Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:00:15.067462Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:00:16.067694Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:00:16.165012Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:00:17.165315Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:00:17.319253Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:00:18.319421Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:00:31.276358","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"UnsupportedOperationException","exc_value":"MongoCollection already exists","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":101,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":1461,"name":"save"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":185,"name":"deco"}]}]}
{"timestamp":"2025-05-12T01:00:32.410785Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 1) / 1]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:02.792426","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:01:08.397331Z","level":"info","event":":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:01:08.709851Z","level":"error","event":"Ivy Default Cache set to: /home/airflow/.ivy2/cache","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:08.710694Z","level":"error","event":"The jars for the packages stored in: /home/airflow/.ivy2/jars","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:08.725513Z","level":"error","event":"org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:08.726155Z","level":"error","event":":: resolving dependencies :: org.apache.spark#spark-submit-parent-5292da92-6852-44f4-91dc-af70f287b320;1.0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:08.726720Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:08.998178Z","level":"error","event":"\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:09.057538Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:09.144728Z","level":"error","event":"\tfound org.mongodb#bson;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:09.236058Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-core;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:09.312585Z","level":"error","event":":: resolution report :: resolve 544ms :: artifacts dl 45ms","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:09.327762Z","level":"error","event":"\t:: modules in use:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:09.328781Z","level":"error","event":"\torg.mongodb#bson;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:09.329510Z","level":"error","event":"\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:09.330327Z","level":"error","event":"\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:09.331240Z","level":"error","event":"\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:09.331936Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:09.332554Z","level":"error","event":"\t|                  |            modules            ||   artifacts   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:09.333142Z","level":"error","event":"\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:09.333879Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:09.334568Z","level":"error","event":"\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:09.335302Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:09.335975Z","level":"error","event":":: retrieving :: org.apache.spark#spark-submit-parent-5292da92-6852-44f4-91dc-af70f287b320","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:09.336996Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:09.370818Z","level":"error","event":"\t0 artifacts copied, 4 already retrieved (0kB/19ms)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:09.863755Z","level":"error","event":"25/05/12 01:01:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:10.377025Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:10.399344Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:01:15.064890Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:01:15.957055Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:01:16.957874Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:01:17.083457Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:01:18.083726Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:01:18.153870Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:01:19.153949Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:01:19.268802Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:01:20.268959Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:01:20.348840Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:01:21.349075Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:01:29.748950","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"UnsupportedOperationException","exc_value":"MongoCollection already exists","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":101,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":1461,"name":"save"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":185,"name":"deco"}]}]}
{"timestamp":"2025-05-12T01:01:30.713872Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 1) / 1]\r\r[Stage 0:===========================================================(1 + 0) / 1]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:00.838403","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:02:08.454857Z","level":"info","event":":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:02:08.579628Z","level":"error","event":"Ivy Default Cache set to: /home/airflow/.ivy2/cache","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:08.593956Z","level":"error","event":"The jars for the packages stored in: /home/airflow/.ivy2/jars","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:08.594487Z","level":"error","event":"org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:08.595013Z","level":"error","event":":: resolving dependencies :: org.apache.spark#spark-submit-parent-8cdde5c4-ed16-41b7-8dab-a1473cba6dd4;1.0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:08.595504Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:09.017681Z","level":"error","event":"\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:09.070431Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:09.114560Z","level":"error","event":"\tfound org.mongodb#bson;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:09.169649Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-core;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:09.226764Z","level":"error","event":":: resolution report :: resolve 613ms :: artifacts dl 20ms","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:09.227586Z","level":"error","event":"\t:: modules in use:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:09.228351Z","level":"error","event":"\torg.mongodb#bson;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:09.243422Z","level":"error","event":"\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:09.244190Z","level":"error","event":"\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:09.245036Z","level":"error","event":"\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:09.245713Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:09.246237Z","level":"error","event":"\t|                  |            modules            ||   artifacts   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:09.246862Z","level":"error","event":"\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:09.247400Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:09.248083Z","level":"error","event":"\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:09.248635Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:09.249347Z","level":"error","event":":: retrieving :: org.apache.spark#spark-submit-parent-8cdde5c4-ed16-41b7-8dab-a1473cba6dd4","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:09.250098Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:09.265373Z","level":"error","event":"\t0 artifacts copied, 4 already retrieved (0kB/13ms)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:09.643729Z","level":"error","event":"25/05/12 01:02:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:10.023796Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:10.024530Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:02:14.348843Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:02:15.038452Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:02:16.038691Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:02:16.141667Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:02:17.141849Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:02:17.224144Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:02:18.224282Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:02:18.318541Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:02:19.318798Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:02:19.397344Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:02:20.397708Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:02:29.799053","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"UnsupportedOperationException","exc_value":"MongoCollection already exists","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":101,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":1461,"name":"save"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":185,"name":"deco"}]}]}
{"timestamp":"2025-05-12T01:02:31.010479Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 1) / 1]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:01.726902","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:03:07.364120Z","level":"info","event":":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:03:07.579748Z","level":"error","event":"Ivy Default Cache set to: /home/airflow/.ivy2/cache","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:07.581573Z","level":"error","event":"The jars for the packages stored in: /home/airflow/.ivy2/jars","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:07.618933Z","level":"error","event":"org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:07.655776Z","level":"error","event":":: resolving dependencies :: org.apache.spark#spark-submit-parent-e3a45069-80d4-4584-967e-42ae6319de55;1.0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:07.656616Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:08.273360Z","level":"error","event":"\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:09.229774Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:09.413268Z","level":"error","event":"\tfound org.mongodb#bson;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:10.964186Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-core;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:11.289721Z","level":"error","event":":: resolution report :: resolve 3517ms :: artifacts dl 138ms","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:11.292493Z","level":"error","event":"\t:: modules in use:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:11.293361Z","level":"error","event":"\torg.mongodb#bson;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:11.296560Z","level":"error","event":"\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:11.299202Z","level":"error","event":"\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:11.300072Z","level":"error","event":"\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:11.300831Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:11.301573Z","level":"error","event":"\t|                  |            modules            ||   artifacts   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:11.302369Z","level":"error","event":"\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:11.303177Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:11.305503Z","level":"error","event":"\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:11.306515Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:11.350463Z","level":"error","event":":: retrieving :: org.apache.spark#spark-submit-parent-e3a45069-80d4-4584-967e-42ae6319de55","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:11.352154Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:11.416335Z","level":"error","event":"\t0 artifacts copied, 4 already retrieved (0kB/59ms)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:12.354696Z","level":"error","event":"25/05/12 01:03:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:13.209512Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:13.211186Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:03:18.242634Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:03:19.499715Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:03:20.499965Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:03:20.623506Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:03:21.623773Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:03:21.691878Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:03:22.692140Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:03:22.808705Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:03:23.808918Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:03:23.897015Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:03:24.897615Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:03:33.732364","level":"error","event":"Process timed out, PID: 907","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-05-12T01:03:33.733118","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/jobs/YFinance.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 907","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1321,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":511,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-05-12T01:03:33.737686","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-05-12T01:03:35.022958Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 1) / 1]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:06.041942","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:04:11.504543Z","level":"info","event":":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:04:11.585455Z","level":"error","event":"Ivy Default Cache set to: /home/airflow/.ivy2/cache","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:11.586131Z","level":"error","event":"The jars for the packages stored in: /home/airflow/.ivy2/jars","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:11.597522Z","level":"error","event":"org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:11.598100Z","level":"error","event":":: resolving dependencies :: org.apache.spark#spark-submit-parent-c4548cb5-dc65-4a5c-a997-b14079edc0ed;1.0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:11.598920Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:11.716110Z","level":"error","event":"\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:11.747866Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:11.780361Z","level":"error","event":"\tfound org.mongodb#bson;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:11.811655Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-core;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:11.852370Z","level":"error","event":":: resolution report :: resolve 245ms :: artifacts dl 13ms","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:11.863807Z","level":"error","event":"\t:: modules in use:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:11.864262Z","level":"error","event":"\torg.mongodb#bson;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:11.864667Z","level":"error","event":"\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:11.865114Z","level":"error","event":"\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:11.865489Z","level":"error","event":"\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:11.866170Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:11.866691Z","level":"error","event":"\t|                  |            modules            ||   artifacts   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:11.867171Z","level":"error","event":"\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:11.867616Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:11.868206Z","level":"error","event":"\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:11.868774Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:11.869570Z","level":"error","event":":: retrieving :: org.apache.spark#spark-submit-parent-c4548cb5-dc65-4a5c-a997-b14079edc0ed","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:11.870243Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:11.882241Z","level":"error","event":"\t0 artifacts copied, 4 already retrieved (0kB/10ms)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:12.103599Z","level":"error","event":"25/05/12 01:04:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:12.357679Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:12.369948Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:04:16.606828Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:04:17.226796Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:04:18.226951Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:04:18.351894Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:04:19.352030Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:04:19.445709Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:04:20.445880Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:04:20.560237Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:04:21.560571Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:04:21.707202Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:04:22.707487Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:04:28.527991","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"UnsupportedOperationException","exc_value":"MongoCollection already exists","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":101,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":1461,"name":"save"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":185,"name":"deco"}]}]}
{"timestamp":"2025-05-12T01:04:29.662975Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 1) / 1]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:00.132537","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:05:03.708352Z","level":"info","event":":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:05:03.826871Z","level":"error","event":"Ivy Default Cache set to: /home/airflow/.ivy2/cache","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:03.840180Z","level":"error","event":"The jars for the packages stored in: /home/airflow/.ivy2/jars","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:03.840919Z","level":"error","event":"org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:03.853150Z","level":"error","event":":: resolving dependencies :: org.apache.spark#spark-submit-parent-56bfc946-741d-4cb0-94f3-54cb76f4edb4;1.0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:03.853783Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:04.058056Z","level":"error","event":"\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:04.111350Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:04.162342Z","level":"error","event":"\tfound org.mongodb#bson;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:04.207530Z","level":"error","event":"\tfound org.mongodb#mongodb-driver-core;4.0.5 in central","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:04.281942Z","level":"error","event":":: resolution report :: resolve 406ms :: artifacts dl 30ms","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:04.282622Z","level":"error","event":"\t:: modules in use:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:04.283137Z","level":"error","event":"\torg.mongodb#bson;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:04.283748Z","level":"error","event":"\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:04.284254Z","level":"error","event":"\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:04.284752Z","level":"error","event":"\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:04.286772Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:04.291045Z","level":"error","event":"\t|                  |            modules            ||   artifacts   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:04.291844Z","level":"error","event":"\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:04.292555Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:04.293218Z","level":"error","event":"\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:04.294971Z","level":"error","event":"\t---------------------------------------------------------------------","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:04.316480Z","level":"error","event":":: retrieving :: org.apache.spark#spark-submit-parent-56bfc946-741d-4cb0-94f3-54cb76f4edb4","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:04.317370Z","level":"error","event":"\tconfs: [default]","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:04.335183Z","level":"error","event":"\t0 artifacts copied, 4 already retrieved (0kB/26ms)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:04.688585Z","level":"error","event":"25/05/12 01:05:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:05.065600Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:05.066405Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:05:11.050901Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:05:12.044659Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:05:13.045718Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:05:13.209633Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:05:14.209839Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:05:14.319239Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:05:15.319452Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:05:15.427704Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:05:16.427836Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:05:17.307911Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:05:19.510523Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:05:23.904019","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"UnsupportedOperationException","exc_value":"MongoCollection already exists","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":101,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":1461,"name":"save"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":185,"name":"deco"}]}]}
{"timestamp":"2025-05-12T01:11:02.531006","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:11:22.502493Z","level":"info","event":":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:11:22.709770Z","level":"error","event":"Exception in thread \"main\" java.lang.IllegalArgumentException: requirement failed: Provided Maven Coordinates must be in the form 'groupId:artifactId:version'. The coordinate provided is: /opt/airflow/jars/mongo-spark-connector.jar","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:11:22.710584Z","level":"error","event":"\tat scala.Predef$.require(Predef.scala:281)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:11:22.711242Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmitUtils$.$anonfun$extractMavenCoordinates$1(SparkSubmit.scala:1226)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:11:22.711923Z","level":"error","event":"\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:11:22.712502Z","level":"error","event":"\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:11:22.713138Z","level":"error","event":"\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:11:22.713702Z","level":"error","event":"\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:11:22.714318Z","level":"error","event":"\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:11:22.714924Z","level":"error","event":"\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:11:22.715525Z","level":"error","event":"\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:11:22.716139Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmitUtils$.extractMavenCoordinates(SparkSubmit.scala:1224)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:11:22.716912Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:1546)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:11:22.717505Z","level":"error","event":"\tat org.apache.spark.util.DependencyUtils$.resolveMavenDependencies(DependencyUtils.scala:185)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:11:22.718326Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:339)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:11:22.718961Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:11:22.719539Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:11:22.720110Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:11:22.720671Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:11:22.721261Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:11:22.722066Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:11:22.723258Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:11:22.802749","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"PySparkRuntimeError","exc_value":"[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":22,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":497,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":515,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":201,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":436,"name":"_ensure_initialized"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/java_gateway.py","lineno":107,"name":"launch_gateway"}]}]}
{"timestamp":"2025-05-12T01:11:54.201936","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:12:00.712241Z","level":"info","event":":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:12:00.972915Z","level":"error","event":"Exception in thread \"main\" java.lang.IllegalArgumentException: requirement failed: Provided Maven Coordinates must be in the form 'groupId:artifactId:version'. The coordinate provided is: /opt/airflow/jars/mongo-spark-connector.jar","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:00.973905Z","level":"error","event":"\tat scala.Predef$.require(Predef.scala:281)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:00.975192Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmitUtils$.$anonfun$extractMavenCoordinates$1(SparkSubmit.scala:1226)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:00.975993Z","level":"error","event":"\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:00.976824Z","level":"error","event":"\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:00.977640Z","level":"error","event":"\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:00.978502Z","level":"error","event":"\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:00.979407Z","level":"error","event":"\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:00.980204Z","level":"error","event":"\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:00.980942Z","level":"error","event":"\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:00.981898Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmitUtils$.extractMavenCoordinates(SparkSubmit.scala:1224)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:00.982651Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:1546)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:00.983383Z","level":"error","event":"\tat org.apache.spark.util.DependencyUtils$.resolveMavenDependencies(DependencyUtils.scala:185)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:00.984161Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:339)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:00.985155Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:00.986202Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:00.988162Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:00.989332Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:00.990286Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:00.990931Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:00.991602Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:01.048715","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"PySparkRuntimeError","exc_value":"[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":22,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":497,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":515,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":201,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":436,"name":"_ensure_initialized"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/java_gateway.py","lineno":107,"name":"launch_gateway"}]}]}
{"timestamp":"2025-05-12T01:12:32.373893","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:12:39.310365Z","level":"info","event":":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:12:39.679573Z","level":"error","event":"Exception in thread \"main\" java.lang.IllegalArgumentException: requirement failed: Provided Maven Coordinates must be in the form 'groupId:artifactId:version'. The coordinate provided is: /opt/airflow/jars/mongo-spark-connector.jar","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:39.681064Z","level":"error","event":"\tat scala.Predef$.require(Predef.scala:281)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:39.682977Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmitUtils$.$anonfun$extractMavenCoordinates$1(SparkSubmit.scala:1226)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:39.684198Z","level":"error","event":"\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:39.691076Z","level":"error","event":"\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:39.692743Z","level":"error","event":"\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:39.694966Z","level":"error","event":"\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:39.695826Z","level":"error","event":"\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:39.696609Z","level":"error","event":"\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:39.697359Z","level":"error","event":"\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:39.698237Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmitUtils$.extractMavenCoordinates(SparkSubmit.scala:1224)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:39.699131Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:1546)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:39.700205Z","level":"error","event":"\tat org.apache.spark.util.DependencyUtils$.resolveMavenDependencies(DependencyUtils.scala:185)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:39.703831Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:339)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:39.708208Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:39.710015Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:39.711033Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:39.711925Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:39.713155Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:39.715108Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:39.716318Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:12:39.780872","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"PySparkRuntimeError","exc_value":"[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":22,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":497,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":515,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":201,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":436,"name":"_ensure_initialized"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/java_gateway.py","lineno":107,"name":"launch_gateway"}]}]}
{"timestamp":"2025-05-12T01:13:10.972247","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:13:16.692805Z","level":"info","event":":: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:13:17.689835Z","level":"error","event":"Exception in thread \"main\" java.lang.IllegalArgumentException: requirement failed: Provided Maven Coordinates must be in the form 'groupId:artifactId:version'. The coordinate provided is: /opt/airflow/jars/mongo-spark-connector.jar","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:13:17.690928Z","level":"error","event":"\tat scala.Predef$.require(Predef.scala:281)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:13:17.691576Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmitUtils$.$anonfun$extractMavenCoordinates$1(SparkSubmit.scala:1226)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:13:17.692201Z","level":"error","event":"\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:13:17.692811Z","level":"error","event":"\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:13:17.693411Z","level":"error","event":"\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:13:17.694182Z","level":"error","event":"\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:13:17.694738Z","level":"error","event":"\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:13:17.695316Z","level":"error","event":"\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:13:17.695952Z","level":"error","event":"\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:13:17.696595Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmitUtils$.extractMavenCoordinates(SparkSubmit.scala:1224)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:13:17.697111Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:1546)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:13:17.697684Z","level":"error","event":"\tat org.apache.spark.util.DependencyUtils$.resolveMavenDependencies(DependencyUtils.scala:185)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:13:17.698267Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:339)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:13:17.698806Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:13:17.699525Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:13:17.700261Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:13:17.700913Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:13:17.701437Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:13:17.701996Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:13:17.702437Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:13:17.733189","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"PySparkRuntimeError","exc_value":"[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":22,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":497,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":515,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":201,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":436,"name":"_ensure_initialized"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/java_gateway.py","lineno":107,"name":"launch_gateway"}]}]}
{"timestamp":"2025-05-12T01:13:43.628476","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:13:48.139295Z","level":"error","event":"25/05/12 01:13:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:13:48.523453Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:13:48.524311Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:13:53.680836Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:13:55.021243Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:13:56.021463Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:13:56.232020Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:13:57.232291Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:13:57.393952Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:13:58.394178Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:13:58.557962Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:13:59.557979Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:13:59.864675Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:14:00.864885Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:14:02.092526","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: java.lang.NoClassDefFoundError: org/bson/conversions/Bson\n\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:89)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: org.bson.conversions.Bson\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\t... 20 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:14:34.101972","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:14:38.881519Z","level":"error","event":"25/05/12 01:14:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:14:39.112428Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:14:39.127439Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:14:41.469900Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:14:42.400314Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:14:43.400672Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:14:43.509389Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:14:44.509564Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:14:44.579853Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:14:45.580195Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:14:45.666505Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:14:46.666716Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:14:46.773143Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:14:47.773575Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:14:49.069164","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: java.lang.NoClassDefFoundError: org/bson/conversions/Bson\n\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:89)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: org.bson.conversions.Bson\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\t... 20 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:15:20.660457","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:15:28.829289Z","level":"error","event":"25/05/12 01:15:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:15:29.527931Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:15:29.528944Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:15:33.334112Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:15:33.962848Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:15:34.963000Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:15:35.066260Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:15:36.066540Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:15:36.167814Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:15:37.167943Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:15:37.278104Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:15:38.278632Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:15:38.382693Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:15:39.383177Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:15:40.243334","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: java.lang.NoClassDefFoundError: org/bson/conversions/Bson\n\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:89)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: org.bson.conversions.Bson\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\t... 20 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:16:11.368530","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:16:19.328698Z","level":"error","event":"25/05/12 01:16:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:16:19.764509Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:16:19.765381Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:16:23.745668Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:16:24.538428Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:16:25.538746Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:16:25.688346Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:16:26.688480Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:16:26.763719Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:16:27.763829Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:16:27.861796Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:16:28.861991Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:16:29.716607Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:16:31.928562Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:16:32.824484","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: java.lang.NoClassDefFoundError: org/bson/conversions/Bson\n\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:89)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: org.bson.conversions.Bson\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\t... 20 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:17:04.613486","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:17:11.894837Z","level":"error","event":"25/05/12 01:17:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:17:12.409009Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:17:12.410211Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:17:17.298988Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:17:18.487760Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:17:19.487976Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:17:19.602496Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:17:20.602728Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:17:20.729091Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:17:21.729308Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:17:21.838438Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:17:22.838684Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:17:22.959261Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:17:23.959393Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:17:25.053192","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: java.lang.NoClassDefFoundError: org/bson/conversions/Bson\n\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:89)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: org.bson.conversions.Bson\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\t... 20 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:17:56.043704","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:17:59.456225Z","level":"error","event":"25/05/12 01:17:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:17:59.823359Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:17:59.823874Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:01.825932Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:18:02.487536Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:18:03.487639Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:18:03.611589Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:18:04.613377Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:18:04.784861Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:18:07.868071Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:18:08.001189Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:18:09.001673Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:18:09.135256Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:18:10.135474Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:18:14.109325","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: java.lang.NoClassDefFoundError: org/bson/conversions/Bson\n\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:89)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: org.bson.conversions.Bson\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\t... 20 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:18:18.899876","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:18:25.818686Z","level":"error","event":"25/05/12 01:18:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:26.146234Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:26.147090Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:27.466178Z","level":"error","event":"25/05/12 01:18:27 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:27.467375Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:27.468290Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:27.469008Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:27.469797Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:27.470634Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:27.471370Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:27.471896Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:27.472362Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:27.472959Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:27.473557Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:27.474159Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:27.474646Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:27.475171Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:27.475750Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:27.476263Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:27.476819Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:27.477395Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:27.478135Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:27.478757Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:27.479202Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:27.479998Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:27.480909Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:27.481756Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:18:28.376372Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:18:29.172169Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:18:30.172335Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:18:30.305361Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:18:31.307442Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:18:31.449514Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:18:32.396215Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:18:32.519111Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:18:33.519589Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:18:33.616604Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:18:34.617625Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:18:36.221459","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: java.lang.NoClassDefFoundError: org/bson/conversions/Bson\n\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:89)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: org.bson.conversions.Bson\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\t... 20 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:19:08.324986","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:19:16.894001Z","level":"error","event":"25/05/12 01:19:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:19:17.152593Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:19:17.153230Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:19:19.039602Z","level":"error","event":"25/05/12 01:19:19 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:19:19.040460Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:19:19.041291Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:19:19.042174Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:19:19.043163Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:19:19.044058Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:19:19.046070Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:19:19.047108Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:19:19.048018Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:19:19.048843Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:19:19.050070Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:19:19.050940Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:19:19.051833Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:19:19.052720Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:19:19.054027Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:19:19.055131Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:19:19.056191Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:19:19.057004Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:19:19.057745Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:19:19.059649Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:19:19.062703Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:19:19.064012Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:19:19.065616Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:19:19.067553Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:19:20.777310Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:19:21.801699Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:19:22.801943Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:19:22.912037Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:19:23.912385Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:19:23.984864Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:19:24.985060Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:19:25.074224Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:19:26.074523Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:19:26.188265Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:19:27.188410Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:22:58.862711","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:23:23.769712Z","level":"error","event":"25/05/12 01:23:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:25.050000Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:25.051073Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:30.908991","level":"error","event":"Process timed out, PID: 38","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-05-12T01:23:30.909492","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/jobs/YFinance.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 38","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":22,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":497,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":515,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":203,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":296,"name":"_do_init"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":421,"name":"_initialize_context"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1586,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1038,"name":"send_command"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/clientserver.py","lineno":511,"name":"send_command"},{"filename":"/usr/local/lib/python3.12/socket.py","lineno":720,"name":"readinto"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-05-12T01:23:30.913504","level":"info","event":"Closing down clientserver connection","logger":"py4j.clientserver"}
{"timestamp":"2025-05-12T01:23:31.178382Z","level":"error","event":"25/05/12 01:23:31 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.188306Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.189009Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.189587Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.190108Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.190632Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.191151Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.191778Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.192363Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.192918Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.193448Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.197237Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.199120Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.199719Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.202113Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.202737Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.203307Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.203866Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.204436Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.205091Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.205862Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.206447Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.207119Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.208596Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.484603Z","level":"error","event":"25/05/12 01:23:31 ERROR SparkContext: Error initializing SparkContext.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.485522Z","level":"error","event":"java.lang.IllegalStateException: Shutdown hooks cannot be modified during shutdown.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.487005Z","level":"error","event":"\tat org.apache.spark.util.SparkShutdownHookManager.add(ShutdownHookManager.scala:195)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.494756Z","level":"error","event":"\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.495612Z","level":"error","event":"\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:142)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.496952Z","level":"error","event":"\tat org.apache.spark.executor.Executor.<init>(Executor.scala:90)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.497944Z","level":"error","event":"\tat org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.509035Z","level":"error","event":"\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.510353Z","level":"error","event":"\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.511112Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:599)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.515755Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.528302Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.528986Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.529632Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.530231Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.530772Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.531337Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.531970Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.533181Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.533876Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.535936Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.536733Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.537412Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.538128Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.581221Z","level":"error","event":"25/05/12 01:23:31 ERROR Utils: Uncaught exception in thread Thread-4","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.581934Z","level":"error","event":"java.lang.NullPointerException: Cannot invoke \"org.apache.spark.rpc.RpcEndpointRef.ask(Object, scala.reflect.ClassTag)\" because the return value of \"org.apache.spark.scheduler.local.LocalSchedulerBackend.localEndpoint()\" is null","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.582740Z","level":"error","event":"\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.org$apache$spark$scheduler$local$LocalSchedulerBackend$$stop(LocalSchedulerBackend.scala:173)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.583475Z","level":"error","event":"\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.stop(LocalSchedulerBackend.scala:144)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.584173Z","level":"error","event":"\tat org.apache.spark.scheduler.SchedulerBackend.stop(SchedulerBackend.scala:33)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.598416Z","level":"error","event":"\tat org.apache.spark.scheduler.SchedulerBackend.stop$(SchedulerBackend.scala:33)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.607700Z","level":"error","event":"\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.stop(LocalSchedulerBackend.scala:103)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.618296Z","level":"error","event":"\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$stop$2(TaskSchedulerImpl.scala:992)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.628644Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.633767Z","level":"error","event":"\tat org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:992)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.638285Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$4(DAGScheduler.scala:2976)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.640380Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.648309Z","level":"error","event":"\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2976)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.650789Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2258)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.652574Z","level":"error","event":"\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.655247Z","level":"error","event":"\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2258)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.668288Z","level":"error","event":"\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.678458Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:706)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.686048Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.688007Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.689590Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.698723Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.700524Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.702611Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.708322Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.713923Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.715723Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.728821Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.738260Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.739961Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.740993Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.741717Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:23:31.754569Z","level":"error","event":"25/05/12 01:23:31 WARN MetricsSystem: Stopping a MetricsSystem that is not running","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:02.471063","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:24:13.184956Z","level":"error","event":"25/05/12 01:24:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:13.621661Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:13.622624Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:15.455940Z","level":"error","event":"25/05/12 01:24:15 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:15.458014Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:15.458726Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:15.459302Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:15.459855Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:15.460419Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:15.460912Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:15.462080Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:15.462717Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:15.463259Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:15.463829Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:15.464333Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:15.465041Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:15.465632Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:15.467939Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:15.468461Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:15.468934Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:15.470001Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:15.470652Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:15.471502Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:15.477211Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:15.477876Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:15.478548Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:15.479174Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:16.661502Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:24:18.022307Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:24:19.022370Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:24:19.221377Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:24:20.221422Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:24:20.370311Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:24:21.370486Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:24:21.560714Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:24:22.560923Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:24:22.709414Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:24:23.709638Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:24:26.079918Z","level":"error","event":"25/05/12 01:24:26 ERROR TorrentBroadcast: Store broadcast broadcast_0 fail, remove all pieces of the broadcast","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:24:26.378477","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: java.lang.NoClassDefFoundError: com/mongodb/client/MongoClient\n\tat java.base/java.lang.Class.getDeclaredMethods0(Native Method)\n\tat java.base/java.lang.Class.privateGetDeclaredMethods(Class.java:3402)\n\tat java.base/java.lang.Class.getDeclaredMethod(Class.java:2673)\n\tat java.base/java.io.ObjectStreamClass.getPrivateMethod(ObjectStreamClass.java:1524)\n\tat java.base/java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:413)\n\tat java.base/java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:384)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:318)\n\tat java.base/java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:384)\n\tat java.base/java.io.ObjectStreamClass$Caches$1.computeValue(ObjectStreamClass.java:110)\n\tat java.base/java.io.ObjectStreamClass$Caches$1.computeValue(ObjectStreamClass.java:107)\n\tat java.base/java.io.ClassCache$1.computeValue(ClassCache.java:73)\n\tat java.base/java.io.ClassCache$1.computeValue(ClassCache.java:70)\n\tat java.base/java.lang.ClassValue.getFromHashMap(ClassValue.java:228)\n\tat java.base/java.lang.ClassValue.getFromBackup(ClassValue.java:210)\n\tat java.base/java.lang.ClassValue.get(ClassValue.java:116)\n\tat java.base/java.io.ClassCache.get(ClassCache.java:84)\n\tat java.base/java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:363)\n\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1137)\n\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$4(TorrentBroadcast.scala:365)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:367)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n\tat com.mongodb.spark.MongoSpark.rdd(MongoSpark.scala:530)\n\tat com.mongodb.spark.MongoSpark.toRDD(MongoSpark.scala:542)\n\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:93)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: com.mongodb.client.MongoClient\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\t... 53 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:24:57.563383","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:25:05.610195Z","level":"error","event":"25/05/12 01:25:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:05.938425Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:05.939171Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:07.506563Z","level":"error","event":"25/05/12 01:25:07 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:07.507514Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:07.508272Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:07.509015Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:07.509734Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:07.510378Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:07.511092Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:07.511926Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:07.512876Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:07.514031Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:07.515076Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:07.516429Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:07.517478Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:07.518504Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:07.519449Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:07.520416Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:07.521917Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:07.522769Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:07.523700Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:07.524748Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:07.525637Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:07.526515Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:07.527354Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:07.528120Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:08.624878Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:25:09.211207Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:25:10.211547Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:25:10.343645Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:25:11.343602Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:25:11.452437Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:25:12.452533Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:25:12.589393Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:25:13.589552Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:25:13.689369Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:25:14.689219Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:25:16.218098Z","level":"error","event":"25/05/12 01:25:16 ERROR TorrentBroadcast: Store broadcast broadcast_0 fail, remove all pieces of the broadcast","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:16.344000","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: java.lang.NoClassDefFoundError: com/mongodb/client/MongoClient\n\tat java.base/java.lang.Class.getDeclaredMethods0(Native Method)\n\tat java.base/java.lang.Class.privateGetDeclaredMethods(Class.java:3402)\n\tat java.base/java.lang.Class.getDeclaredMethod(Class.java:2673)\n\tat java.base/java.io.ObjectStreamClass.getPrivateMethod(ObjectStreamClass.java:1524)\n\tat java.base/java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:413)\n\tat java.base/java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:384)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:318)\n\tat java.base/java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:384)\n\tat java.base/java.io.ObjectStreamClass$Caches$1.computeValue(ObjectStreamClass.java:110)\n\tat java.base/java.io.ObjectStreamClass$Caches$1.computeValue(ObjectStreamClass.java:107)\n\tat java.base/java.io.ClassCache$1.computeValue(ClassCache.java:73)\n\tat java.base/java.io.ClassCache$1.computeValue(ClassCache.java:70)\n\tat java.base/java.lang.ClassValue.getFromHashMap(ClassValue.java:228)\n\tat java.base/java.lang.ClassValue.getFromBackup(ClassValue.java:210)\n\tat java.base/java.lang.ClassValue.get(ClassValue.java:116)\n\tat java.base/java.io.ClassCache.get(ClassCache.java:84)\n\tat java.base/java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:363)\n\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1137)\n\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$4(TorrentBroadcast.scala:365)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:367)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n\tat com.mongodb.spark.MongoSpark.rdd(MongoSpark.scala:530)\n\tat com.mongodb.spark.MongoSpark.toRDD(MongoSpark.scala:542)\n\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:93)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: com.mongodb.client.MongoClient\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\t... 53 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:25:47.976605","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:25:53.215808Z","level":"error","event":"25/05/12 01:25:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:53.603201Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:53.622154Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:55.046468Z","level":"error","event":"25/05/12 01:25:55 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:55.047129Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:55.047700Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:55.048226Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:55.048772Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:55.049253Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:55.049826Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:55.050380Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:55.050894Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:55.051478Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:55.052086Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:55.052631Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:55.053099Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:55.053572Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:55.054045Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:55.054499Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:55.055149Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:55.055714Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:55.056383Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:55.057045Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:55.057713Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:55.058407Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:55.059218Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:55.059980Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:25:55.740763Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:25:56.346315Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:25:57.346585Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:25:57.458615Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:25:58.458823Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:25:58.549182Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:25:59.549462Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:25:59.647492Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:26:00.647648Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:26:00.760735Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:26:01.760942Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:26:03.386319Z","level":"error","event":"25/05/12 01:26:03 ERROR TorrentBroadcast: Store broadcast broadcast_0 fail, remove all pieces of the broadcast","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:03.533954","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: java.lang.NoClassDefFoundError: com/mongodb/client/MongoClient\n\tat java.base/java.lang.Class.getDeclaredMethods0(Native Method)\n\tat java.base/java.lang.Class.privateGetDeclaredMethods(Class.java:3402)\n\tat java.base/java.lang.Class.getDeclaredMethod(Class.java:2673)\n\tat java.base/java.io.ObjectStreamClass.getPrivateMethod(ObjectStreamClass.java:1524)\n\tat java.base/java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:413)\n\tat java.base/java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:384)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:318)\n\tat java.base/java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:384)\n\tat java.base/java.io.ObjectStreamClass$Caches$1.computeValue(ObjectStreamClass.java:110)\n\tat java.base/java.io.ObjectStreamClass$Caches$1.computeValue(ObjectStreamClass.java:107)\n\tat java.base/java.io.ClassCache$1.computeValue(ClassCache.java:73)\n\tat java.base/java.io.ClassCache$1.computeValue(ClassCache.java:70)\n\tat java.base/java.lang.ClassValue.getFromHashMap(ClassValue.java:228)\n\tat java.base/java.lang.ClassValue.getFromBackup(ClassValue.java:210)\n\tat java.base/java.lang.ClassValue.get(ClassValue.java:116)\n\tat java.base/java.io.ClassCache.get(ClassCache.java:84)\n\tat java.base/java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:363)\n\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1137)\n\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$4(TorrentBroadcast.scala:365)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:367)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n\tat com.mongodb.spark.MongoSpark.rdd(MongoSpark.scala:530)\n\tat com.mongodb.spark.MongoSpark.toRDD(MongoSpark.scala:542)\n\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:93)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: com.mongodb.client.MongoClient\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\t... 53 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:26:34.672366","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:26:40.679375Z","level":"error","event":"25/05/12 01:26:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:40.912605Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:40.924460Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:42.710686Z","level":"error","event":"25/05/12 01:26:42 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:42.711471Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:42.712328Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:42.713222Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:42.713974Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:42.714531Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:42.715051Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:42.715542Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:42.716046Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:42.716607Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:42.717196Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:42.717842Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:42.718390Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:42.719108Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:42.719752Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:42.720274Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:42.720927Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:42.721656Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:42.722390Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:42.723126Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:42.723699Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:42.724792Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:42.725538Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:42.726306Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:43.340840Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:26:44.147191Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:26:45.147312Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:26:45.256489Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:26:46.256645Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:26:46.325465Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:26:47.325579Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:26:47.415298Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:26:48.415640Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:26:48.498209Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:26:49.498384Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:26:51.422430Z","level":"error","event":"25/05/12 01:26:51 ERROR TorrentBroadcast: Store broadcast broadcast_0 fail, remove all pieces of the broadcast","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:26:51.537260","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: java.lang.NoClassDefFoundError: com/mongodb/client/MongoClient\n\tat java.base/java.lang.Class.getDeclaredMethods0(Native Method)\n\tat java.base/java.lang.Class.privateGetDeclaredMethods(Class.java:3402)\n\tat java.base/java.lang.Class.getDeclaredMethod(Class.java:2673)\n\tat java.base/java.io.ObjectStreamClass.getPrivateMethod(ObjectStreamClass.java:1524)\n\tat java.base/java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:413)\n\tat java.base/java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:384)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:318)\n\tat java.base/java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:384)\n\tat java.base/java.io.ObjectStreamClass$Caches$1.computeValue(ObjectStreamClass.java:110)\n\tat java.base/java.io.ObjectStreamClass$Caches$1.computeValue(ObjectStreamClass.java:107)\n\tat java.base/java.io.ClassCache$1.computeValue(ClassCache.java:73)\n\tat java.base/java.io.ClassCache$1.computeValue(ClassCache.java:70)\n\tat java.base/java.lang.ClassValue.getFromHashMap(ClassValue.java:228)\n\tat java.base/java.lang.ClassValue.getFromBackup(ClassValue.java:210)\n\tat java.base/java.lang.ClassValue.get(ClassValue.java:116)\n\tat java.base/java.io.ClassCache.get(ClassCache.java:84)\n\tat java.base/java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:363)\n\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1137)\n\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$4(TorrentBroadcast.scala:365)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:367)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n\tat com.mongodb.spark.MongoSpark.rdd(MongoSpark.scala:530)\n\tat com.mongodb.spark.MongoSpark.toRDD(MongoSpark.scala:542)\n\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:93)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: com.mongodb.client.MongoClient\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\t... 53 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:27:22.799390","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:27:26.322511Z","level":"error","event":"25/05/12 01:27:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:26.573837Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:26.589535Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:27.668993Z","level":"error","event":"25/05/12 01:27:27 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:27.669490Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:27.669849Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:27.670166Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:27.670462Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:27.670821Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:27.671213Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:27.671682Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:27.672006Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:27.672300Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:27.672593Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:27.672871Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:27.673148Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:27.673427Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:27.673718Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:27.674022Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:27.674305Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:27.674637Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:27.674979Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:27.675320Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:27.675624Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:27.675933Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:27.676291Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:27.676565Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:28.332497Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:27:29.151014Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:27:30.151173Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:27:30.251490Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:27:31.251717Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:27:31.346185Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:27:32.346423Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:27:32.459778Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:27:33.459950Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:27:33.557246Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:27:34.557540Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:27:35.831105Z","level":"error","event":"25/05/12 01:27:35 ERROR TorrentBroadcast: Store broadcast broadcast_0 fail, remove all pieces of the broadcast","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:27:35.917825","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: java.lang.NoClassDefFoundError: com/mongodb/client/MongoClient\n\tat java.base/java.lang.Class.getDeclaredMethods0(Native Method)\n\tat java.base/java.lang.Class.privateGetDeclaredMethods(Class.java:3402)\n\tat java.base/java.lang.Class.getDeclaredMethod(Class.java:2673)\n\tat java.base/java.io.ObjectStreamClass.getPrivateMethod(ObjectStreamClass.java:1524)\n\tat java.base/java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:413)\n\tat java.base/java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:384)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:318)\n\tat java.base/java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:384)\n\tat java.base/java.io.ObjectStreamClass$Caches$1.computeValue(ObjectStreamClass.java:110)\n\tat java.base/java.io.ObjectStreamClass$Caches$1.computeValue(ObjectStreamClass.java:107)\n\tat java.base/java.io.ClassCache$1.computeValue(ClassCache.java:73)\n\tat java.base/java.io.ClassCache$1.computeValue(ClassCache.java:70)\n\tat java.base/java.lang.ClassValue.getFromHashMap(ClassValue.java:228)\n\tat java.base/java.lang.ClassValue.getFromBackup(ClassValue.java:210)\n\tat java.base/java.lang.ClassValue.get(ClassValue.java:116)\n\tat java.base/java.io.ClassCache.get(ClassCache.java:84)\n\tat java.base/java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:363)\n\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1137)\n\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$4(TorrentBroadcast.scala:365)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:367)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n\tat com.mongodb.spark.MongoSpark.rdd(MongoSpark.scala:530)\n\tat com.mongodb.spark.MongoSpark.toRDD(MongoSpark.scala:542)\n\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:93)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: com.mongodb.client.MongoClient\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\t... 53 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:28:07.508575","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:28:17.221863Z","level":"error","event":"25/05/12 01:28:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:17.639756Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:17.661918Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:20.761551Z","level":"error","event":"25/05/12 01:28:20 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:20.765061Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:20.767918Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:20.769533Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:20.770440Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:20.771203Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:20.771888Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:20.772556Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:20.773569Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:20.774317Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:20.775095Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:20.775896Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:20.780005Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:20.780935Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:20.782115Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:20.784721Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:20.785806Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:20.786496Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:20.789431Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:20.790318Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:20.790958Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:20.791602Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:20.792165Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:20.792806Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:22.037486Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:28:22.782754Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:28:23.782882Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:28:23.893301Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:28:24.893519Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:28:24.971481Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:28:25.971978Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:28:26.075888Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:28:27.076064Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:28:27.175414Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:28:28.175845Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:28:29.208527Z","level":"error","event":"25/05/12 01:28:29 ERROR TorrentBroadcast: Store broadcast broadcast_0 fail, remove all pieces of the broadcast","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:28:29.402526","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: java.lang.NoClassDefFoundError: com/mongodb/client/MongoClient\n\tat java.base/java.lang.Class.getDeclaredMethods0(Native Method)\n\tat java.base/java.lang.Class.privateGetDeclaredMethods(Class.java:3402)\n\tat java.base/java.lang.Class.getDeclaredMethod(Class.java:2673)\n\tat java.base/java.io.ObjectStreamClass.getPrivateMethod(ObjectStreamClass.java:1524)\n\tat java.base/java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:413)\n\tat java.base/java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:384)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:318)\n\tat java.base/java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:384)\n\tat java.base/java.io.ObjectStreamClass$Caches$1.computeValue(ObjectStreamClass.java:110)\n\tat java.base/java.io.ObjectStreamClass$Caches$1.computeValue(ObjectStreamClass.java:107)\n\tat java.base/java.io.ClassCache$1.computeValue(ClassCache.java:73)\n\tat java.base/java.io.ClassCache$1.computeValue(ClassCache.java:70)\n\tat java.base/java.lang.ClassValue.getFromHashMap(ClassValue.java:228)\n\tat java.base/java.lang.ClassValue.getFromBackup(ClassValue.java:210)\n\tat java.base/java.lang.ClassValue.get(ClassValue.java:116)\n\tat java.base/java.io.ClassCache.get(ClassCache.java:84)\n\tat java.base/java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:363)\n\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1137)\n\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$4(TorrentBroadcast.scala:365)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:367)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n\tat com.mongodb.spark.MongoSpark.rdd(MongoSpark.scala:530)\n\tat com.mongodb.spark.MongoSpark.toRDD(MongoSpark.scala:542)\n\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:93)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: com.mongodb.client.MongoClient\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\t... 53 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:29:00.638253","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:29:06.692915Z","level":"error","event":"25/05/12 01:29:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:07.151173Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:07.165336Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:09.212596Z","level":"error","event":"25/05/12 01:29:09 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:09.213718Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:09.214682Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:09.215451Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:09.216405Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:09.217211Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:09.217985Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:09.218743Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:09.219469Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:09.220261Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:09.221173Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:09.221859Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:09.222654Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:09.223389Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:09.223936Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:09.224582Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:09.225344Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:09.226090Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:09.226849Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:09.227614Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:09.228463Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:09.229170Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:09.230779Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:09.231604Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:10.349465Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:29:11.062797Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:29:12.062962Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:29:12.184161Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:29:13.184723Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:29:13.280454Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:29:14.280960Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:29:14.387322Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:29:15.387394Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:29:15.495820Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:29:16.496072Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:29:17.427640Z","level":"error","event":"25/05/12 01:29:17 ERROR TorrentBroadcast: Store broadcast broadcast_0 fail, remove all pieces of the broadcast","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:17.494540","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: java.lang.NoClassDefFoundError: com/mongodb/client/MongoClient\n\tat java.base/java.lang.Class.getDeclaredMethods0(Native Method)\n\tat java.base/java.lang.Class.privateGetDeclaredMethods(Class.java:3402)\n\tat java.base/java.lang.Class.getDeclaredMethod(Class.java:2673)\n\tat java.base/java.io.ObjectStreamClass.getPrivateMethod(ObjectStreamClass.java:1524)\n\tat java.base/java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:413)\n\tat java.base/java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:384)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:318)\n\tat java.base/java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:384)\n\tat java.base/java.io.ObjectStreamClass$Caches$1.computeValue(ObjectStreamClass.java:110)\n\tat java.base/java.io.ObjectStreamClass$Caches$1.computeValue(ObjectStreamClass.java:107)\n\tat java.base/java.io.ClassCache$1.computeValue(ClassCache.java:73)\n\tat java.base/java.io.ClassCache$1.computeValue(ClassCache.java:70)\n\tat java.base/java.lang.ClassValue.getFromHashMap(ClassValue.java:228)\n\tat java.base/java.lang.ClassValue.getFromBackup(ClassValue.java:210)\n\tat java.base/java.lang.ClassValue.get(ClassValue.java:116)\n\tat java.base/java.io.ClassCache.get(ClassCache.java:84)\n\tat java.base/java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:363)\n\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1137)\n\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$4(TorrentBroadcast.scala:365)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:367)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n\tat com.mongodb.spark.MongoSpark.rdd(MongoSpark.scala:530)\n\tat com.mongodb.spark.MongoSpark.toRDD(MongoSpark.scala:542)\n\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:93)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: com.mongodb.client.MongoClient\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\t... 53 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:29:52.747392","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:29:56.215834Z","level":"error","event":"25/05/12 01:29:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:56.507997Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:56.520630Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:57.592891Z","level":"error","event":"25/05/12 01:29:57 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:57.593425Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:57.593911Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:57.594517Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:57.595060Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:57.595514Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:57.595930Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:57.596305Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:57.596659Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:57.597033Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:57.597432Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:57.597791Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:57.598157Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:57.598713Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:57.599141Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:57.599549Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:57.599934Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:57.600360Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:57.600815Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:57.601232Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:57.601641Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:57.602092Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:57.602534Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:57.603053Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:29:58.143086Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:29:58.712057Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:29:59.712218Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:29:59.849183Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:30:00.849354Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:30:00.938445Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:30:01.938646Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:30:02.121534Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:30:03.121641Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:30:03.482356Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:30:04.482528Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:30:07.191202Z","level":"error","event":"25/05/12 01:30:07 ERROR TorrentBroadcast: Store broadcast broadcast_0 fail, remove all pieces of the broadcast","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:07.699608","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: java.lang.NoClassDefFoundError: com/mongodb/client/MongoClient\n\tat java.base/java.lang.Class.getDeclaredMethods0(Native Method)\n\tat java.base/java.lang.Class.privateGetDeclaredMethods(Class.java:3402)\n\tat java.base/java.lang.Class.getDeclaredMethod(Class.java:2673)\n\tat java.base/java.io.ObjectStreamClass.getPrivateMethod(ObjectStreamClass.java:1524)\n\tat java.base/java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:413)\n\tat java.base/java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:384)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:318)\n\tat java.base/java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:384)\n\tat java.base/java.io.ObjectStreamClass$Caches$1.computeValue(ObjectStreamClass.java:110)\n\tat java.base/java.io.ObjectStreamClass$Caches$1.computeValue(ObjectStreamClass.java:107)\n\tat java.base/java.io.ClassCache$1.computeValue(ClassCache.java:73)\n\tat java.base/java.io.ClassCache$1.computeValue(ClassCache.java:70)\n\tat java.base/java.lang.ClassValue.getFromHashMap(ClassValue.java:228)\n\tat java.base/java.lang.ClassValue.getFromBackup(ClassValue.java:210)\n\tat java.base/java.lang.ClassValue.get(ClassValue.java:116)\n\tat java.base/java.io.ClassCache.get(ClassCache.java:84)\n\tat java.base/java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:363)\n\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1137)\n\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$4(TorrentBroadcast.scala:365)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:367)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n\tat com.mongodb.spark.MongoSpark.rdd(MongoSpark.scala:530)\n\tat com.mongodb.spark.MongoSpark.toRDD(MongoSpark.scala:542)\n\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:93)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: com.mongodb.client.MongoClient\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\t... 53 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:30:39.719040","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:30:44.430277Z","level":"error","event":"25/05/12 01:30:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:44.738491Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:44.738987Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:46.126519Z","level":"error","event":"25/05/12 01:30:46 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:46.127511Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:46.128114Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:46.128687Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:46.129207Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:46.129688Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:46.130225Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:46.130808Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:46.131393Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:46.131949Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:46.132600Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:46.133133Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:46.133702Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:46.134409Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:46.135271Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:46.135912Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:46.136658Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:46.137429Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:46.138202Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:46.138999Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:46.139788Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:46.140451Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:46.141109Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:46.141751Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:46.964713Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:30:47.852264Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:30:48.852488Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:30:48.967572Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:30:49.967950Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:30:50.043677Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:30:51.043863Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:30:51.164224Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:30:52.164353Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:30:52.266398Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:30:53.266628Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:30:56.926188Z","level":"error","event":"25/05/12 01:30:56 ERROR TorrentBroadcast: Store broadcast broadcast_0 fail, remove all pieces of the broadcast","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:30:57.110418","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: java.lang.NoClassDefFoundError: com/mongodb/client/MongoClient\n\tat java.base/java.lang.Class.getDeclaredMethods0(Native Method)\n\tat java.base/java.lang.Class.privateGetDeclaredMethods(Class.java:3402)\n\tat java.base/java.lang.Class.getDeclaredMethod(Class.java:2673)\n\tat java.base/java.io.ObjectStreamClass.getPrivateMethod(ObjectStreamClass.java:1524)\n\tat java.base/java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:413)\n\tat java.base/java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:384)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:318)\n\tat java.base/java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:384)\n\tat java.base/java.io.ObjectStreamClass$Caches$1.computeValue(ObjectStreamClass.java:110)\n\tat java.base/java.io.ObjectStreamClass$Caches$1.computeValue(ObjectStreamClass.java:107)\n\tat java.base/java.io.ClassCache$1.computeValue(ClassCache.java:73)\n\tat java.base/java.io.ClassCache$1.computeValue(ClassCache.java:70)\n\tat java.base/java.lang.ClassValue.getFromHashMap(ClassValue.java:228)\n\tat java.base/java.lang.ClassValue.getFromBackup(ClassValue.java:210)\n\tat java.base/java.lang.ClassValue.get(ClassValue.java:116)\n\tat java.base/java.io.ClassCache.get(ClassCache.java:84)\n\tat java.base/java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:363)\n\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1137)\n\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$4(TorrentBroadcast.scala:365)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:367)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n\tat com.mongodb.spark.MongoSpark.rdd(MongoSpark.scala:530)\n\tat com.mongodb.spark.MongoSpark.toRDD(MongoSpark.scala:542)\n\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:93)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: com.mongodb.client.MongoClient\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\t... 53 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:31:28.581969","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:31:33.969888Z","level":"error","event":"25/05/12 01:31:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:34.375856Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:34.376966Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:35.986777Z","level":"error","event":"25/05/12 01:31:35 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:35.987490Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:35.988030Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:35.988591Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:35.989095Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:35.989587Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:35.990091Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:35.990673Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:35.991311Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:35.991794Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:35.992275Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:35.992852Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:35.993675Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:35.994354Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:35.995089Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:35.995827Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:35.996518Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:35.997114Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:35.997809Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:35.998463Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:35.999106Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:35.999719Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:36.000384Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:36.001176Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:36.915806Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:31:37.595422Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:31:38.595629Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:31:38.713483Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:31:39.713672Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:31:39.803133Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:31:40.803270Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:31:40.916586Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:31:41.916870Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:31:42.022010Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:31:43.022097Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:31:44.151552Z","level":"error","event":"25/05/12 01:31:44 ERROR TorrentBroadcast: Store broadcast broadcast_0 fail, remove all pieces of the broadcast","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:31:44.216213","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: java.lang.NoClassDefFoundError: com/mongodb/client/MongoClient\n\tat java.base/java.lang.Class.getDeclaredMethods0(Native Method)\n\tat java.base/java.lang.Class.privateGetDeclaredMethods(Class.java:3402)\n\tat java.base/java.lang.Class.getDeclaredMethod(Class.java:2673)\n\tat java.base/java.io.ObjectStreamClass.getPrivateMethod(ObjectStreamClass.java:1524)\n\tat java.base/java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:413)\n\tat java.base/java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:384)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:318)\n\tat java.base/java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:384)\n\tat java.base/java.io.ObjectStreamClass$Caches$1.computeValue(ObjectStreamClass.java:110)\n\tat java.base/java.io.ObjectStreamClass$Caches$1.computeValue(ObjectStreamClass.java:107)\n\tat java.base/java.io.ClassCache$1.computeValue(ClassCache.java:73)\n\tat java.base/java.io.ClassCache$1.computeValue(ClassCache.java:70)\n\tat java.base/java.lang.ClassValue.getFromHashMap(ClassValue.java:228)\n\tat java.base/java.lang.ClassValue.getFromBackup(ClassValue.java:210)\n\tat java.base/java.lang.ClassValue.get(ClassValue.java:116)\n\tat java.base/java.io.ClassCache.get(ClassCache.java:84)\n\tat java.base/java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:363)\n\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1137)\n\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.$anonfun$blockifyObject$4(TorrentBroadcast.scala:365)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:367)\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:161)\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\n\tat com.mongodb.spark.MongoSpark.rdd(MongoSpark.scala:530)\n\tat com.mongodb.spark.MongoSpark.toRDD(MongoSpark.scala:542)\n\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:93)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: com.mongodb.client.MongoClient\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\t... 53 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:34:57.961172","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:35:17.344276Z","level":"error","event":"25/05/12 01:35:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:35:18.334863Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:35:18.336313Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:35:22.847422Z","level":"error","event":"25/05/12 01:35:22 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:35:22.850670Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:35:22.860833Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:35:22.861738Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:35:22.863731Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:35:22.866564Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:35:22.868631Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:35:22.869504Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:35:22.870142Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:35:22.870676Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:35:22.871256Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:35:22.871839Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:35:22.872579Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:35:22.873240Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:35:22.873828Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:35:22.874458Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:35:22.876087Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:35:22.877814Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:35:22.879896Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:35:22.881386Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:35:22.884493Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:35:22.885478Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:35:22.886164Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:35:22.887759Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:35:24.534937Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:35:25.983407Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:35:26.983548Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:35:27.357712Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:35:28.358235Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:35:28.535496Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:35:29.536654Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:35:29.827715Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:35:30.060103","level":"error","event":"Process timed out, PID: 37","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-05-12T01:35:30.061211","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/jobs/YFinance.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 37","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":53,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-05-12T01:36:02.601268","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:36:12.405158Z","level":"error","event":"25/05/12 01:36:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:36:13.020465Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:36:13.021514Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:36:17.455119Z","level":"error","event":"25/05/12 01:36:17 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:36:17.455837Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:36:17.456462Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:36:17.457021Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:36:17.457913Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:36:17.458848Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:36:17.459791Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:36:17.461044Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:36:17.461805Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:36:17.462440Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:36:17.463059Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:36:17.463612Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:36:17.464210Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:36:17.464738Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:36:17.465438Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:36:17.466095Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:36:17.466710Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:36:17.467246Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:36:17.467742Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:36:17.468196Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:36:17.468644Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:36:17.469152Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:36:17.469811Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:36:17.470413Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:36:18.622263Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:36:19.485736Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:36:20.486076Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:36:20.631449Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:36:21.631667Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:36:21.719179Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:36:22.719371Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:36:22.831214Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:36:23.831482Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:36:24.130560Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:36:25.130930Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:36:28.846861","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:37:01.224642","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:37:11.828393Z","level":"error","event":"25/05/12 01:37:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:37:12.579153Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:37:12.580666Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:37:15.048552Z","level":"error","event":"25/05/12 01:37:15 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:37:15.049537Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:37:15.050383Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:37:15.051174Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:37:15.051986Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:37:15.052902Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:37:15.053674Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:37:15.056087Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:37:15.056951Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:37:15.058526Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:37:15.059517Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:37:15.060338Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:37:15.061212Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:37:15.062132Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:37:15.062982Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:37:15.063806Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:37:15.064640Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:37:15.065607Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:37:15.066407Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:37:15.067285Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:37:15.068043Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:37:15.068712Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:37:15.069564Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:37:15.070356Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:37:16.439450Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:37:17.257954Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:37:18.258148Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:37:18.382185Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:37:21.384068Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:37:21.456871Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:37:22.457062Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:37:22.546629Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:37:23.546966Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:37:23.636717Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:37:24.636963Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:37:26.457639","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:37:57.680925","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:38:01.762278Z","level":"error","event":"25/05/12 01:38:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:02.079916Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:02.080707Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:04.142720Z","level":"error","event":"25/05/12 01:38:04 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:04.144392Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:04.145328Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:04.146322Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:04.147082Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:04.149014Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:04.152978Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:04.153801Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:04.154913Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:04.155938Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:04.156727Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:04.157591Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:04.158479Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:04.162649Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:04.165006Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:04.168643Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:04.174357Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:04.176048Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:04.177056Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:04.180469Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:04.182417Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:04.187883Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:04.189356Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:04.190779Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:05.340031Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:38:06.159880Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:38:07.160033Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:38:07.268282Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:38:08.268453Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:38:08.365939Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:38:09.367100Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:38:09.509441Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:38:10.509768Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:38:10.606341Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:38:11.606472Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:38:14.055588","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:38:45.317193","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:38:51.131001Z","level":"error","event":"25/05/12 01:38:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:51.869762Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:51.872053Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:54.067297Z","level":"error","event":"25/05/12 01:38:54 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:54.068079Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:54.068777Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:54.069570Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:54.070282Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:54.071010Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:54.071714Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:54.072403Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:54.073115Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:54.073743Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:54.074421Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:54.075183Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:54.075953Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:54.076959Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:54.077910Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:54.078693Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:54.079711Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:54.080436Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:54.081640Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:54.082422Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:54.082955Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:54.083586Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:54.084313Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:54.085039Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:38:57.375258Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:38:58.106487Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:38:59.106746Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:38:59.234641Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:39:00.234854Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:39:00.384141Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:39:01.384421Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:39:01.508699Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:39:02.508855Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:39:02.652926Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:39:03.653872Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:39:07.908483","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:39:39.954843","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:39:52.209444Z","level":"error","event":"25/05/12 01:39:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:39:52.602615Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:39:52.620094Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:39:54.046720Z","level":"error","event":"25/05/12 01:39:54 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:39:54.047883Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:39:54.048874Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:39:54.050174Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:39:54.052249Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:39:54.053453Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:39:54.054508Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:39:54.055244Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:39:54.055894Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:39:54.056603Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:39:54.057213Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:39:54.057783Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:39:54.058631Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:39:54.059417Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:39:54.060318Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:39:54.061180Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:39:54.061872Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:39:54.062421Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:39:54.063061Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:39:54.063685Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:39:54.064316Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:39:54.064864Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:39:54.065419Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:39:54.066068Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:39:55.055413Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:39:55.769566Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:39:56.769892Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:39:56.905790Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:39:57.906062Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:39:57.991230Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:40:01.068223Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:40:01.169722Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:40:02.174283Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:40:02.266452Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:40:03.266727Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:40:06.575692","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:47:44.087267","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:48:04.513530Z","level":"error","event":"25/05/12 01:48:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:05.467247Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:05.467970Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:08.789225Z","level":"error","event":"25/05/12 01:48:08 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:08.790046Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:08.790664Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:08.791239Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:08.791781Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:08.792382Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:08.792942Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:08.793726Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:08.795584Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:08.796507Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:08.797142Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:08.797722Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:08.798346Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:08.798961Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:08.799809Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:08.801382Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:08.802480Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:08.809549Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:08.810788Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:08.811461Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:08.812238Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:08.813648Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:08.814551Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:08.819512Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:11.324799Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:48:12.914155Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:48:13.914333Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:48:14.148449Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:48:15.149921Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:48:15.351597Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:48:16.138870","level":"error","event":"Process timed out, PID: 39","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-05-12T01:48:16.139361","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/jobs/YFinance.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 39","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":53,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-05-12T01:48:48.144496","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:48:56.747239Z","level":"error","event":"25/05/12 01:48:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:57.406809Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:48:57.407557Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:49:01.549034Z","level":"error","event":"25/05/12 01:49:01 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:49:01.550085Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:49:01.551044Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:49:01.554020Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:49:01.557151Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:49:01.557891Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:49:01.558765Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:49:01.560351Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:49:01.561443Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:49:01.564765Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:49:01.565614Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:49:01.566557Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:49:01.569612Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:49:01.570432Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:49:01.576260Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:49:01.579774Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:49:01.586983Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:49:01.587897Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:49:01.588876Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:49:01.596841Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:49:01.598772Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:49:01.600707Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:49:01.604585Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:49:01.605894Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:49:07.591582Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:49:08.770590Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:49:09.770827Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:49:09.934325Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:49:10.934631Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:49:11.028776Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:49:12.029702Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:49:12.220282Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:49:13.220525Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:49:13.382421Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:49:14.382692Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:49:17.364702","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:49:48.861317","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:49:58.205496Z","level":"error","event":"25/05/12 01:49:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:49:58.720715Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:49:58.744508Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:01.215126Z","level":"error","event":"25/05/12 01:50:01 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:01.217245Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:01.218063Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:01.218803Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:01.219615Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:01.221109Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:01.222719Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:01.223631Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:01.225128Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:01.226996Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:01.228703Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:01.233299Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:01.234832Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:01.235644Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:01.239605Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:01.240642Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:01.241299Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:01.242562Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:01.243475Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:01.244376Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:01.245632Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:01.247039Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:01.247841Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:01.248705Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:02.968660Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:50:03.737579Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:50:04.737742Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:50:04.859715Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:50:05.859792Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:50:06.012855Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:50:07.013060Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:50:07.151555Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:50:08.984237Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:50:10.365207Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:50:11.365413Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:50:13.417030","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:50:44.859520","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:50:48.432439Z","level":"error","event":"25/05/12 01:50:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:48.694859Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:48.706740Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:49.743926Z","level":"error","event":"25/05/12 01:50:49 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:49.744537Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:49.745000Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:49.745414Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:49.745983Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:49.746572Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:49.747166Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:49.747751Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:49.748376Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:49.748904Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:49.749478Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:49.750150Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:49.750700Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:49.751127Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:49.751541Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:49.751916Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:49.752454Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:49.753088Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:49.753923Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:49.754951Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:49.755859Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:49.757130Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:49.757926Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:49.758704Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:50:50.331351Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:50:51.166367Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:50:52.166642Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:50:52.289402Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:50:53.289531Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:50:53.356699Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:50:54.356723Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:50:54.469709Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:50:55.469839Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:50:55.571567Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:50:56.571719Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:50:58.597359","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:51:30.183265","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:51:35.633187Z","level":"error","event":"25/05/12 01:51:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:36.109158Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:36.109971Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:37.702114Z","level":"error","event":"25/05/12 01:51:37 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:37.702825Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:37.703342Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:37.703849Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:37.704358Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:37.705079Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:37.706138Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:37.706957Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:37.707769Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:37.708553Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:37.709267Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:37.709944Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:37.710751Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:37.712418Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:37.713110Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:37.713798Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:37.714468Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:37.715154Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:37.715830Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:37.716711Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:37.717347Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:37.718070Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:37.719000Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:37.720321Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:39.139889Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:51:40.238881Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:51:41.239126Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:51:41.403347Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:51:42.403542Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:51:42.499868Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:51:43.500600Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:51:43.619142Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:51:46.652848Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:51:46.763612Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:51:47.763718Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:51:49.653418","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:51:53.853056","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:51:57.969537Z","level":"error","event":"25/05/12 01:51:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:58.696117Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:51:58.698793Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:00.267937Z","level":"error","event":"25/05/12 01:52:00 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:00.268736Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:00.269452Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:00.270106Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:00.270777Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:00.271492Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:00.272181Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:00.272869Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:00.273595Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:00.274264Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:00.274901Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:00.275527Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:00.276176Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:00.276708Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:00.277179Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:00.277588Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:00.278070Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:00.278735Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:00.279450Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:00.280207Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:00.280950Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:00.281637Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:00.282287Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:00.282978Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:01.676481Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:52:02.614004Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:52:03.614198Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:52:03.759883Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:52:04.760103Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:52:04.882376Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:52:05.882488Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:52:06.060555Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:52:07.060691Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:52:07.183937Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:52:08.184121Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:52:12.479820","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:52:44.330942","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:52:48.097765Z","level":"error","event":"25/05/12 01:52:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:50.702122Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:50.703331Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:51.876092Z","level":"error","event":"25/05/12 01:52:51 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:51.876807Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:51.877484Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:51.878253Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:51.878993Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:51.879842Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:51.880559Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:51.881431Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:51.882162Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:51.882915Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:51.883726Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:51.884715Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:51.885625Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:51.886392Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:51.887136Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:51.887939Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:51.888585Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:51.889346Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:51.889942Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:51.890504Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:51.891188Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:51.893569Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:51.894322Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:51.894977Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:52:52.553125Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:52:53.114967Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:52:54.115273Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:52:54.249661Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:52:55.250110Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:52:55.334839Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:52:56.335187Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:52:56.428870Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:52:57.429099Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:52:57.522762Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:52:58.523108Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:53:01.452172","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:53:29.159574","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:53:29.202153","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"IndentationError","exc_value":"unexpected indent (YFinance.py, line 102)","exc_notes":[],"syntax_error":{"offset":4,"filename":"/opt/airflow/dags/jobs/YFinance.py","line":"    .save()\n","lineno":102,"msg":"unexpected indent"},"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":995,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap_external>","lineno":1133,"name":"get_code"},{"filename":"<frozen importlib._bootstrap_external>","lineno":1063,"name":"source_to_code"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"}]}]}
{"timestamp":"2025-05-12T01:53:34.289874","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:53:39.700832Z","level":"error","event":"25/05/12 01:53:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:53:40.185850Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:53:40.204711Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:53:42.091441Z","level":"error","event":"25/05/12 01:53:42 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:53:42.092288Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:53:42.093094Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:53:42.093895Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:53:42.094692Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:53:42.095374Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:53:42.096061Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:53:42.096809Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:53:42.097956Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:53:42.099691Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:53:42.100780Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:53:42.101822Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:53:42.102825Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:53:42.103768Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:53:42.104423Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:53:42.105081Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:53:42.105687Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:53:42.106213Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:53:42.106774Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:53:42.107391Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:53:42.107993Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:53:42.108478Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:53:42.109106Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:53:42.109738Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:53:43.471719Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:53:44.387121Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:53:45.387314Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:53:45.519685Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:53:46.519874Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:53:46.617940Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:53:47.618074Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:53:47.728195Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:53:48.728463Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:53:48.824230Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:53:49.824370Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:53:51.906189","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:54:26.971799","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:54:33.528998Z","level":"error","event":"25/05/12 01:54:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:54:33.794765Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:54:33.795684Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:54:34.998847Z","level":"error","event":"25/05/12 01:54:34 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:54:34.999483Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:54:35.000276Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:54:35.001040Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:54:35.001856Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:54:35.002644Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:54:35.003477Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:54:35.004274Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:54:35.005073Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:54:35.005824Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:54:35.006387Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:54:35.007007Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:54:35.007605Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:54:35.008274Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:54:35.009098Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:54:35.009735Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:54:35.010286Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:54:35.010777Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:54:35.011457Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:54:35.012127Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:54:35.012959Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:54:35.013582Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:54:35.014235Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:54:35.014911Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:54:35.648054Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:54:36.186459Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:54:37.186596Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:54:37.289878Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:54:38.290011Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:54:38.353151Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:54:39.353234Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:54:39.456017Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:54:40.456211Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:54:40.537988Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:54:41.538195Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:54:43.036141","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:55:14.141166","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:55:17.897419Z","level":"error","event":"25/05/12 01:55:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:18.187394Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:18.200476Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:19.657463Z","level":"error","event":"25/05/12 01:55:19 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:19.658386Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:19.659183Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:19.659943Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:19.660855Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:19.661615Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:19.662360Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:19.663157Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:19.663937Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:19.664761Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:19.665454Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:19.666173Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:19.667987Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:19.669288Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:19.670143Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:19.670981Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:19.671798Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:19.672706Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:19.673615Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:19.674410Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:19.675434Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:19.676131Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:19.676705Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:19.677318Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:20.653217Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:55:21.267077Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:55:22.276109Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:55:22.396651Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:55:23.396916Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:55:23.472785Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:55:24.473044Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:55:24.567501Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:55:25.567631Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:55:25.655621Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:55:26.655845Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:55:28.214808","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T01:55:37.745508","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:55:37.772789","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"IndentationError","exc_value":"unexpected indent (YFinance.py, line 103)","exc_notes":[],"syntax_error":{"offset":4,"filename":"/opt/airflow/dags/jobs/YFinance.py","line":"    .save()\n","lineno":103,"msg":"unexpected indent"},"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":995,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap_external>","lineno":1133,"name":"get_code"},{"filename":"<frozen importlib._bootstrap_external>","lineno":1063,"name":"source_to_code"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"}]}]}
{"timestamp":"2025-05-12T01:55:39.997495","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:55:40.029125","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"SyntaxError","exc_value":"invalid syntax (YFinance.py, line 102)","exc_notes":[],"syntax_error":{"offset":6,"filename":"/opt/airflow/dags/jobs/YFinance.py","line":"    .\n","lineno":102,"msg":"invalid syntax"},"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":995,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap_external>","lineno":1133,"name":"get_code"},{"filename":"<frozen importlib._bootstrap_external>","lineno":1063,"name":"source_to_code"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"}]}]}
{"timestamp":"2025-05-12T01:55:44.268994","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:55:44.301981","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"SyntaxError","exc_value":"unterminated string literal (detected at line 102) (YFinance.py, line 102)","exc_notes":[],"syntax_error":{"offset":11,"filename":"/opt/airflow/dags/jobs/YFinance.py","line":"    .mode(\"","lineno":102,"msg":"unterminated string literal (detected at line 102)"},"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":995,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap_external>","lineno":1133,"name":"get_code"},{"filename":"<frozen importlib._bootstrap_external>","lineno":1063,"name":"source_to_code"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"}]}]}
{"timestamp":"2025-05-12T01:55:46.514145","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:55:53.874577Z","level":"error","event":"25/05/12 01:55:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:54.433322Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:54.475178Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:56.977199Z","level":"error","event":"25/05/12 01:55:56 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:56.978387Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:56.979950Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:56.980899Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:56.981708Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:56.982419Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:56.983311Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:56.983964Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:56.984591Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:56.985367Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:56.986510Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:56.987349Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:56.988668Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:56.989836Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:56.990664Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:56.992256Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:56.993383Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:56.994378Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:56.995424Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:56.996368Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:56.997225Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:56.998160Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:56.999703Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:57.000837Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:55:58.222774Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:55:58.837825Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:55:59.838024Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:55:59.961744Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:56:01.749908Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:56:03.103831Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:56:04.104065Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:56:04.219302Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:56:35.657411","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:56:44.019875Z","level":"error","event":"25/05/12 01:56:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:56:44.337442Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:56:44.348189Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:56:45.541678Z","level":"error","event":"25/05/12 01:56:45 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:56:45.542135Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:56:45.542508Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:56:45.542848Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:56:45.543202Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:56:45.543550Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:56:45.543952Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:56:45.544326Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:56:45.544699Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:56:45.545033Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:56:45.545467Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:56:45.546121Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:56:45.546545Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:56:45.546907Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:56:45.547290Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:56:45.547640Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:56:45.548024Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:56:45.548460Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:56:45.549166Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:56:45.549876Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:56:45.550477Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:56:45.550964Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:56:45.551409Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:56:45.551994Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:56:46.109827Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:56:46.679209Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:56:47.679190Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:56:47.798159Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:56:48.798356Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:56:48.867575Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:56:49.867758Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:56:49.965525Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:57:21.262196","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:57:28.897306Z","level":"error","event":"25/05/12 01:57:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:57:30.767089Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:57:30.792308Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:04.558628","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:58:13.374555Z","level":"error","event":"25/05/12 01:58:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:13.767554Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:13.791787Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:46.216988","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:58:49.997381Z","level":"error","event":"25/05/12 01:58:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:50.284978Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:50.285527Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:51.587726Z","level":"error","event":"25/05/12 01:58:51 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:51.588480Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:51.589106Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:51.589783Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:51.590526Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:51.591156Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:51.591812Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:51.592472Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:51.593090Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:51.593716Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:51.594367Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:51.594929Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:51.595666Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:51.596277Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:51.596757Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:51.597233Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:51.597714Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:51.598320Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:51.599120Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:51.599823Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:51.600462Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:51.601052Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:51.601651Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:58:51.602279Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:22.883932","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:59:28.276761Z","level":"error","event":"25/05/12 01:59:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:43.962126","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T01:59:50.573725Z","level":"error","event":"25/05/12 01:59:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:51.037695Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:51.063313Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:52.554042Z","level":"error","event":"25/05/12 01:59:52 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:52.554954Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:52.555732Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:52.556557Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:52.557283Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:52.557974Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:52.558659Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:52.559336Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:52.560369Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:52.561147Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:52.561749Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:52.562381Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:52.562995Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:52.563569Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:52.564100Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:52.564708Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:52.565435Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:52.566127Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:52.567054Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:52.567813Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:52.568523Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:52.569313Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:52.569898Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:52.570734Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T01:59:53.348731Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:59:54.121524Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:59:55.111169Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:59:55.212824Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:59:56.212991Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:59:56.297186Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:59:57.297404Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:59:57.382855Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:59:58.383100Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:59:58.464049Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T01:59:59.464403Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:00:01.014688","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:00:32.419699","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:00:39.378781Z","level":"error","event":"25/05/12 02:00:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:00:39.949914Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:00:39.952958Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:00:42.694138Z","level":"error","event":"25/05/12 02:00:42 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:00:42.695790Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:00:42.696784Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:00:42.698035Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:00:42.699261Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:00:42.700192Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:00:42.701303Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:00:42.702137Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:00:42.703052Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:00:42.703864Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:00:42.704678Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:00:42.705449Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:00:42.706404Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:00:42.707378Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:00:42.709234Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:00:42.713874Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:00:42.714959Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:00:42.715723Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:00:42.716522Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:00:42.717321Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:00:42.718187Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:00:42.719190Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:00:42.719998Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:00:42.720839Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:00:44.125284Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:00:45.091647Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:00:46.091827Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:00:46.201160Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:00:47.201351Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:00:47.277274Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:00:48.277537Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:00:48.389797Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:00:50.234970Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:00:50.360919Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:00:52.630560Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:00:54.858053","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:01:26.754649","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:01:33.989780Z","level":"error","event":"25/05/12 02:01:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:01:34.326268Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:01:34.326910Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:01:36.219011Z","level":"error","event":"25/05/12 02:01:36 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:01:36.219767Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:01:36.220655Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:01:36.221418Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:01:36.222171Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:01:36.223160Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:01:36.224196Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:01:36.224992Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:01:36.225912Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:01:36.226751Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:01:36.227741Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:01:36.228961Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:01:36.231304Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:01:36.233231Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:01:36.234256Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:01:36.235098Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:01:36.236524Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:01:36.237535Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:01:36.238532Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:01:36.239352Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:01:36.240164Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:01:36.241636Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:01:36.242482Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:01:36.243393Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:01:37.712345Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:01:38.399308Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:01:39.399428Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:01:39.538496Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:01:40.538739Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:01:40.609706Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:01:41.609867Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:01:41.723163Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:01:42.723391Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:01:42.810174Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:01:43.810388Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:01:46.548419","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:02:18.366577","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:02:24.946238Z","level":"error","event":"25/05/12 02:02:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:02:27.992734Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:02:27.993651Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:02:31.406728Z","level":"error","event":"25/05/12 02:02:31 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:02:31.407410Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:02:31.408044Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:02:31.408691Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:02:31.409301Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:02:31.409952Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:02:31.410550Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:02:31.411071Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:02:31.411560Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:02:31.412039Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:02:31.412635Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:02:31.413255Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:02:31.413798Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:02:31.414379Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:02:31.414879Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:02:31.415478Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:02:31.416118Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:02:31.416853Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:02:31.417576Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:02:31.418162Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:02:31.418818Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:02:31.419605Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:02:31.420348Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:02:31.421109Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:02:32.312759Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:02:33.402886Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:02:34.407717Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:02:34.552588Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:02:35.552772Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:02:35.643063Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:02:36.643164Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:02:36.810794Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:02:37.810997Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:02:37.949109Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:02:38.949296Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:02:41.264010","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:03:12.446640","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:03:17.663295Z","level":"error","event":"25/05/12 02:03:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:03:18.139855Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:03:18.145366Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:03:20.340316Z","level":"error","event":"25/05/12 02:03:20 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:03:20.341274Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:03:20.342002Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:03:20.342770Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:03:20.343608Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:03:20.344398Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:03:20.345306Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:03:20.346145Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:03:20.346961Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:03:20.347740Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:03:20.348667Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:03:20.349586Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:03:20.350387Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:03:20.351218Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:03:20.351957Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:03:20.352855Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:03:20.353640Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:03:20.354422Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:03:20.355362Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:03:20.356050Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:03:20.356772Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:03:20.357501Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:03:20.358302Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:03:20.359537Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:03:21.451684Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:03:22.357029Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:03:23.357315Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:03:23.509197Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:03:24.512976Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:03:24.613160Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:03:25.613405Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:03:25.725652Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:03:26.725784Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:03:26.844160Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:03:27.844330Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:03:32.257189","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:04:04.453180","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:04:09.783211Z","level":"error","event":"25/05/12 02:04:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:10.131823Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:10.132547Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:11.871109Z","level":"error","event":"25/05/12 02:04:11 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:11.871870Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:11.872716Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:11.873394Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:11.874016Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:11.874642Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:11.875479Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:11.876193Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:11.876834Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:11.877523Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:11.878135Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:11.878647Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:11.879669Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:11.880206Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:11.880680Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:11.881122Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:11.881645Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:11.882140Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:11.882857Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:11.883398Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:11.883788Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:11.884169Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:11.884493Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:11.884842Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:12.727117Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:04:13.975063Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:04:14.975301Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:04:15.090324Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:04:16.090532Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:04:16.186280Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:04:17.186836Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:04:17.286643Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:04:18.287222Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:04:18.412512Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:04:19.412766Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:04:21.544721","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:04:53.393277","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:04:57.750293Z","level":"error","event":"25/05/12 02:04:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:58.080335Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:58.081002Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:59.444354Z","level":"error","event":"25/05/12 02:04:59 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:59.445143Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:59.445737Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:59.446369Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:59.447019Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:59.447674Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:59.448303Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:59.448975Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:59.449653Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:59.450446Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:59.451216Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:59.451861Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:59.452396Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:59.453030Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:59.453622Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:59.454093Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:59.454625Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:59.455312Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:59.455928Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:59.456602Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:59.457444Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:59.458445Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:59.459495Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:04:59.460399Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:00.152947Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:05:00.962380Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:05:01.962771Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:05:02.265009Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:05:03.265643Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:05:03.430157Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:05:04.430300Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:05:04.565180Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:05:05.565407Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:05:05.780795Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:05:08.877696Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:05:11.401166","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:05:43.109465","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:05:47.478146Z","level":"error","event":"25/05/12 02:05:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:47.902965Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:47.903614Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:49.352917Z","level":"error","event":"25/05/12 02:05:49 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:49.353621Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:49.354363Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:49.355539Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:49.356355Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:49.357380Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:49.358193Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:49.359039Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:49.360129Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:49.361096Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:49.362064Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:49.362898Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:49.363649Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:49.364291Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:49.364874Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:49.365387Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:49.365850Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:49.366333Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:49.366772Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:49.367220Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:49.367639Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:49.368025Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:49.368430Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:49.369005Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:05:49.993851Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:05:50.705665Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:05:51.705861Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:05:51.810589Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:05:52.810880Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:05:52.953948Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:05:53.954144Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:05:54.081905Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:05:55.082123Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:05:55.168328Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:05:56.168615Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:05:58.448889","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:06:29.909602","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:06:34.507338Z","level":"error","event":"25/05/12 02:06:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:06:34.852556Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:06:34.853386Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:06:38.391377Z","level":"error","event":"25/05/12 02:06:38 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:06:38.399151Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:06:38.400918Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:06:38.403477Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:06:38.404442Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:06:38.405240Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:06:38.406056Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:06:38.406904Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:06:38.408830Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:06:38.420842Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:06:38.423745Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:06:38.439108Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:06:38.442337Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:06:38.443361Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:06:38.447459Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:06:38.448505Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:06:38.449353Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:06:38.450925Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:06:38.453905Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:06:38.455670Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:06:38.485526Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:06:38.540117Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:06:38.550580Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:06:38.552859Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:06:39.526520Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:06:40.275446Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:06:41.275623Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:06:41.442055Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:06:43.224117Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:06:43.325036Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:06:45.601358Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:06:45.717161Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:06:46.718034Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:06:46.813223Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:06:47.813114Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:06:49.175598","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:07:20.837803","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:07:27.224205Z","level":"error","event":"25/05/12 02:07:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:07:27.703360Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:07:27.704390Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:07:31.093966Z","level":"error","event":"25/05/12 02:07:31 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:07:31.094911Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:07:31.095995Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:07:31.096925Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:07:31.097854Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:07:31.099795Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:07:31.100732Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:07:31.101563Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:07:31.102747Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:07:31.103782Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:07:31.104612Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:07:31.105426Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:07:31.106469Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:07:31.107187Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:07:31.107884Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:07:31.108685Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:07:31.109761Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:07:31.111195Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:07:31.112122Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:07:31.113035Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:07:31.113877Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:07:31.114720Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:07:31.115607Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:07:31.116496Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:07:32.227116Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:07:32.897361Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:07:33.897516Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:07:34.018821Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:07:35.018964Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:07:35.102485Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:07:36.102592Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:07:36.200009Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:07:37.200167Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:07:37.284736Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:07:38.284924Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:08:09.957370","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:08:14.147805Z","level":"error","event":"25/05/12 02:08:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:08:14.400165Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:08:14.400703Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:08:15.861623Z","level":"error","event":"25/05/12 02:08:15 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:08:15.862318Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:08:15.863212Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:08:15.863876Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:08:15.864537Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:08:15.865235Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:08:15.865865Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:08:15.866478Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:08:15.866985Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:08:15.867459Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:08:15.867984Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:08:15.868483Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:08:15.869092Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:08:15.869626Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:08:15.870123Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:08:15.870547Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:08:15.871057Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:08:15.871563Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:08:15.872057Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:08:15.872569Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:08:15.873332Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:08:15.874004Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:08:15.874607Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:08:15.875225Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:08:16.613736Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:08:17.558262Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:08:18.558521Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:08:19.523498Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:08:21.795608Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:08:21.882461Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:08:22.882659Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:08:22.997551Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:08:23.997778Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:08:24.101066Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:08:25.101308Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:08:56.296167","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:08:59.873040Z","level":"error","event":"25/05/12 02:08:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:00.107526Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:00.120466Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:01.463617Z","level":"error","event":"25/05/12 02:09:01 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:01.464442Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:01.465073Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:01.465712Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:01.466347Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:01.466799Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:01.467283Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:01.467695Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:01.468182Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:01.468735Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:01.469151Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:01.469569Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:01.470064Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:01.470717Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:01.471316Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:01.471804Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:01.472198Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:01.472596Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:01.473060Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:01.473462Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:01.473909Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:01.474379Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:01.474808Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:01.475395Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:02.149517Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:09:02.807595Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:09:03.807976Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:09:03.911512Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:09:04.911590Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:09:04.989733Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:09:05.989850Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:09:06.096584Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:09:07.096717Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:09:07.186195Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:09:38.953393","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:09:42.505709Z","level":"error","event":"25/05/12 02:09:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:42.757861Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:42.771614Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:43.810877Z","level":"error","event":"25/05/12 02:09:43 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:43.811492Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:43.812144Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:43.812660Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:43.813194Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:43.813665Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:43.814275Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:43.814922Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:43.815463Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:43.816052Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:43.816637Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:43.817259Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:43.817760Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:43.818202Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:43.818653Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:43.819086Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:43.819522Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:43.820021Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:43.820516Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:43.821031Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:43.821536Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:43.821983Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:43.822418Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:43.822965Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:09:44.337378Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:09:44.889660Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:09:45.917214Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:09:46.001303Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:09:47.001467Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:09:47.076506Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:09:48.076767Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:09:48.179246Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:10:20.525790","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:10:26.753695Z","level":"error","event":"25/05/12 02:10:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:10:29.415816Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:10:29.459722Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:00.044189","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:11:23.838393","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:11:31.098350Z","level":"error","event":"25/05/12 02:11:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:33.634593Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:33.635496Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:35.515677Z","level":"error","event":"25/05/12 02:11:35 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:35.516722Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:35.517631Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:35.518379Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:35.519126Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:35.520052Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:35.520891Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:35.522137Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:35.523432Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:35.524354Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:35.525126Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:35.526032Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:35.526896Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:35.527917Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:35.528748Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:35.529779Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:35.530560Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:35.531497Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:35.532371Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:35.533254Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:35.534266Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:35.535385Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:35.536396Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:35.537259Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:11:36.669172Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:11:37.467142Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:11:38.467326Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:11:38.582277Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:11:39.582433Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:11:39.666661Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:11:40.666912Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:11:40.854941Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:11:41.855388Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:11:42.052373Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:11:43.052499Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:11:45.434487","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:12:16.873279","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:12:24.822408Z","level":"error","event":"25/05/12 02:12:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:12:25.455406Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:12:25.460161Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:12:28.377644Z","level":"error","event":"25/05/12 02:12:28 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:12:28.382642Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:12:28.393268Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:12:28.398982Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:12:28.409747Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:12:28.414337Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:12:28.437029Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:12:28.454950Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:12:28.469527Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:12:28.479887Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:12:28.482808Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:12:28.484213Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:12:28.487046Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:12:28.493407Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:12:28.495568Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:12:28.496622Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:12:28.505044Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:12:28.513607Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:12:28.518407Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:12:28.524369Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:12:28.528943Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:12:28.530076Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:12:28.531146Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:12:28.533229Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:12:31.412668Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:12:32.281100Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:12:33.307704Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:12:33.507323Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:12:34.508078Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:12:34.679625Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:12:37.842192Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:12:38.052552Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:12:39.053153Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:12:39.160944Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:12:40.161180Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:12:43.272764","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:13:15.457052","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:13:21.806323Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:21.808777Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:22.213427Z","level":"error","event":"25/05/12 02:13:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:26.151515Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:13:27.074014Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:13:28.074122Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:13:28.177449Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:13:29.177688Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:13:29.268237Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:13:30.268787Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:13:30.368654Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:13:31.368877Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:13:31.451414Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:13:32.451554Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:13:34.054094","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o31.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":60,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:13:43.849793","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:13:49.798792Z","level":"error","event":"25/05/12 02:13:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:50.197925Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:50.214319Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:51.842752Z","level":"error","event":"25/05/12 02:13:51 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:51.843475Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:51.844316Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:51.845077Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:51.845676Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:51.846305Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:51.847896Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:51.852220Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:51.853919Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:51.855041Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:51.855734Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:51.856457Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:51.857062Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:51.857694Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:51.858227Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:51.858784Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:51.859358Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:51.859942Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:51.860577Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:51.861242Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:51.861932Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:51.862627Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:51.863358Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:51.864472Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:13:52.701927Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:13:53.469760Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:13:54.469909Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:13:54.568951Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:13:55.569101Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:13:55.636655Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:13:56.637127Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:13:56.733480Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:13:57.733649Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:13:57.821629Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:13:58.821759Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:14:01.156699","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:14:33.116548","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:14:39.984933Z","level":"error","event":"25/05/12 02:14:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:14:40.552990Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:14:40.554052Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:14:42.826757Z","level":"error","event":"25/05/12 02:14:42 ERROR SparkContext: Failed to add /opt/airflow/jars/* to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:14:42.827679Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/* not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:14:42.828502Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:14:42.829290Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:14:42.830184Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:14:42.831123Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:14:42.831871Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:14:42.833648Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:14:42.834805Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:14:42.835627Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:14:42.836659Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:14:42.837523Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:14:42.838349Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:14:42.839421Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:14:42.840465Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:14:42.841886Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:14:42.842813Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:14:42.844011Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:14:42.845113Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:14:42.845940Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:14:42.846972Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:14:42.848124Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:14:42.848906Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:14:42.849756Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:14:46.178505Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:14:46.854147Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:14:47.854281Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:14:47.981450Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:14:48.981903Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:14:49.095740Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:14:50.094422Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:14:50.202682Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:14:51.202804Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:14:51.340872Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:14:52.340968Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:14:53.975764","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:15:02.658195","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:15:09.136033Z","level":"error","event":"25/05/12 02:15:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:09.869351Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:09.869983Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.810814Z","level":"error","event":"25/05/12 02:15:12 ERROR SparkContext: Failed to add /opt/airflow/ to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.811844Z","level":"error","event":"java.lang.IllegalArgumentException: Directory /opt/airflow is not allowed for addJar","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.812670Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2099)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.813819Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.816106Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.818305Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.819144Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.820037Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.820831Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.821575Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.822246Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.823092Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.823922Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.824735Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.825710Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.826662Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.827390Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.828088Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.828793Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.829931Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.830667Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.831616Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.833055Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.834390Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.838326Z","level":"error","event":"25/05/12 02:15:12 ERROR SparkContext: Failed to add /opt/airflow/ to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.839209Z","level":"error","event":"java.lang.IllegalArgumentException: Directory /opt/airflow is not allowed for addJar","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.840142Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2099)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.840893Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.841765Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.842614Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.843300Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.844436Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.845184Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.846057Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.846658Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.849270Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.850289Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.851040Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.851798Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.852528Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.854058Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.855186Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.855971Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.856726Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.857651Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.859925Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.860653Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:12.861327Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:13.972915Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:15:14.608551Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:15:15.608597Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:15:15.741815Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:15:18.879936Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:15:18.986186Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:15:19.986542Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:15:20.132152Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:15:21.106107Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:15:21.226533Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:15:22.226679Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:15:24.905134","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:15:30.092025","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:15:34.384248Z","level":"error","event":"25/05/12 02:15:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:34.670072Z","level":"error","event":"25/05/12 02:15:34 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:34.682427Z","level":"error","event":"25/05/12 02:15:34 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:34.682885Z","level":"error","event":"25/05/12 02:15:34 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:34.683273Z","level":"error","event":"25/05/12 02:15:34 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:34.876303Z","level":"error","event":"25/05/12 02:15:34 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:34.889785Z","level":"error","event":"25/05/12 02:15:34 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:34.890344Z","level":"error","event":"25/05/12 02:15:34 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:34.919286Z","level":"error","event":"25/05/12 02:15:34 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:34.931678Z","level":"error","event":"25/05/12 02:15:34 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:34.932594Z","level":"error","event":"25/05/12 02:15:34 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:34.933133Z","level":"error","event":"25/05/12 02:15:34 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:34.949319Z","level":"error","event":"25/05/12 02:15:34 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:34.968059Z","level":"error","event":"25/05/12 02:15:34 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:34.982532Z","level":"error","event":"25/05/12 02:15:34 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.052364Z","level":"error","event":"25/05/12 02:15:35 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.073056Z","level":"error","event":"25/05/12 02:15:35 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.073765Z","level":"error","event":"25/05/12 02:15:35 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.074355Z","level":"error","event":"25/05/12 02:15:35 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.075190Z","level":"error","event":"25/05/12 02:15:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.332073Z","level":"error","event":"25/05/12 02:15:35 INFO Utils: Successfully started service 'sparkDriver' on port 46407.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.372132Z","level":"error","event":"25/05/12 02:15:35 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.421950Z","level":"error","event":"25/05/12 02:15:35 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.446855Z","level":"error","event":"25/05/12 02:15:35 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.461487Z","level":"error","event":"25/05/12 02:15:35 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.462122Z","level":"error","event":"25/05/12 02:15:35 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.486200Z","level":"error","event":"25/05/12 02:15:35 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9749045e-f6a3-4a37-9773-61b3c19aa90b","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.508487Z","level":"error","event":"25/05/12 02:15:35 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.529643Z","level":"error","event":"25/05/12 02:15:35 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.706961Z","level":"error","event":"25/05/12 02:15:35 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.811638Z","level":"error","event":"25/05/12 02:15:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.883421Z","level":"error","event":"25/05/12 02:15:35 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.884108Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.884776Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.885288Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.885683Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.886185Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.886840Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.887337Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.887819Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.888394Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.888949Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.889501Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.890056Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.890572Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.891317Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.891971Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.892612Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.893334Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.893982Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.894735Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.895378Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.896066Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.896758Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.897402Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.914622Z","level":"error","event":"25/05/12 02:15:35 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.915419Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.916195Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.916999Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.917618Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.918190Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.918747Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.919389Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.920008Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.920681Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.921436Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.922153Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.923843Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.924743Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.925708Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.926784Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.927781Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.929138Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.930156Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.931304Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.932443Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.933370Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.934562Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.935260Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.936169Z","level":"error","event":"25/05/12 02:15:35 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.937023Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.938150Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.939332Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.940032Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.940633Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.941358Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.942103Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.943349Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.944238Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.945234Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.946248Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.947277Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.948370Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.949160Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.949803Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.950483Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.951384Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.952353Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.953169Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.953913Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.954677Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.955735Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.956337Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.956892Z","level":"error","event":"25/05/12 02:15:35 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.957372Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.957922Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.971523Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.972107Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.972654Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.973205Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.973699Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.974172Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.974621Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.975047Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.975558Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.976072Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.976651Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.977210Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.977760Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.978451Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.979102Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.979877Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.980433Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.981008Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.981554Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.982040Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:35.982458Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:36.008689Z","level":"error","event":"25/05/12 02:15:36 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:36.027121Z","level":"error","event":"25/05/12 02:15:36 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:36.027910Z","level":"error","event":"25/05/12 02:15:36 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:36.047303Z","level":"error","event":"25/05/12 02:15:36 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:36.047971Z","level":"error","event":"25/05/12 02:15:36 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6ff21454 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:36.088797Z","level":"error","event":"25/05/12 02:15:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44707.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:36.104910Z","level":"error","event":"25/05/12 02:15:36 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:44707","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:36.105599Z","level":"error","event":"25/05/12 02:15:36 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:36.106260Z","level":"error","event":"25/05/12 02:15:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 44707, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:36.120561Z","level":"error","event":"25/05/12 02:15:36 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:44707 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 44707, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:36.121790Z","level":"error","event":"25/05/12 02:15:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 44707, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:36.122465Z","level":"error","event":"25/05/12 02:15:36 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 44707, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:36.719406Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:15:37.456013Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:15:38.456434Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:15:38.606093Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:15:39.617500Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:15:39.767901Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:15:40.768077Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:15:40.874155Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:15:41.874356Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:15:41.954357Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:15:42.954463Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:15:42.968650Z","level":"error","event":"25/05/12 02:15:42 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:42.981846Z","level":"error","event":"25/05/12 02:15:42 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:45.443567","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":61,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:15:45.958612Z","level":"error","event":"25/05/12 02:15:45 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:45.959222Z","level":"error","event":"25/05/12 02:15:45 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:45.974656Z","level":"error","event":"25/05/12 02:15:45 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:45.997414Z","level":"error","event":"25/05/12 02:15:45 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:46.022499Z","level":"error","event":"25/05/12 02:15:46 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:46.038674Z","level":"error","event":"25/05/12 02:15:46 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:46.040180Z","level":"error","event":"25/05/12 02:15:46 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:46.055954Z","level":"error","event":"25/05/12 02:15:46 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:46.072207Z","level":"error","event":"25/05/12 02:15:46 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:46.072778Z","level":"error","event":"25/05/12 02:15:46 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:46.073286Z","level":"error","event":"25/05/12 02:15:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-175223c7-7177-4be9-b09f-ad57812f637c","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:46.073835Z","level":"error","event":"25/05/12 02:15:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-8b32f39b-aa0d-45e5-9156-1b506da067a4","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:46.087471Z","level":"error","event":"25/05/12 02:15:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-8b32f39b-aa0d-45e5-9156-1b506da067a4/pyspark-c52f9e90-604c-4475-89ec-71591090310b","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:46.649643","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:15:54.861328Z","level":"error","event":"25/05/12 02:15:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:55.262979Z","level":"error","event":"25/05/12 02:15:55 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:55.281823Z","level":"error","event":"25/05/12 02:15:55 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:55.282654Z","level":"error","event":"25/05/12 02:15:55 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:55.283397Z","level":"error","event":"25/05/12 02:15:55 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:55.545249Z","level":"error","event":"25/05/12 02:15:55 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:55.562865Z","level":"error","event":"25/05/12 02:15:55 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:55.563601Z","level":"error","event":"25/05/12 02:15:55 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:55.619554Z","level":"error","event":"25/05/12 02:15:55 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:55.632715Z","level":"error","event":"25/05/12 02:15:55 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:55.633516Z","level":"error","event":"25/05/12 02:15:55 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:55.634479Z","level":"error","event":"25/05/12 02:15:55 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:55.662411Z","level":"error","event":"25/05/12 02:15:55 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:55.681451Z","level":"error","event":"25/05/12 02:15:55 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:55.694961Z","level":"error","event":"25/05/12 02:15:55 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:55.775409Z","level":"error","event":"25/05/12 02:15:55 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:55.800464Z","level":"error","event":"25/05/12 02:15:55 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:55.801364Z","level":"error","event":"25/05/12 02:15:55 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:55.801900Z","level":"error","event":"25/05/12 02:15:55 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:55.802663Z","level":"error","event":"25/05/12 02:15:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:56.483484Z","level":"error","event":"25/05/12 02:15:56 INFO Utils: Successfully started service 'sparkDriver' on port 37971.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:56.622260Z","level":"error","event":"25/05/12 02:15:56 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:56.736415Z","level":"error","event":"25/05/12 02:15:56 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:56.773092Z","level":"error","event":"25/05/12 02:15:56 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:56.791028Z","level":"error","event":"25/05/12 02:15:56 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:56.791496Z","level":"error","event":"25/05/12 02:15:56 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:56.824454Z","level":"error","event":"25/05/12 02:15:56 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-640d2ca9-055f-4db5-9acf-7f98e36e9851","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:56.850580Z","level":"error","event":"25/05/12 02:15:56 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:56.890563Z","level":"error","event":"25/05/12 02:15:56 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.147525Z","level":"error","event":"25/05/12 02:15:57 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.279022Z","level":"error","event":"25/05/12 02:15:57 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.360656Z","level":"error","event":"25/05/12 02:15:57 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.361474Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.362127Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.362890Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.363601Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.364268Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.364953Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.365620Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.366184Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.367208Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.368374Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.369205Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.370057Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.370780Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.371637Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.372386Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.373226Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.374134Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.374962Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.376319Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.377269Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.378191Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.379155Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.380590Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.412526Z","level":"error","event":"25/05/12 02:15:57 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.416329Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.417307Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.420153Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.421144Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.422096Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.430859Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.435272Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.436817Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.437864Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.439306Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.443726Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.445202Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.448843Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.450275Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.451400Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.452274Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.453700Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.454874Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.455700Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.456629Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.457763Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.458585Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.459533Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.461298Z","level":"error","event":"25/05/12 02:15:57 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.462321Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.470070Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.471107Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.472923Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.474152Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.475325Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.476282Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.477177Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.478249Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.480839Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.484115Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.485150Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.486593Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.488703Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.489708Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.490649Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.491474Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.492881Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.493837Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.494840Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.495727Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.496585Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.497387Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.498281Z","level":"error","event":"25/05/12 02:15:57 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.499137Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.500079Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.520802Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.521602Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.522341Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.523136Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.523803Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.524524Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.525315Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.526147Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.526950Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.528329Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.529464Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.530998Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.532046Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.533572Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.534619Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.535729Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.536620Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.537299Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.538179Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.539874Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.540923Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.590215Z","level":"error","event":"25/05/12 02:15:57 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.610696Z","level":"error","event":"25/05/12 02:15:57 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.611605Z","level":"error","event":"25/05/12 02:15:57 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.628842Z","level":"error","event":"25/05/12 02:15:57 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.629632Z","level":"error","event":"25/05/12 02:15:57 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6ff21454 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.673867Z","level":"error","event":"25/05/12 02:15:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44651.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.687942Z","level":"error","event":"25/05/12 02:15:57 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:44651","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.689673Z","level":"error","event":"25/05/12 02:15:57 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.690674Z","level":"error","event":"25/05/12 02:15:57 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 44651, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.711695Z","level":"error","event":"25/05/12 02:15:57 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:44651 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 44651, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.712409Z","level":"error","event":"25/05/12 02:15:57 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 44651, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:57.713194Z","level":"error","event":"25/05/12 02:15:57 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 44651, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:15:58.296040Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:15:59.099296Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:16:00.099476Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:16:00.206721Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:16:01.206957Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:16:01.295248Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:16:02.295476Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:16:02.419347Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:16:03.419506Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:16:03.515025Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:16:04.515179Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:16:04.528684Z","level":"error","event":"25/05/12 02:16:04 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:04.541408Z","level":"error","event":"25/05/12 02:16:04 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:07.074600","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":62,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:16:07.707515Z","level":"error","event":"25/05/12 02:16:07 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:07.708961Z","level":"error","event":"25/05/12 02:16:07 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:07.746682Z","level":"error","event":"25/05/12 02:16:07 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:07.803511Z","level":"error","event":"25/05/12 02:16:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:07.865725Z","level":"error","event":"25/05/12 02:16:07 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:07.917188Z","level":"error","event":"25/05/12 02:16:07 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:07.919036Z","level":"error","event":"25/05/12 02:16:07 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:07.963336Z","level":"error","event":"25/05/12 02:16:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:07.994005Z","level":"error","event":"25/05/12 02:16:07 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:07.999885Z","level":"error","event":"25/05/12 02:16:07 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:08.009111Z","level":"error","event":"25/05/12 02:16:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-5660ff95-1cd3-4c61-8726-6e4876771a83/pyspark-672f2d0b-bce8-46d5-a348-fb8b01caabf0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:08.046893Z","level":"error","event":"25/05/12 02:16:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-5660ff95-1cd3-4c61-8726-6e4876771a83","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:08.047981Z","level":"error","event":"25/05/12 02:16:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-2aa88876-fb63-4b1d-9a98-1e67401a53ef","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:38.721215","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:16:45.896175Z","level":"error","event":"25/05/12 02:16:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:46.282143Z","level":"error","event":"25/05/12 02:16:46 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:46.301502Z","level":"error","event":"25/05/12 02:16:46 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:46.302188Z","level":"error","event":"25/05/12 02:16:46 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:46.302859Z","level":"error","event":"25/05/12 02:16:46 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:46.492916Z","level":"error","event":"25/05/12 02:16:46 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:46.509794Z","level":"error","event":"25/05/12 02:16:46 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:46.510737Z","level":"error","event":"25/05/12 02:16:46 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:46.561804Z","level":"error","event":"25/05/12 02:16:46 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:46.574398Z","level":"error","event":"25/05/12 02:16:46 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:46.575131Z","level":"error","event":"25/05/12 02:16:46 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:46.575656Z","level":"error","event":"25/05/12 02:16:46 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:46.609341Z","level":"error","event":"25/05/12 02:16:46 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:46.625647Z","level":"error","event":"25/05/12 02:16:46 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:46.639078Z","level":"error","event":"25/05/12 02:16:46 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:46.711101Z","level":"error","event":"25/05/12 02:16:46 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:46.725657Z","level":"error","event":"25/05/12 02:16:46 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:46.726286Z","level":"error","event":"25/05/12 02:16:46 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:46.727016Z","level":"error","event":"25/05/12 02:16:46 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:46.727780Z","level":"error","event":"25/05/12 02:16:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:47.105521Z","level":"error","event":"25/05/12 02:16:47 INFO Utils: Successfully started service 'sparkDriver' on port 38733.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:47.172966Z","level":"error","event":"25/05/12 02:16:47 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:47.249507Z","level":"error","event":"25/05/12 02:16:47 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:47.282330Z","level":"error","event":"25/05/12 02:16:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:47.298799Z","level":"error","event":"25/05/12 02:16:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:47.299423Z","level":"error","event":"25/05/12 02:16:47 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:47.334312Z","level":"error","event":"25/05/12 02:16:47 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b20fd210-89d3-4a3b-b6b3-60b66505d140","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:47.366050Z","level":"error","event":"25/05/12 02:16:47 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:47.402661Z","level":"error","event":"25/05/12 02:16:47 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:47.624781Z","level":"error","event":"25/05/12 02:16:47 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:47.822641Z","level":"error","event":"25/05/12 02:16:47 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:47.988113Z","level":"error","event":"25/05/12 02:16:47 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:47.990407Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:47.994214Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:47.999952Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.004067Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.013361Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.015133Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.023427Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.031949Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.048368Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.058688Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.076447Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.088421Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.094636Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.100443Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.115543Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.148518Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.159490Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.162068Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.186335Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.199593Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.206656Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.210237Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.213476Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.301931Z","level":"error","event":"25/05/12 02:16:47 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.303323Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.304290Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.305221Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.306240Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.307221Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.308692Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.309623Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.310703Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.311920Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.313020Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.317797Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.318797Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.319674Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.320488Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.321441Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.322293Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.323345Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.324303Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.325180Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.325960Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.326682Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.327291Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.327959Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.328704Z","level":"error","event":"25/05/12 02:16:48 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.329416Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.330088Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.330826Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.331613Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.332798Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.335095Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.336119Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.338162Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.342829Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.343769Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.344933Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.346283Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.348460Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.350252Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.354147Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.357745Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.359723Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.363323Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.365497Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.366896Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.376006Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.376831Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.377484Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.381308Z","level":"error","event":"25/05/12 02:16:48 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.382724Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.389246Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.419190Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.420382Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.421835Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.422689Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.425033Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.426030Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.426969Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.428998Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.430254Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.431304Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.435439Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.437403Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.441591Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.442639Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.443573Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.444427Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.445480Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.446266Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.447601Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.451790Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.452985Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.453820Z","level":"error","event":"25/05/12 02:16:48 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.454848Z","level":"error","event":"25/05/12 02:16:48 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.456304Z","level":"error","event":"25/05/12 02:16:48 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.457400Z","level":"error","event":"25/05/12 02:16:48 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.458334Z","level":"error","event":"25/05/12 02:16:48 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1f04a8db for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.504619Z","level":"error","event":"25/05/12 02:16:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37713.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.520803Z","level":"error","event":"25/05/12 02:16:48 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:37713","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.521497Z","level":"error","event":"25/05/12 02:16:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.541174Z","level":"error","event":"25/05/12 02:16:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 37713, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.544241Z","level":"error","event":"25/05/12 02:16:48 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:37713 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 37713, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.545152Z","level":"error","event":"25/05/12 02:16:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 37713, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:48.566711Z","level":"error","event":"25/05/12 02:16:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 37713, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:49.285707Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:16:49.971710Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:16:50.972233Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:16:51.087024Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:16:52.087253Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:16:52.157910Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:16:55.231105Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:16:55.352202Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:16:56.360077Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:16:56.468637Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:16:57.468763Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:16:57.478586Z","level":"error","event":"25/05/12 02:16:57 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:57.489754Z","level":"error","event":"25/05/12 02:16:57 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:59.168396","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:16:59.679798Z","level":"error","event":"25/05/12 02:16:59 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:59.680407Z","level":"error","event":"25/05/12 02:16:59 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:59.694588Z","level":"error","event":"25/05/12 02:16:59 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:59.712897Z","level":"error","event":"25/05/12 02:16:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:59.730242Z","level":"error","event":"25/05/12 02:16:59 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:59.742145Z","level":"error","event":"25/05/12 02:16:59 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:59.742881Z","level":"error","event":"25/05/12 02:16:59 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:59.743536Z","level":"error","event":"25/05/12 02:16:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:59.758343Z","level":"error","event":"25/05/12 02:16:59 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:59.771627Z","level":"error","event":"25/05/12 02:16:59 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:59.772206Z","level":"error","event":"25/05/12 02:16:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-89c7019f-38c4-483a-bcf5-6db2290abc33","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:59.772738Z","level":"error","event":"25/05/12 02:16:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-5c908adc-7d41-4a4f-8ce2-0b5574dc5d0f","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:16:59.784145Z","level":"error","event":"25/05/12 02:16:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-5c908adc-7d41-4a4f-8ce2-0b5574dc5d0f/pyspark-98c18b6c-152d-4767-a95a-8342c947a16f","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:30.376439","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:17:35.664636Z","level":"error","event":"25/05/12 02:17:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:35.951397Z","level":"error","event":"25/05/12 02:17:35 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:35.965656Z","level":"error","event":"25/05/12 02:17:35 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:35.966457Z","level":"error","event":"25/05/12 02:17:35 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:35.967254Z","level":"error","event":"25/05/12 02:17:35 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:36.164088Z","level":"error","event":"25/05/12 02:17:36 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:36.178250Z","level":"error","event":"25/05/12 02:17:36 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:36.178907Z","level":"error","event":"25/05/12 02:17:36 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:36.221746Z","level":"error","event":"25/05/12 02:17:36 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:36.233456Z","level":"error","event":"25/05/12 02:17:36 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:36.234001Z","level":"error","event":"25/05/12 02:17:36 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:36.234591Z","level":"error","event":"25/05/12 02:17:36 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:36.260734Z","level":"error","event":"25/05/12 02:17:36 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:36.275222Z","level":"error","event":"25/05/12 02:17:36 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:36.290102Z","level":"error","event":"25/05/12 02:17:36 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:36.354646Z","level":"error","event":"25/05/12 02:17:36 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:36.368319Z","level":"error","event":"25/05/12 02:17:36 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:36.368814Z","level":"error","event":"25/05/12 02:17:36 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:36.369246Z","level":"error","event":"25/05/12 02:17:36 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:36.369872Z","level":"error","event":"25/05/12 02:17:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:36.822088Z","level":"error","event":"25/05/12 02:17:36 INFO Utils: Successfully started service 'sparkDriver' on port 42145.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:36.920310Z","level":"error","event":"25/05/12 02:17:36 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.037619Z","level":"error","event":"25/05/12 02:17:37 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.080457Z","level":"error","event":"25/05/12 02:17:37 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.094755Z","level":"error","event":"25/05/12 02:17:37 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.095432Z","level":"error","event":"25/05/12 02:17:37 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.129770Z","level":"error","event":"25/05/12 02:17:37 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d2039473-e193-4b57-9d3d-42fd5c5a99cd","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.178324Z","level":"error","event":"25/05/12 02:17:37 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.215500Z","level":"error","event":"25/05/12 02:17:37 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.455748Z","level":"error","event":"25/05/12 02:17:37 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.564797Z","level":"error","event":"25/05/12 02:17:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.628054Z","level":"error","event":"25/05/12 02:17:37 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.628611Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.629109Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.629626Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.630355Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.630905Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.631362Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.631866Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.632465Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.632922Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.633436Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.633986Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.634498Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.635083Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.635541Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.635984Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.636438Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.636894Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.637337Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.637818Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.638290Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.638765Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.639215Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.639739Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.660506Z","level":"error","event":"25/05/12 02:17:37 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.662279Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.663126Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.663789Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.666697Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.667686Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.668507Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.670331Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.671672Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.672819Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.673577Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.674784Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.676059Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.677531Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.678377Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.679046Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.680029Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.680927Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.681861Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.683038Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.686686Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.688054Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.688946Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.689680Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.690490Z","level":"error","event":"25/05/12 02:17:37 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.691274Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.691891Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.692566Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.693480Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.694279Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.695020Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.695844Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.696490Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.697256Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.698129Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.700213Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.701019Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.701808Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.702464Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.703296Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.703951Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.704662Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.705341Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.706047Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.706620Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.707194Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.708086Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.708720Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.709276Z","level":"error","event":"25/05/12 02:17:37 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.709793Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.710369Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.727332Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.727945Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.728490Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.729143Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.729845Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.730548Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.731110Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.731638Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.732317Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.733016Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.733860Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.734539Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.735641Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.736406Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.736983Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.737644Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.738353Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.739059Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.739637Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.740164Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.740653Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.762854Z","level":"error","event":"25/05/12 02:17:37 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.775081Z","level":"error","event":"25/05/12 02:17:37 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.775661Z","level":"error","event":"25/05/12 02:17:37 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.776255Z","level":"error","event":"25/05/12 02:17:37 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.788100Z","level":"error","event":"25/05/12 02:17:37 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@244a7a7d for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.810910Z","level":"error","event":"25/05/12 02:17:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36245.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.831564Z","level":"error","event":"25/05/12 02:17:37 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:36245","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.832301Z","level":"error","event":"25/05/12 02:17:37 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.847991Z","level":"error","event":"25/05/12 02:17:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 36245, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.848988Z","level":"error","event":"25/05/12 02:17:37 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:36245 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 36245, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.849753Z","level":"error","event":"25/05/12 02:17:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 36245, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:37.871268Z","level":"error","event":"25/05/12 02:17:37 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 36245, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:38.671716Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:17:39.254625Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:17:40.254849Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:17:40.361720Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:17:41.361993Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:17:41.451681Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:17:42.451862Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:17:42.567430Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:17:43.567646Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:17:43.652406Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:17:44.652590Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:17:44.668086Z","level":"error","event":"25/05/12 02:17:44 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:44.684230Z","level":"error","event":"25/05/12 02:17:44 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:47.010292","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:17:47.390078Z","level":"error","event":"25/05/12 02:17:47 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:47.395378Z","level":"error","event":"25/05/12 02:17:47 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:47.464449Z","level":"error","event":"25/05/12 02:17:47 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:47.484648Z","level":"error","event":"25/05/12 02:17:47 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:47.512806Z","level":"error","event":"25/05/12 02:17:47 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:47.513589Z","level":"error","event":"25/05/12 02:17:47 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:47.528076Z","level":"error","event":"25/05/12 02:17:47 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:47.542723Z","level":"error","event":"25/05/12 02:17:47 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:47.557416Z","level":"error","event":"25/05/12 02:17:47 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:47.558113Z","level":"error","event":"25/05/12 02:17:47 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:47.558963Z","level":"error","event":"25/05/12 02:17:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-ed0f9964-1c3e-4ee1-972f-9e2a9477b769","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:47.571906Z","level":"error","event":"25/05/12 02:17:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-91638236-bd7b-4222-9207-3e1efff6e8cf","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:17:47.572550Z","level":"error","event":"25/05/12 02:17:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-91638236-bd7b-4222-9207-3e1efff6e8cf/pyspark-7c39ea80-ae56-497f-b9f9-1ec296f12a1a","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:18.466241","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:18:26.240396Z","level":"error","event":"25/05/12 02:18:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:26.576257Z","level":"error","event":"25/05/12 02:18:26 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:26.593075Z","level":"error","event":"25/05/12 02:18:26 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:26.593760Z","level":"error","event":"25/05/12 02:18:26 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:26.594336Z","level":"error","event":"25/05/12 02:18:26 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:26.843581Z","level":"error","event":"25/05/12 02:18:26 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:26.862127Z","level":"error","event":"25/05/12 02:18:26 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:26.862853Z","level":"error","event":"25/05/12 02:18:26 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:26.932898Z","level":"error","event":"25/05/12 02:18:26 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:26.953797Z","level":"error","event":"25/05/12 02:18:26 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:26.954691Z","level":"error","event":"25/05/12 02:18:26 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:26.955581Z","level":"error","event":"25/05/12 02:18:26 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:27.011887Z","level":"error","event":"25/05/12 02:18:27 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:27.039418Z","level":"error","event":"25/05/12 02:18:27 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:27.061040Z","level":"error","event":"25/05/12 02:18:27 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:27.154372Z","level":"error","event":"25/05/12 02:18:27 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:27.170611Z","level":"error","event":"25/05/12 02:18:27 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:27.171375Z","level":"error","event":"25/05/12 02:18:27 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:27.172037Z","level":"error","event":"25/05/12 02:18:27 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:27.172697Z","level":"error","event":"25/05/12 02:18:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:27.791453Z","level":"error","event":"25/05/12 02:18:27 INFO Utils: Successfully started service 'sparkDriver' on port 46263.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:27.848533Z","level":"error","event":"25/05/12 02:18:27 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:27.979951Z","level":"error","event":"25/05/12 02:18:27 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:28.031469Z","level":"error","event":"25/05/12 02:18:28 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:28.051327Z","level":"error","event":"25/05/12 02:18:28 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:28.074853Z","level":"error","event":"25/05/12 02:18:28 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:28.182174Z","level":"error","event":"25/05/12 02:18:28 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e6bc61f4-e660-4777-8912-61b81e204e37","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:28.297192Z","level":"error","event":"25/05/12 02:18:28 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:28.376144Z","level":"error","event":"25/05/12 02:18:28 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:29.613633Z","level":"error","event":"25/05/12 02:18:29 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.136281Z","level":"error","event":"25/05/12 02:18:31 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.272738Z","level":"error","event":"25/05/12 02:18:31 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.273882Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.274686Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.275359Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.276119Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.277130Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.282075Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.283190Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.284095Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.285136Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.285923Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.286798Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.287955Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.288752Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.289474Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.290182Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.290910Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.292349Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.293446Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.294462Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.295129Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.295787Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.296439Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.297153Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.297923Z","level":"error","event":"25/05/12 02:18:31 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.298619Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.299263Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.299928Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.300506Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.301461Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.302809Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.304043Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.305061Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.305805Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.306649Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.307500Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.308748Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.309854Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.310731Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.311654Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.312449Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.313328Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.314162Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.314972Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.315724Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.316554Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.317455Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.318209Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.318917Z","level":"error","event":"25/05/12 02:18:31 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.319690Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.597696Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.598788Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.600606Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.601882Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.602851Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.608937Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.612023Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.613261Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.614081Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.615000Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.623596Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.627425Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.628330Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.629164Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.630093Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.630900Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.631720Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.632468Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.635065Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.636866Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.639675Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.640743Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.641636Z","level":"error","event":"25/05/12 02:18:31 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.642528Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.643372Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.644086Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.645013Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.645953Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.646874Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.648618Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.649870Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.650815Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.663919Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.666138Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.674835Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.677930Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.678991Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.679969Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.681024Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.690307Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.693115Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.694241Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.700925Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.707393Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.708727Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.710010Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.752098Z","level":"error","event":"25/05/12 02:18:31 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.762004Z","level":"error","event":"25/05/12 02:18:31 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.763144Z","level":"error","event":"25/05/12 02:18:31 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.814110Z","level":"error","event":"25/05/12 02:18:31 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.815072Z","level":"error","event":"25/05/12 02:18:31 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@4259330d for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.915610Z","level":"error","event":"25/05/12 02:18:31 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41575.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.931557Z","level":"error","event":"25/05/12 02:18:31 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:41575","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.932275Z","level":"error","event":"25/05/12 02:18:31 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.953607Z","level":"error","event":"25/05/12 02:18:31 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 41575, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.987821Z","level":"error","event":"25/05/12 02:18:31 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:41575 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 41575, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.989002Z","level":"error","event":"25/05/12 02:18:31 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 41575, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:31.989916Z","level":"error","event":"25/05/12 02:18:31 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 41575, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:32.885488Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:18:33.922398Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:18:34.922495Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:18:35.054445Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:18:36.054626Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:18:36.128876Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:18:37.129047Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:18:37.223032Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:18:38.223168Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:18:38.320056Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:18:39.320260Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:18:39.331101Z","level":"error","event":"25/05/12 02:18:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:39.341678Z","level":"error","event":"25/05/12 02:18:39 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:18:53.824761","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:19:00.285966Z","level":"error","event":"25/05/12 02:19:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:01.572267Z","level":"error","event":"25/05/12 02:19:01 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:01.592456Z","level":"error","event":"25/05/12 02:19:01 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:01.593369Z","level":"error","event":"25/05/12 02:19:01 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:01.594222Z","level":"error","event":"25/05/12 02:19:01 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:03.137370Z","level":"error","event":"25/05/12 02:19:03 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:03.166021Z","level":"error","event":"25/05/12 02:19:03 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:03.166988Z","level":"error","event":"25/05/12 02:19:03 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:03.238738Z","level":"error","event":"25/05/12 02:19:03 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:03.260566Z","level":"error","event":"25/05/12 02:19:03 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:03.261643Z","level":"error","event":"25/05/12 02:19:03 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:03.262796Z","level":"error","event":"25/05/12 02:19:03 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:03.305830Z","level":"error","event":"25/05/12 02:19:03 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:03.326825Z","level":"error","event":"25/05/12 02:19:03 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:03.345002Z","level":"error","event":"25/05/12 02:19:03 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:03.452246Z","level":"error","event":"25/05/12 02:19:03 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:03.477605Z","level":"error","event":"25/05/12 02:19:03 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:03.478453Z","level":"error","event":"25/05/12 02:19:03 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:03.479932Z","level":"error","event":"25/05/12 02:19:03 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:03.480965Z","level":"error","event":"25/05/12 02:19:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:04.290220Z","level":"error","event":"25/05/12 02:19:04 INFO Utils: Successfully started service 'sparkDriver' on port 39995.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:04.379341Z","level":"error","event":"25/05/12 02:19:04 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:04.473838Z","level":"error","event":"25/05/12 02:19:04 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:04.524074Z","level":"error","event":"25/05/12 02:19:04 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:04.550877Z","level":"error","event":"25/05/12 02:19:04 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:04.551820Z","level":"error","event":"25/05/12 02:19:04 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:04.606813Z","level":"error","event":"25/05/12 02:19:04 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-af452054-5f1e-4edd-bd1f-37e6f6f6b48f","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:04.644933Z","level":"error","event":"25/05/12 02:19:04 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:04.682397Z","level":"error","event":"25/05/12 02:19:04 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.106081Z","level":"error","event":"25/05/12 02:19:05 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.271910Z","level":"error","event":"25/05/12 02:19:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.359558Z","level":"error","event":"25/05/12 02:19:05 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.360268Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.360916Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.361578Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.362237Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.362895Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.363513Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.364823Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.365682Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.366274Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.366974Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.367632Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.368359Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.369074Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.369751Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.370371Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.371220Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.371860Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.372795Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.373546Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.374277Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.374981Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.375843Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.376573Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.400313Z","level":"error","event":"25/05/12 02:19:05 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.401113Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.401802Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.402809Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.403438Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.404087Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.404763Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.406582Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.407735Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.409196Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.410024Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.412814Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.415423Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.416307Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.417476Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.418758Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.420589Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.421954Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.423234Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.424075Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.426228Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.428759Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.429874Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.431217Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.433217Z","level":"error","event":"25/05/12 02:19:05 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.433970Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.434637Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.435305Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.435917Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.436598Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.437356Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.438215Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.439065Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.439847Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.443646Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.444543Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.445553Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.446890Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.449525Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.451005Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.452006Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.452814Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.453572Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.454345Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.455364Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.460003Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.460962Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.461716Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.462339Z","level":"error","event":"25/05/12 02:19:05 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.463056Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.463553Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.480494Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.481220Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.482071Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.482892Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.483607Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.484663Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.485447Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.486199Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.487028Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.489057Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.489923Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.490602Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.491341Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.492130Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.492888Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.493574Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.494210Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.494949Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.495578Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.496216Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.496846Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.543390Z","level":"error","event":"25/05/12 02:19:05 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.559146Z","level":"error","event":"25/05/12 02:19:05 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.562002Z","level":"error","event":"25/05/12 02:19:05 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.580891Z","level":"error","event":"25/05/12 02:19:05 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.581687Z","level":"error","event":"25/05/12 02:19:05 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2e55cb96 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.629709Z","level":"error","event":"25/05/12 02:19:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40165.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.631889Z","level":"error","event":"25/05/12 02:19:05 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:40165","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.652039Z","level":"error","event":"25/05/12 02:19:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.652921Z","level":"error","event":"25/05/12 02:19:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 40165, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.675625Z","level":"error","event":"25/05/12 02:19:05 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:40165 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 40165, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.676500Z","level":"error","event":"25/05/12 02:19:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 40165, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:05.677463Z","level":"error","event":"25/05/12 02:19:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 40165, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:06.695261Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:19:07.493916Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:19:08.494077Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:19:08.616210Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:19:09.616488Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:19:09.809553Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:19:10.809762Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:19:10.971167Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:19:11.971297Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:19:12.115604Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:19:13.115775Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:19:13.134596Z","level":"error","event":"25/05/12 02:19:13 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:13.155634Z","level":"error","event":"25/05/12 02:19:13 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:15.603937","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:19:16.127243Z","level":"error","event":"25/05/12 02:19:16 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:16.127858Z","level":"error","event":"25/05/12 02:19:16 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:16.144363Z","level":"error","event":"25/05/12 02:19:16 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:16.175279Z","level":"error","event":"25/05/12 02:19:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:16.197489Z","level":"error","event":"25/05/12 02:19:16 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:16.210245Z","level":"error","event":"25/05/12 02:19:16 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:16.210842Z","level":"error","event":"25/05/12 02:19:16 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:16.225603Z","level":"error","event":"25/05/12 02:19:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:16.238285Z","level":"error","event":"25/05/12 02:19:16 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:16.239031Z","level":"error","event":"25/05/12 02:19:16 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:16.239603Z","level":"error","event":"25/05/12 02:19:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-b36ad58b-7fe1-4b82-9810-51f93a98d01f","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:16.252761Z","level":"error","event":"25/05/12 02:19:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-6690f1eb-203a-4978-bc21-4e40d12e856c","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:16.253699Z","level":"error","event":"25/05/12 02:19:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-b36ad58b-7fe1-4b82-9810-51f93a98d01f/pyspark-01635ac1-4e37-4d99-b942-e760db37ce00","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:46.853897","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:19:52.587498Z","level":"error","event":"25/05/12 02:19:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:52.992830Z","level":"error","event":"25/05/12 02:19:52 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:53.010562Z","level":"error","event":"25/05/12 02:19:52 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:53.011422Z","level":"error","event":"25/05/12 02:19:52 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:53.012219Z","level":"error","event":"25/05/12 02:19:52 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:53.244017Z","level":"error","event":"25/05/12 02:19:53 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:53.258628Z","level":"error","event":"25/05/12 02:19:53 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:53.259326Z","level":"error","event":"25/05/12 02:19:53 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:53.295967Z","level":"error","event":"25/05/12 02:19:53 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:53.308484Z","level":"error","event":"25/05/12 02:19:53 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:53.309204Z","level":"error","event":"25/05/12 02:19:53 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:53.309826Z","level":"error","event":"25/05/12 02:19:53 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:53.330978Z","level":"error","event":"25/05/12 02:19:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:53.346159Z","level":"error","event":"25/05/12 02:19:53 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:53.360312Z","level":"error","event":"25/05/12 02:19:53 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:53.449851Z","level":"error","event":"25/05/12 02:19:53 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:53.464787Z","level":"error","event":"25/05/12 02:19:53 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:53.465580Z","level":"error","event":"25/05/12 02:19:53 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:53.466269Z","level":"error","event":"25/05/12 02:19:53 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:53.467020Z","level":"error","event":"25/05/12 02:19:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:53.809138Z","level":"error","event":"25/05/12 02:19:53 INFO Utils: Successfully started service 'sparkDriver' on port 39517.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:53.851715Z","level":"error","event":"25/05/12 02:19:53 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:53.942482Z","level":"error","event":"25/05/12 02:19:53 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:53.981483Z","level":"error","event":"25/05/12 02:19:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.012253Z","level":"error","event":"25/05/12 02:19:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.013022Z","level":"error","event":"25/05/12 02:19:54 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.044905Z","level":"error","event":"25/05/12 02:19:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7cf2bef0-b842-4bbb-9586-93ac9cd1530f","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.070607Z","level":"error","event":"25/05/12 02:19:54 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.101102Z","level":"error","event":"25/05/12 02:19:54 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.376957Z","level":"error","event":"25/05/12 02:19:54 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.497239Z","level":"error","event":"25/05/12 02:19:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.589487Z","level":"error","event":"25/05/12 02:19:54 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.590491Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.591190Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.591922Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.592655Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.593392Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.594016Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.594605Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.595228Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.595811Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.596453Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.597151Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.598072Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.598797Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.599517Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.600276Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.601057Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.601725Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.602513Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.603198Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.603949Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.604635Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.605349Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.606075Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.623530Z","level":"error","event":"25/05/12 02:19:54 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.624680Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.625588Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.626433Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.627205Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.628036Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.630838Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.631938Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.632708Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.633457Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.640163Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.641122Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.642119Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.642871Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.643875Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.644774Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.645536Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.646159Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.646825Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.647484Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.648099Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.648729Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.649422Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.650228Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.651002Z","level":"error","event":"25/05/12 02:19:54 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.651825Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.652563Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.653333Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.654123Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.654869Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.655733Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.656586Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.657413Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.658309Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.658992Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.659844Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.660452Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.661019Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.661748Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.662338Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.663071Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.663786Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.664572Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.665084Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.665590Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.666071Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.666610Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.667169Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.667731Z","level":"error","event":"25/05/12 02:19:54 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.668388Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.669100Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.695104Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.695928Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.696671Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.697494Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.698187Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.698862Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.699604Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.700563Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.701374Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.702420Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.703438Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.704254Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.705232Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.706072Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.706967Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.707805Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.708576Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.709287Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.710124Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.710842Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.711596Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.739731Z","level":"error","event":"25/05/12 02:19:54 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.756505Z","level":"error","event":"25/05/12 02:19:54 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.757402Z","level":"error","event":"25/05/12 02:19:54 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.776534Z","level":"error","event":"25/05/12 02:19:54 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.777279Z","level":"error","event":"25/05/12 02:19:54 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6ff21454 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.804537Z","level":"error","event":"25/05/12 02:19:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43365.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.817033Z","level":"error","event":"25/05/12 02:19:54 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:43365","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.817659Z","level":"error","event":"25/05/12 02:19:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.818089Z","level":"error","event":"25/05/12 02:19:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 43365, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.831080Z","level":"error","event":"25/05/12 02:19:54 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:43365 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 43365, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.831633Z","level":"error","event":"25/05/12 02:19:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 43365, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:54.832167Z","level":"error","event":"25/05/12 02:19:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 43365, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:19:55.561175Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:19:56.104322Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:19:57.104573Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:19:57.205661Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:19:58.205858Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:19:58.286318Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:19:59.286440Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:19:59.381865Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:20:00.382085Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:20:00.503175Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:20:01.503386Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:20:01.519536Z","level":"error","event":"25/05/12 02:20:01 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:01.539499Z","level":"error","event":"25/05/12 02:20:01 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:03.684765","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:20:04.180349Z","level":"error","event":"25/05/12 02:20:04 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:04.181096Z","level":"error","event":"25/05/12 02:20:04 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:04.198713Z","level":"error","event":"25/05/12 02:20:04 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:04.219323Z","level":"error","event":"25/05/12 02:20:04 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:04.242294Z","level":"error","event":"25/05/12 02:20:04 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:04.255300Z","level":"error","event":"25/05/12 02:20:04 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:04.256035Z","level":"error","event":"25/05/12 02:20:04 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:04.269691Z","level":"error","event":"25/05/12 02:20:04 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:04.312874Z","level":"error","event":"25/05/12 02:20:04 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:04.332593Z","level":"error","event":"25/05/12 02:20:04 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:04.333292Z","level":"error","event":"25/05/12 02:20:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-8b355288-4bf5-436f-8aa5-30678f179f64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:04.334099Z","level":"error","event":"25/05/12 02:20:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-8241f96c-98ff-41c9-b2ec-2e02d325f67f","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:04.354367Z","level":"error","event":"25/05/12 02:20:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-8241f96c-98ff-41c9-b2ec-2e02d325f67f/pyspark-49bbd0e5-6ad1-481c-95b6-e93c56802bfe","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:35.612048","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:20:44.231434Z","level":"error","event":"25/05/12 02:20:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:45.136433Z","level":"error","event":"25/05/12 02:20:45 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:45.172119Z","level":"error","event":"25/05/12 02:20:45 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:45.173049Z","level":"error","event":"25/05/12 02:20:45 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:45.174010Z","level":"error","event":"25/05/12 02:20:45 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:45.335498Z","level":"error","event":"25/05/12 02:20:45 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:45.348933Z","level":"error","event":"25/05/12 02:20:45 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:45.349550Z","level":"error","event":"25/05/12 02:20:45 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:45.394580Z","level":"error","event":"25/05/12 02:20:45 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:45.405386Z","level":"error","event":"25/05/12 02:20:45 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:45.406169Z","level":"error","event":"25/05/12 02:20:45 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:45.406698Z","level":"error","event":"25/05/12 02:20:45 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:45.435306Z","level":"error","event":"25/05/12 02:20:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:45.447970Z","level":"error","event":"25/05/12 02:20:45 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:45.464580Z","level":"error","event":"25/05/12 02:20:45 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:45.534814Z","level":"error","event":"25/05/12 02:20:45 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:45.548637Z","level":"error","event":"25/05/12 02:20:45 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:45.549892Z","level":"error","event":"25/05/12 02:20:45 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:45.550543Z","level":"error","event":"25/05/12 02:20:45 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:45.551761Z","level":"error","event":"25/05/12 02:20:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:45.910843Z","level":"error","event":"25/05/12 02:20:45 INFO Utils: Successfully started service 'sparkDriver' on port 34285.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:45.959737Z","level":"error","event":"25/05/12 02:20:45 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.017584Z","level":"error","event":"25/05/12 02:20:46 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.051227Z","level":"error","event":"25/05/12 02:20:46 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.066483Z","level":"error","event":"25/05/12 02:20:46 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.067013Z","level":"error","event":"25/05/12 02:20:46 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.111932Z","level":"error","event":"25/05/12 02:20:46 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ae8bc4c5-cba4-4af7-a657-738d0a5279b6","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.142871Z","level":"error","event":"25/05/12 02:20:46 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.175764Z","level":"error","event":"25/05/12 02:20:46 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.529703Z","level":"error","event":"25/05/12 02:20:46 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.826698Z","level":"error","event":"25/05/12 02:20:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.931008Z","level":"error","event":"25/05/12 02:20:46 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.931753Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.932666Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.933549Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.934208Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.935228Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.936001Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.936692Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.937449Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.938327Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.939139Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.942089Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.942980Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.943605Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.944479Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.945369Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.946605Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.947393Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.948200Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.949057Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.949814Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.950511Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.951380Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.952151Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.974616Z","level":"error","event":"25/05/12 02:20:46 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.979080Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.980856Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.982004Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.982992Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.983863Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.984662Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.985668Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.986473Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.987338Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.988075Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.988880Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.989566Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.990420Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.991684Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.995311Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.997345Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:46.999682Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.000572Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.001632Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.002812Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.003722Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.004449Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.005181Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.005813Z","level":"error","event":"25/05/12 02:20:46 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.006560Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.007257Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.007978Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.013460Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.014512Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.015264Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.017978Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.018836Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.019580Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.020283Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.021052Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.021788Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.022582Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.023413Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.024126Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.024794Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.025723Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.026561Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.027347Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.028066Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.028831Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.030078Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.030902Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.031633Z","level":"error","event":"25/05/12 02:20:46 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.032334Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.033038Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.062309Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.063234Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.064112Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.064923Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.065713Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.066559Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.067862Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.069054Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.070630Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.071758Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.072662Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.073655Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.074843Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.076582Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.077417Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.078895Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.079704Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.080568Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.081746Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.082647Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.083511Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.145561Z","level":"error","event":"25/05/12 02:20:47 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.170120Z","level":"error","event":"25/05/12 02:20:47 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.170932Z","level":"error","event":"25/05/12 02:20:47 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.171693Z","level":"error","event":"25/05/12 02:20:47 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.191859Z","level":"error","event":"25/05/12 02:20:47 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@240f8e49 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.267260Z","level":"error","event":"25/05/12 02:20:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34593.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.305754Z","level":"error","event":"25/05/12 02:20:47 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:34593","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.306962Z","level":"error","event":"25/05/12 02:20:47 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.392434Z","level":"error","event":"25/05/12 02:20:47 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 34593, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.406642Z","level":"error","event":"25/05/12 02:20:47 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:34593 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 34593, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.453476Z","level":"error","event":"25/05/12 02:20:47 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 34593, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:47.517858Z","level":"error","event":"25/05/12 02:20:47 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 34593, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:48.293768Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:20:49.111341Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:20:50.111529Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:20:50.211338Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:20:51.211561Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:20:51.302317Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:20:52.302627Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:20:52.510585Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:20:53.510692Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:20:53.600629Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:20:54.600778Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:20:54.622336Z","level":"error","event":"25/05/12 02:20:54 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:54.634425Z","level":"error","event":"25/05/12 02:20:54 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:56.375681","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:20:56.919066Z","level":"error","event":"25/05/12 02:20:56 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:56.921874Z","level":"error","event":"25/05/12 02:20:56 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:56.939190Z","level":"error","event":"25/05/12 02:20:56 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:56.967836Z","level":"error","event":"25/05/12 02:20:56 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:56.996260Z","level":"error","event":"25/05/12 02:20:56 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:57.034971Z","level":"error","event":"25/05/12 02:20:56 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:57.036131Z","level":"error","event":"25/05/12 02:20:57 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:57.037251Z","level":"error","event":"25/05/12 02:20:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:57.081193Z","level":"error","event":"25/05/12 02:20:57 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:57.142602Z","level":"error","event":"25/05/12 02:20:57 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:57.226082Z","level":"error","event":"25/05/12 02:20:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-97bb9881-d4c1-4824-815a-ecace51efda2","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:57.298923Z","level":"error","event":"25/05/12 02:20:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-a51a1fa7-bdfc-4587-9387-441b40aa1012","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:57.299682Z","level":"error","event":"25/05/12 02:20:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-a51a1fa7-bdfc-4587-9387-441b40aa1012/pyspark-48788c46-ee19-4446-b504-c3d29eef4ada","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:20:59.489404","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:20:59.515753","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"IndentationError","exc_value":"unexpected indent (YFinance.py, line 21)","exc_notes":[],"syntax_error":{"offset":4,"filename":"/opt/airflow/dags/jobs/YFinance.py","line":"    .config(\"spark.jars\", \"/opt/airflow/jars/mongo-spark-connector.jar,\"\n","lineno":21,"msg":"unexpected indent"},"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":995,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap_external>","lineno":1133,"name":"get_code"},{"filename":"<frozen importlib._bootstrap_external>","lineno":1063,"name":"source_to_code"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"}]}]}
{"timestamp":"2025-05-12T02:21:00.672494","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:21:00.701732","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"IndentationError","exc_value":"unexpected indent (YFinance.py, line 21)","exc_notes":[],"syntax_error":{"offset":4,"filename":"/opt/airflow/dags/jobs/YFinance.py","line":"    .config(\"spark.jars\", \"/opt/airflow/jars/mongo-spark-connector.jar,\"\n","lineno":21,"msg":"unexpected indent"},"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":995,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap_external>","lineno":1133,"name":"get_code"},{"filename":"<frozen importlib._bootstrap_external>","lineno":1063,"name":"source_to_code"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"}]}]}
{"timestamp":"2025-05-12T02:21:03.915409","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:21:03.944153","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"IndentationError","exc_value":"unexpected indent (YFinance.py, line 21)","exc_notes":[],"syntax_error":{"offset":4,"filename":"/opt/airflow/dags/jobs/YFinance.py","line":"    .config(\"spark.jars\", \"/opt/airflow/jars/mongo-spark-connector.jar,\"\n","lineno":21,"msg":"unexpected indent"},"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":995,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap_external>","lineno":1133,"name":"get_code"},{"filename":"<frozen importlib._bootstrap_external>","lineno":1063,"name":"source_to_code"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"}]}]}
{"timestamp":"2025-05-12T02:21:07.157660","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:21:07.183179","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"SyntaxError","exc_value":"invalid syntax (YFinance.py, line 20)","exc_notes":[],"syntax_error":{"offset":6,"filename":"/opt/airflow/dags/jobs/YFinance.py","line":"    .\n","lineno":20,"msg":"invalid syntax"},"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":995,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap_external>","lineno":1133,"name":"get_code"},{"filename":"<frozen importlib._bootstrap_external>","lineno":1063,"name":"source_to_code"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"}]}]}
{"timestamp":"2025-05-12T02:21:11.497752","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:21:11.533081","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"IndentationError","exc_value":"unexpected indent (YFinance.py, line 21)","exc_notes":[],"syntax_error":{"offset":4,"filename":"/opt/airflow/dags/jobs/YFinance.py","line":"    .config(\"spark.jars\", \"/opt/airflow/jars/mongo-spark-connector.jar,\"\n","lineno":21,"msg":"unexpected indent"},"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":995,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap_external>","lineno":1133,"name":"get_code"},{"filename":"<frozen importlib._bootstrap_external>","lineno":1063,"name":"source_to_code"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"}]}]}
{"timestamp":"2025-05-12T02:21:13.701537","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:21:15.769382","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AttributeError","exc_value":"'Builder' object has no attribute 'setLogLevel'","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":20,"name":"<module>"}]}]}
{"timestamp":"2025-05-12T02:21:47.267941","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:21:49.047193","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AttributeError","exc_value":"'Builder' object has no attribute 'setLogLevel'","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":20,"name":"<module>"}]}]}
{"timestamp":"2025-05-12T02:22:18.245931","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:22:18.280447","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"IndentationError","exc_value":"unexpected indent (YFinance.py, line 21)","exc_notes":[],"syntax_error":{"offset":4,"filename":"/opt/airflow/dags/jobs/YFinance.py","line":"    .config(\"spark.jars\", \"/opt/airflow/jars/mongo-spark-connector.jar,\"\n","lineno":21,"msg":"unexpected indent"},"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":995,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap_external>","lineno":1133,"name":"get_code"},{"filename":"<frozen importlib._bootstrap_external>","lineno":1063,"name":"source_to_code"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"}]}]}
{"timestamp":"2025-05-12T02:22:31.685500","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:22:37.605916Z","level":"error","event":"25/05/12 02:22:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:38.058228Z","level":"error","event":"25/05/12 02:22:38 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:38.088564Z","level":"error","event":"25/05/12 02:22:38 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:38.089766Z","level":"error","event":"25/05/12 02:22:38 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:38.090598Z","level":"error","event":"25/05/12 02:22:38 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:38.336523Z","level":"error","event":"25/05/12 02:22:38 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:38.352015Z","level":"error","event":"25/05/12 02:22:38 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:38.352639Z","level":"error","event":"25/05/12 02:22:38 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:38.387168Z","level":"error","event":"25/05/12 02:22:38 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:38.401287Z","level":"error","event":"25/05/12 02:22:38 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:38.401878Z","level":"error","event":"25/05/12 02:22:38 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:38.402465Z","level":"error","event":"25/05/12 02:22:38 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:38.422943Z","level":"error","event":"25/05/12 02:22:38 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:38.436220Z","level":"error","event":"25/05/12 02:22:38 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:38.447655Z","level":"error","event":"25/05/12 02:22:38 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:38.505442Z","level":"error","event":"25/05/12 02:22:38 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:38.518124Z","level":"error","event":"25/05/12 02:22:38 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:38.518616Z","level":"error","event":"25/05/12 02:22:38 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:38.519109Z","level":"error","event":"25/05/12 02:22:38 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:38.519616Z","level":"error","event":"25/05/12 02:22:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:38.856737Z","level":"error","event":"25/05/12 02:22:38 INFO Utils: Successfully started service 'sparkDriver' on port 46543.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:38.894286Z","level":"error","event":"25/05/12 02:22:38 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:38.943542Z","level":"error","event":"25/05/12 02:22:38 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:38.980379Z","level":"error","event":"25/05/12 02:22:38 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.003571Z","level":"error","event":"25/05/12 02:22:38 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.004255Z","level":"error","event":"25/05/12 02:22:38 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.032975Z","level":"error","event":"25/05/12 02:22:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8e107abb-3a50-4ec9-812b-5d119e4c8c10","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.067915Z","level":"error","event":"25/05/12 02:22:39 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.092653Z","level":"error","event":"25/05/12 02:22:39 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.285213Z","level":"error","event":"25/05/12 02:22:39 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.374082Z","level":"error","event":"25/05/12 02:22:39 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.430387Z","level":"error","event":"25/05/12 02:22:39 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.430896Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.431341Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.431863Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.432286Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.432701Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.433102Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.433577Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.434126Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.434700Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.435198Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.435787Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.436358Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.436881Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.437456Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.438067Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.438654Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.439286Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.439939Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.440564Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.441265Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.441951Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.442533Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.443200Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.457224Z","level":"error","event":"25/05/12 02:22:39 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.457783Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.458208Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.458836Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.459389Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.460198Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.460934Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.461644Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.462409Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.463047Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.463825Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.464570Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.465496Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.466371Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.467136Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.468067Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.468823Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.469515Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.470088Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.470703Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.471414Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.472073Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.472793Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.473419Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.473962Z","level":"error","event":"25/05/12 02:22:39 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.474666Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.475353Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.476298Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.477087Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.477636Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.478102Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.478582Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.479240Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.479879Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.480658Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.481465Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.482230Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.482845Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.483561Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.484233Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.484932Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.485593Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.486144Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.486740Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.487274Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.487918Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.488517Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.489183Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.489760Z","level":"error","event":"25/05/12 02:22:39 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.490626Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.491382Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.504246Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.504831Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.505276Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.505846Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.506489Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.507167Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.507843Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.508429Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.509015Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.509568Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.510124Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.510617Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.511086Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.511589Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.512132Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.512618Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.513081Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.513580Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.514076Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.514534Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.514949Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.532642Z","level":"error","event":"25/05/12 02:22:39 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.543836Z","level":"error","event":"25/05/12 02:22:39 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.544369Z","level":"error","event":"25/05/12 02:22:39 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.544976Z","level":"error","event":"25/05/12 02:22:39 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.556824Z","level":"error","event":"25/05/12 02:22:39 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@73663445 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.576878Z","level":"error","event":"25/05/12 02:22:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32953.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.587364Z","level":"error","event":"25/05/12 02:22:39 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:32953","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.587843Z","level":"error","event":"25/05/12 02:22:39 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.588259Z","level":"error","event":"25/05/12 02:22:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 32953, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.597543Z","level":"error","event":"25/05/12 02:22:39 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:32953 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 32953, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.597945Z","level":"error","event":"25/05/12 02:22:39 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 32953, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.598271Z","level":"error","event":"25/05/12 02:22:39 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 32953, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.748363Z","level":"error","event":"25/05/12 02:22:39 ERROR SparkContext: Error initializing SparkContext.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.748959Z","level":"error","event":"java.io.FileNotFoundException: File file:/tmp/spark-events does not exist","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.749463Z","level":"error","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.749926Z","level":"error","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.750383Z","level":"error","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.750791Z","level":"error","event":"\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.751120Z","level":"error","event":"\tat org.apache.spark.deploy.history.EventLogFileWriter.requireLogBaseDirAsDirectory(EventLogFileWriters.scala:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.751477Z","level":"error","event":"\tat org.apache.spark.deploy.history.SingleEventLogFileWriter.start(EventLogFileWriters.scala:221)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.751783Z","level":"error","event":"\tat org.apache.spark.scheduler.EventLoggingListener.start(EventLoggingListener.scala:81)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.752115Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:632)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.752462Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.752884Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.753232Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.753583Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.753954Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.754282Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.754582Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.754895Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.755185Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.755477Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.755769Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.756155Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.756492Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.756805Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.767364Z","level":"error","event":"25/05/12 02:22:39 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.767841Z","level":"error","event":"25/05/12 02:22:39 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.782365Z","level":"error","event":"25/05/12 02:22:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.804827Z","level":"error","event":"25/05/12 02:22:39 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.815647Z","level":"error","event":"25/05/12 02:22:39 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.816137Z","level":"error","event":"25/05/12 02:22:39 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.826543Z","level":"error","event":"25/05/12 02:22:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.835284Z","level":"error","event":"25/05/12 02:22:39 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:39.830196","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.io.FileNotFoundException: File file:/tmp/spark-events does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.spark.deploy.history.EventLogFileWriter.requireLogBaseDirAsDirectory(EventLogFileWriters.scala:77)\n\tat org.apache.spark.deploy.history.SingleEventLogFileWriter.start(EventLogFileWriters.scala:221)\n\tat org.apache.spark.scheduler.EventLoggingListener.start(EventLoggingListener.scala:81)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:632)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":30,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":497,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":515,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":203,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":296,"name":"_do_init"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":421,"name":"_initialize_context"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1587,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:22:40.349424Z","level":"error","event":"25/05/12 02:22:40 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:40.349800Z","level":"error","event":"25/05/12 02:22:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-5b2eb1ed-01b3-4fab-87a5-c08ffe089e8e","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:22:40.357774Z","level":"error","event":"25/05/12 02:22:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-0942dbde-2a97-445a-a7c2-29bced2b7977","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:11.528320","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:23:17.365359Z","level":"error","event":"25/05/12 02:23:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:20.155640Z","level":"error","event":"25/05/12 02:23:20 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:20.192493Z","level":"error","event":"25/05/12 02:23:20 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:20.193944Z","level":"error","event":"25/05/12 02:23:20 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:20.195043Z","level":"error","event":"25/05/12 02:23:20 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:20.392683Z","level":"error","event":"25/05/12 02:23:20 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:20.412219Z","level":"error","event":"25/05/12 02:23:20 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:20.413078Z","level":"error","event":"25/05/12 02:23:20 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:20.503099Z","level":"error","event":"25/05/12 02:23:20 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:20.523428Z","level":"error","event":"25/05/12 02:23:20 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:20.524755Z","level":"error","event":"25/05/12 02:23:20 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:20.533062Z","level":"error","event":"25/05/12 02:23:20 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:20.652523Z","level":"error","event":"25/05/12 02:23:20 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:20.653279Z","level":"error","event":"25/05/12 02:23:20 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:20.653983Z","level":"error","event":"25/05/12 02:23:20 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:20.762612Z","level":"error","event":"25/05/12 02:23:20 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:20.780445Z","level":"error","event":"25/05/12 02:23:20 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:20.781425Z","level":"error","event":"25/05/12 02:23:20 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:20.782533Z","level":"error","event":"25/05/12 02:23:20 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:20.783406Z","level":"error","event":"25/05/12 02:23:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:21.288943Z","level":"error","event":"25/05/12 02:23:21 INFO Utils: Successfully started service 'sparkDriver' on port 45631.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:21.353080Z","level":"error","event":"25/05/12 02:23:21 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:21.445471Z","level":"error","event":"25/05/12 02:23:21 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:21.490124Z","level":"error","event":"25/05/12 02:23:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:21.517204Z","level":"error","event":"25/05/12 02:23:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:21.518086Z","level":"error","event":"25/05/12 02:23:21 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:21.559547Z","level":"error","event":"25/05/12 02:23:21 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fd082344-5be0-4dc5-8619-2409b515dc6a","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:21.596606Z","level":"error","event":"25/05/12 02:23:21 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:21.639375Z","level":"error","event":"25/05/12 02:23:21 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:21.986487Z","level":"error","event":"25/05/12 02:23:21 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.137783Z","level":"error","event":"25/05/12 02:23:22 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.234128Z","level":"error","event":"25/05/12 02:23:22 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.234932Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.235640Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.236443Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.237078Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.238008Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.239002Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.239963Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.241040Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.242177Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.242829Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.243624Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.244326Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.245064Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.245850Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.246590Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.247346Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.248145Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.249014Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.250122Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.251020Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.252055Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.252917Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.253966Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.273482Z","level":"error","event":"25/05/12 02:23:22 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.274266Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.275083Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.275779Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.276666Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.277465Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.278172Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.278852Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.279579Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.280280Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.280997Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.281758Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.282415Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.283098Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.283761Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.284504Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.285370Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.286487Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.288692Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.289488Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.290573Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.291347Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.292110Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.292779Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.293466Z","level":"error","event":"25/05/12 02:23:22 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.294193Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.294849Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.295590Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.296450Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.297436Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.298599Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.299474Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.300543Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.303821Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.306097Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.307124Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.308151Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.309631Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.310668Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.311560Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.313020Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.313650Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.314477Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.315196Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.316172Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.317030Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.318203Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.319183Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.320377Z","level":"error","event":"25/05/12 02:23:22 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.321375Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.322542Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.339855Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.340842Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.341698Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.343099Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.347683Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.348646Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.349664Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.352034Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.353019Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.353816Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.354519Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.355072Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.355663Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.356262Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.357111Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.357646Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.358140Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.358670Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.359286Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.359883Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.360648Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.408944Z","level":"error","event":"25/05/12 02:23:22 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.420847Z","level":"error","event":"25/05/12 02:23:22 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.421462Z","level":"error","event":"25/05/12 02:23:22 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.432951Z","level":"error","event":"25/05/12 02:23:22 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.433466Z","level":"error","event":"25/05/12 02:23:22 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@240f8e49 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.463799Z","level":"error","event":"25/05/12 02:23:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44087.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.475219Z","level":"error","event":"25/05/12 02:23:22 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:44087","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.475785Z","level":"error","event":"25/05/12 02:23:22 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.476307Z","level":"error","event":"25/05/12 02:23:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 44087, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.489326Z","level":"error","event":"25/05/12 02:23:22 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:44087 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 44087, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.489952Z","level":"error","event":"25/05/12 02:23:22 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 44087, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.490609Z","level":"error","event":"25/05/12 02:23:22 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 44087, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.732541Z","level":"error","event":"25/05/12 02:23:22 ERROR SparkContext: Error initializing SparkContext.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.733177Z","level":"error","event":"java.io.FileNotFoundException: File file:/tmp/spark-events does not exist","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.733677Z","level":"error","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.734141Z","level":"error","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.734758Z","level":"error","event":"\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.735284Z","level":"error","event":"\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.735860Z","level":"error","event":"\tat org.apache.spark.deploy.history.EventLogFileWriter.requireLogBaseDirAsDirectory(EventLogFileWriters.scala:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.736381Z","level":"error","event":"\tat org.apache.spark.deploy.history.SingleEventLogFileWriter.start(EventLogFileWriters.scala:221)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.736978Z","level":"error","event":"\tat org.apache.spark.scheduler.EventLoggingListener.start(EventLoggingListener.scala:81)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.737496Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:632)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.738493Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.739444Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.740258Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.741006Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.741624Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.742185Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.742677Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.743242Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.743721Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.744191Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.744643Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.745067Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.745503Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.745988Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.759447Z","level":"error","event":"25/05/12 02:23:22 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.759908Z","level":"error","event":"25/05/12 02:23:22 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.787443Z","level":"error","event":"25/05/12 02:23:22 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.826152Z","level":"error","event":"25/05/12 02:23:22 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.841966Z","level":"error","event":"25/05/12 02:23:22 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.842749Z","level":"error","event":"25/05/12 02:23:22 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.859635Z","level":"error","event":"25/05/12 02:23:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.872651Z","level":"error","event":"25/05/12 02:23:22 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:22.875897","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.io.FileNotFoundException: File file:/tmp/spark-events does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.spark.deploy.history.EventLogFileWriter.requireLogBaseDirAsDirectory(EventLogFileWriters.scala:77)\n\tat org.apache.spark.deploy.history.SingleEventLogFileWriter.start(EventLogFileWriters.scala:221)\n\tat org.apache.spark.scheduler.EventLoggingListener.start(EventLoggingListener.scala:81)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:632)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":30,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":497,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":515,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":203,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":296,"name":"_do_init"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":421,"name":"_initialize_context"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1587,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:23:23.209878Z","level":"error","event":"25/05/12 02:23:23 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:23.210552Z","level":"error","event":"25/05/12 02:23:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-cb7450bd-7bc9-400a-aac8-fea468687b5f","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:23.222551Z","level":"error","event":"25/05/12 02:23:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-8c4e05ca-4bf9-40c5-9a20-e48cf96182e9","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:41.977776","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:23:45.855746Z","level":"error","event":"25/05/12 02:23:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:46.519038Z","level":"error","event":"25/05/12 02:23:46 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:46.534186Z","level":"error","event":"25/05/12 02:23:46 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:46.534865Z","level":"error","event":"25/05/12 02:23:46 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:46.535545Z","level":"error","event":"25/05/12 02:23:46 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:46.734701Z","level":"error","event":"25/05/12 02:23:46 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:46.747943Z","level":"error","event":"25/05/12 02:23:46 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:46.748543Z","level":"error","event":"25/05/12 02:23:46 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:46.780945Z","level":"error","event":"25/05/12 02:23:46 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:46.790778Z","level":"error","event":"25/05/12 02:23:46 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:46.791339Z","level":"error","event":"25/05/12 02:23:46 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:46.791930Z","level":"error","event":"25/05/12 02:23:46 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:46.814538Z","level":"error","event":"25/05/12 02:23:46 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:46.827692Z","level":"error","event":"25/05/12 02:23:46 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:46.840712Z","level":"error","event":"25/05/12 02:23:46 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:46.898459Z","level":"error","event":"25/05/12 02:23:46 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:46.910999Z","level":"error","event":"25/05/12 02:23:46 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:46.911599Z","level":"error","event":"25/05/12 02:23:46 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:46.912184Z","level":"error","event":"25/05/12 02:23:46 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:46.912664Z","level":"error","event":"25/05/12 02:23:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:47.367923Z","level":"error","event":"25/05/12 02:23:47 INFO Utils: Successfully started service 'sparkDriver' on port 41653.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:47.453862Z","level":"error","event":"25/05/12 02:23:47 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:47.542012Z","level":"error","event":"25/05/12 02:23:47 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:47.581533Z","level":"error","event":"25/05/12 02:23:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:47.599935Z","level":"error","event":"25/05/12 02:23:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:47.600703Z","level":"error","event":"25/05/12 02:23:47 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:47.629106Z","level":"error","event":"25/05/12 02:23:47 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a9b780d9-8d6f-49e5-873f-51f790b7beaa","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:47.670273Z","level":"error","event":"25/05/12 02:23:47 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:47.695844Z","level":"error","event":"25/05/12 02:23:47 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:47.939407Z","level":"error","event":"25/05/12 02:23:47 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.033636Z","level":"error","event":"25/05/12 02:23:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.097321Z","level":"error","event":"25/05/12 02:23:48 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.097972Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.098424Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.098967Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.099551Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.100000Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.100674Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.101159Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.101705Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.102229Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.102825Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.103474Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.104067Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.104560Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.105187Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.105855Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.106442Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.107302Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.108189Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.108997Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.109799Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.110729Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.111484Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.112069Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.130062Z","level":"error","event":"25/05/12 02:23:48 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.130680Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.131200Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.131855Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.132495Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.133129Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.133931Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.134690Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.135401Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.136244Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.137002Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.137615Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.138633Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.139495Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.141571Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.142585Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.143599Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.144745Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.145611Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.146216Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.146910Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.147466Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.147913Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.148443Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.148916Z","level":"error","event":"25/05/12 02:23:48 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.149317Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.149822Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.150486Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.151034Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.151473Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.151920Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.152538Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.153303Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.154330Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.155298Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.156118Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.156853Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.157847Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.158544Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.159140Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.159702Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.160301Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.160811Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.161224Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.161586Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.161975Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.162314Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.163001Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.163638Z","level":"error","event":"25/05/12 02:23:48 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.164146Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.164866Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.178560Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.179107Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.179703Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.180271Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.180851Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.181414Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.181939Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.182454Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.182906Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.183383Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.183852Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.184296Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.184838Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.185983Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.186820Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.187497Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.188203Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.188954Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.189636Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.190261Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.190897Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.215000Z","level":"error","event":"25/05/12 02:23:48 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.228572Z","level":"error","event":"25/05/12 02:23:48 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.229305Z","level":"error","event":"25/05/12 02:23:48 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.250374Z","level":"error","event":"25/05/12 02:23:48 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.251521Z","level":"error","event":"25/05/12 02:23:48 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6ff21454 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.286463Z","level":"error","event":"25/05/12 02:23:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44577.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.299812Z","level":"error","event":"25/05/12 02:23:48 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:44577","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.300456Z","level":"error","event":"25/05/12 02:23:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.301166Z","level":"error","event":"25/05/12 02:23:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 44577, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.315619Z","level":"error","event":"25/05/12 02:23:48 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:44577 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 44577, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.316416Z","level":"error","event":"25/05/12 02:23:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 44577, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:48.317179Z","level":"error","event":"25/05/12 02:23:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 44577, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:49.081490Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:23:50.708228Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:23:52.982926Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:23:53.105374Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:23:54.105705Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:23:54.174923Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:23:55.175276Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:23:55.272755Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:23:56.272946Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:23:56.352563Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:23:57.353327Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:23:57.393002Z","level":"error","event":"25/05/12 02:23:57 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:57.394420Z","level":"error","event":"25/05/12 02:23:57 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:23:59.814740","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:24:00.237507Z","level":"error","event":"25/05/12 02:24:00 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:00.238009Z","level":"error","event":"25/05/12 02:24:00 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:00.251043Z","level":"error","event":"25/05/12 02:24:00 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:00.268311Z","level":"error","event":"25/05/12 02:24:00 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:00.287619Z","level":"error","event":"25/05/12 02:24:00 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:00.299694Z","level":"error","event":"25/05/12 02:24:00 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:00.300397Z","level":"error","event":"25/05/12 02:24:00 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:00.301191Z","level":"error","event":"25/05/12 02:24:00 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:00.315246Z","level":"error","event":"25/05/12 02:24:00 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:00.328206Z","level":"error","event":"25/05/12 02:24:00 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:00.328641Z","level":"error","event":"25/05/12 02:24:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-9cdd99fe-d372-4d3e-944e-ed03fd4a24a7","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:00.329013Z","level":"error","event":"25/05/12 02:24:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-7e5a1c9d-dc88-431b-9d4f-3d032fec3409","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:00.340883Z","level":"error","event":"25/05/12 02:24:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-7e5a1c9d-dc88-431b-9d4f-3d032fec3409/pyspark-7055fe18-08a2-465f-af76-58bdca279fd8","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:30.960109","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:24:38.572151Z","level":"error","event":"25/05/12 02:24:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:39.102500Z","level":"error","event":"25/05/12 02:24:39 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:39.117373Z","level":"error","event":"25/05/12 02:24:39 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:39.118031Z","level":"error","event":"25/05/12 02:24:39 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:39.119139Z","level":"error","event":"25/05/12 02:24:39 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:39.387576Z","level":"error","event":"25/05/12 02:24:39 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:39.407750Z","level":"error","event":"25/05/12 02:24:39 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:39.408476Z","level":"error","event":"25/05/12 02:24:39 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:39.480225Z","level":"error","event":"25/05/12 02:24:39 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:39.511212Z","level":"error","event":"25/05/12 02:24:39 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:39.512667Z","level":"error","event":"25/05/12 02:24:39 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:39.513475Z","level":"error","event":"25/05/12 02:24:39 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:39.547025Z","level":"error","event":"25/05/12 02:24:39 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:39.565663Z","level":"error","event":"25/05/12 02:24:39 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:39.579791Z","level":"error","event":"25/05/12 02:24:39 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:39.669688Z","level":"error","event":"25/05/12 02:24:39 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:39.688699Z","level":"error","event":"25/05/12 02:24:39 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:39.689559Z","level":"error","event":"25/05/12 02:24:39 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:39.690404Z","level":"error","event":"25/05/12 02:24:39 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:39.691192Z","level":"error","event":"25/05/12 02:24:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:40.242837Z","level":"error","event":"25/05/12 02:24:40 INFO Utils: Successfully started service 'sparkDriver' on port 38411.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:40.327960Z","level":"error","event":"25/05/12 02:24:40 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:40.425116Z","level":"error","event":"25/05/12 02:24:40 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:40.464126Z","level":"error","event":"25/05/12 02:24:40 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:40.494028Z","level":"error","event":"25/05/12 02:24:40 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:40.494689Z","level":"error","event":"25/05/12 02:24:40 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:40.528465Z","level":"error","event":"25/05/12 02:24:40 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7aa98239-3e4c-44e8-97d8-b5b56a0d6e05","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:40.560249Z","level":"error","event":"25/05/12 02:24:40 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:40.592419Z","level":"error","event":"25/05/12 02:24:40 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:40.846025Z","level":"error","event":"25/05/12 02:24:40 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:40.967291Z","level":"error","event":"25/05/12 02:24:40 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.048906Z","level":"error","event":"25/05/12 02:24:41 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.049589Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.050230Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.050838Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.051375Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.051803Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.052317Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.052783Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.053404Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.054041Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.054554Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.055086Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.055684Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.056212Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.056895Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.057627Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.058381Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.059075Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.059692Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.060284Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.061066Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.061778Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.062708Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.063400Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.083458Z","level":"error","event":"25/05/12 02:24:41 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.084098Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.084730Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.085288Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.085918Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.086587Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.087392Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.088128Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.088842Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.089440Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.090126Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.090762Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.091748Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.092476Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.093297Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.094127Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.094972Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.095913Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.096713Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.097435Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.098122Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.099003Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.099918Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.100887Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.101936Z","level":"error","event":"25/05/12 02:24:41 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.102701Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.103413Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.104073Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.104772Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.105445Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.106122Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.106807Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.107567Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.108352Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.109185Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.109908Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.110605Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.111284Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.112019Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.112707Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.113567Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.114446Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.115206Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.115871Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.116559Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.117308Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.118170Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.118983Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.119692Z","level":"error","event":"25/05/12 02:24:41 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.120424Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.121099Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.134649Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.135155Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.135659Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.136102Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.136691Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.137371Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.138090Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.138844Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.139656Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.140522Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.141300Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.142099Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.142840Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.143604Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.144296Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.145101Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.146474Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.147245Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.148123Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.148802Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.149485Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.183829Z","level":"error","event":"25/05/12 02:24:41 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.197060Z","level":"error","event":"25/05/12 02:24:41 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.197822Z","level":"error","event":"25/05/12 02:24:41 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.220725Z","level":"error","event":"25/05/12 02:24:41 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.221382Z","level":"error","event":"25/05/12 02:24:41 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2065ac4e for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.250582Z","level":"error","event":"25/05/12 02:24:41 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34957.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.262921Z","level":"error","event":"25/05/12 02:24:41 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:34957","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.263671Z","level":"error","event":"25/05/12 02:24:41 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.278305Z","level":"error","event":"25/05/12 02:24:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 34957, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.278987Z","level":"error","event":"25/05/12 02:24:41 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:34957 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 34957, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.279747Z","level":"error","event":"25/05/12 02:24:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 34957, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:41.280626Z","level":"error","event":"25/05/12 02:24:41 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 34957, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:42.009042Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:24:42.715641Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:24:43.715871Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:24:43.840147Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:24:44.840310Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:24:44.921484Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:24:45.921595Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:24:46.015606Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:24:47.015875Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:24:47.099628Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:24:48.099825Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:24:48.109837Z","level":"error","event":"25/05/12 02:24:48 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:48.110318Z","level":"error","event":"25/05/12 02:24:48 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:49.403342","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:24:49.913909Z","level":"error","event":"25/05/12 02:24:49 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:49.914534Z","level":"error","event":"25/05/12 02:24:49 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:49.927843Z","level":"error","event":"25/05/12 02:24:49 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:49.942298Z","level":"error","event":"25/05/12 02:24:49 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:49.958595Z","level":"error","event":"25/05/12 02:24:49 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:49.967494Z","level":"error","event":"25/05/12 02:24:49 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:49.967891Z","level":"error","event":"25/05/12 02:24:49 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:49.980812Z","level":"error","event":"25/05/12 02:24:49 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:49.993165Z","level":"error","event":"25/05/12 02:24:49 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:49.993718Z","level":"error","event":"25/05/12 02:24:49 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:49.994183Z","level":"error","event":"25/05/12 02:24:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-017929d6-e936-4e35-8abc-ab519d67b76e","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:50.005375Z","level":"error","event":"25/05/12 02:24:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-d96a8b10-3740-4e23-a714-c6962976a679/pyspark-30d929f4-5c56-4b7f-a715-59783812c926","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:24:50.005990Z","level":"error","event":"25/05/12 02:24:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-d96a8b10-3740-4e23-a714-c6962976a679","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:20.984435","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:25:30.417345Z","level":"error","event":"25/05/12 02:25:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:30.838657Z","level":"error","event":"25/05/12 02:25:30 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:30.858382Z","level":"error","event":"25/05/12 02:25:30 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:30.859286Z","level":"error","event":"25/05/12 02:25:30 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:30.860105Z","level":"error","event":"25/05/12 02:25:30 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:31.143474Z","level":"error","event":"25/05/12 02:25:31 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:31.167534Z","level":"error","event":"25/05/12 02:25:31 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:31.168510Z","level":"error","event":"25/05/12 02:25:31 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:31.230133Z","level":"error","event":"25/05/12 02:25:31 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:31.245322Z","level":"error","event":"25/05/12 02:25:31 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:31.246098Z","level":"error","event":"25/05/12 02:25:31 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:31.247016Z","level":"error","event":"25/05/12 02:25:31 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:31.298725Z","level":"error","event":"25/05/12 02:25:31 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:31.323038Z","level":"error","event":"25/05/12 02:25:31 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:31.383883Z","level":"error","event":"25/05/12 02:25:31 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:31.461573Z","level":"error","event":"25/05/12 02:25:31 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:31.485197Z","level":"error","event":"25/05/12 02:25:31 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:31.486578Z","level":"error","event":"25/05/12 02:25:31 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:31.487701Z","level":"error","event":"25/05/12 02:25:31 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:31.488642Z","level":"error","event":"25/05/12 02:25:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.002721Z","level":"error","event":"25/05/12 02:25:32 INFO Utils: Successfully started service 'sparkDriver' on port 43117.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.054020Z","level":"error","event":"25/05/12 02:25:32 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.114125Z","level":"error","event":"25/05/12 02:25:32 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.142384Z","level":"error","event":"25/05/12 02:25:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.159412Z","level":"error","event":"25/05/12 02:25:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.160114Z","level":"error","event":"25/05/12 02:25:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.187469Z","level":"error","event":"25/05/12 02:25:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0dedfd61-74c9-4a5e-be6a-72eb2174e49c","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.217328Z","level":"error","event":"25/05/12 02:25:32 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.247680Z","level":"error","event":"25/05/12 02:25:32 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.499281Z","level":"error","event":"25/05/12 02:25:32 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.679537Z","level":"error","event":"25/05/12 02:25:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.758788Z","level":"error","event":"25/05/12 02:25:32 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.759511Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.760134Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.760655Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.761238Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.761761Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.762270Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.763006Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.763673Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.764244Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.764876Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.765575Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.766291Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.767014Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.767857Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.768609Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.769224Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.769881Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.770501Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.771061Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.771540Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.771951Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.772467Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.772954Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.789854Z","level":"error","event":"25/05/12 02:25:32 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.790682Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.791354Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.792022Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.792585Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.793190Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.793992Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.794822Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.795634Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.796449Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.797329Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.798326Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.799638Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.800455Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.801447Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.802336Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.803243Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.804323Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.805151Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.806156Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.807119Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.807955Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.808858Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.809955Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.810807Z","level":"error","event":"25/05/12 02:25:32 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.811644Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.812445Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.813138Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.813750Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.814499Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.815359Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.816304Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.817157Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.818056Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.818988Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.819781Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.820723Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.821563Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.822372Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.823280Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.824137Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.825081Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.826001Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.826873Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.827621Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.828462Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.829337Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.830204Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.831016Z","level":"error","event":"25/05/12 02:25:32 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.831777Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.832637Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.849047Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.849643Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.850220Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.850722Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.851163Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.851651Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.852099Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.852566Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.853161Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.853678Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.854125Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.854661Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.855221Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.855735Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.856420Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.857227Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.857880Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.858576Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.859215Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.859815Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.860482Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.902563Z","level":"error","event":"25/05/12 02:25:32 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.921340Z","level":"error","event":"25/05/12 02:25:32 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.922077Z","level":"error","event":"25/05/12 02:25:32 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.940829Z","level":"error","event":"25/05/12 02:25:32 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.941988Z","level":"error","event":"25/05/12 02:25:32 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2065ac4e for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.980017Z","level":"error","event":"25/05/12 02:25:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41069.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.995277Z","level":"error","event":"25/05/12 02:25:32 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:41069","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:32.995921Z","level":"error","event":"25/05/12 02:25:32 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:33.012557Z","level":"error","event":"25/05/12 02:25:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 41069, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:33.013456Z","level":"error","event":"25/05/12 02:25:33 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:41069 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 41069, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:33.014230Z","level":"error","event":"25/05/12 02:25:33 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 41069, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:33.014969Z","level":"error","event":"25/05/12 02:25:33 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 41069, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:33.725241Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:25:34.486836Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:25:35.487184Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:25:35.606817Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:25:36.606879Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:25:36.731989Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:25:37.732127Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:25:37.844943Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:25:38.845105Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:25:38.925917Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:25:39.926244Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:25:39.941406Z","level":"error","event":"25/05/12 02:25:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:39.953775Z","level":"error","event":"25/05/12 02:25:39 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:41.874534","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:25:42.396631Z","level":"error","event":"25/05/12 02:25:42 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:42.397431Z","level":"error","event":"25/05/12 02:25:42 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:42.440366Z","level":"error","event":"25/05/12 02:25:42 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:42.488879Z","level":"error","event":"25/05/12 02:25:42 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:42.527040Z","level":"error","event":"25/05/12 02:25:42 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:42.549958Z","level":"error","event":"25/05/12 02:25:42 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:42.574465Z","level":"error","event":"25/05/12 02:25:42 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:42.576045Z","level":"error","event":"25/05/12 02:25:42 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:42.612426Z","level":"error","event":"25/05/12 02:25:42 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:42.642451Z","level":"error","event":"25/05/12 02:25:42 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:42.643571Z","level":"error","event":"25/05/12 02:25:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-267e9282-1ec6-4d17-acae-ac08e15ca071","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:42.644610Z","level":"error","event":"25/05/12 02:25:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-1d7227d5-b7d5-4865-8643-aa05ed998cf3/pyspark-e2bb5926-1ffc-41e5-a6dd-1b8c7afc29fa","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:25:42.678799Z","level":"error","event":"25/05/12 02:25:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-1d7227d5-b7d5-4865-8643-aa05ed998cf3","chan":"stderr","logger":"processor"}
