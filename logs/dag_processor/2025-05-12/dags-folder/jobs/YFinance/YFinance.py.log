{"timestamp":"2025-05-12T02:29:01.549973","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:29:05.111580Z","level":"error","event":"25/05/12 02:29:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:05.623454Z","level":"error","event":"25/05/12 02:29:05 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:05.651235Z","level":"error","event":"25/05/12 02:29:05 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:05.651826Z","level":"error","event":"25/05/12 02:29:05 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:05.652285Z","level":"error","event":"25/05/12 02:29:05 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:05.798736Z","level":"error","event":"25/05/12 02:29:05 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:05.811312Z","level":"error","event":"25/05/12 02:29:05 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:05.811753Z","level":"error","event":"25/05/12 02:29:05 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:05.844575Z","level":"error","event":"25/05/12 02:29:05 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:05.857093Z","level":"error","event":"25/05/12 02:29:05 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:05.857499Z","level":"error","event":"25/05/12 02:29:05 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:05.857850Z","level":"error","event":"25/05/12 02:29:05 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:05.884799Z","level":"error","event":"25/05/12 02:29:05 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:05.898935Z","level":"error","event":"25/05/12 02:29:05 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:05.910506Z","level":"error","event":"25/05/12 02:29:05 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:05.970705Z","level":"error","event":"25/05/12 02:29:05 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:05.985547Z","level":"error","event":"25/05/12 02:29:05 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:05.986176Z","level":"error","event":"25/05/12 02:29:05 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:05.986833Z","level":"error","event":"25/05/12 02:29:05 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:05.987640Z","level":"error","event":"25/05/12 02:29:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:06.364431Z","level":"error","event":"25/05/12 02:29:06 INFO Utils: Successfully started service 'sparkDriver' on port 33607.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:06.459833Z","level":"error","event":"25/05/12 02:29:06 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:06.592517Z","level":"error","event":"25/05/12 02:29:06 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:06.662038Z","level":"error","event":"25/05/12 02:29:06 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:06.663014Z","level":"error","event":"25/05/12 02:29:06 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:06.684490Z","level":"error","event":"25/05/12 02:29:06 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:06.733028Z","level":"error","event":"25/05/12 02:29:06 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-822d3137-1571-4815-aced-db2ca09ebba8","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:06.762387Z","level":"error","event":"25/05/12 02:29:06 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:06.872514Z","level":"error","event":"25/05/12 02:29:06 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.233022Z","level":"error","event":"25/05/12 02:29:07 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.320108Z","level":"error","event":"25/05/12 02:29:07 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.405000Z","level":"error","event":"25/05/12 02:29:07 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.405621Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.406113Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.406784Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.407375Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.407895Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.408569Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.409176Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.409741Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.410374Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.410925Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.411565Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.412298Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.413134Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.413823Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.414473Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.415643Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.416568Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.417606Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.418630Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.419472Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.420354Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.421063Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.421846Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.444247Z","level":"error","event":"25/05/12 02:29:07 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.445126Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.445956Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.446686Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.447523Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.451058Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.451845Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.452541Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.453146Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.453957Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.454733Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.455535Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.456241Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.456897Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.457553Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.458214Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.458883Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.459677Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.460271Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.460903Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.461642Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.462354Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.463059Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.464435Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.465229Z","level":"error","event":"25/05/12 02:29:07 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.465976Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.466678Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.467610Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.468738Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.469696Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.470431Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.471345Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.471949Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.472709Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.473404Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.474135Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.474835Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.475542Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.476284Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.476978Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.477698Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.478307Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.479075Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.479852Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.480473Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.481080Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.482542Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.483158Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.483702Z","level":"error","event":"25/05/12 02:29:07 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.484264Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.484828Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.500605Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.501226Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.501754Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.502280Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.502851Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.503427Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.504018Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.504592Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.505290Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.505837Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.506264Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.506677Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.507036Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.507543Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.508078Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.508567Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.509106Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.509580Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.510009Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.510529Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.511223Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.556979Z","level":"error","event":"25/05/12 02:29:07 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.571314Z","level":"error","event":"25/05/12 02:29:07 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.572027Z","level":"error","event":"25/05/12 02:29:07 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.587146Z","level":"error","event":"25/05/12 02:29:07 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.587885Z","level":"error","event":"25/05/12 02:29:07 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@67055e93 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.626921Z","level":"error","event":"25/05/12 02:29:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36879.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.643997Z","level":"error","event":"25/05/12 02:29:07 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:36879","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.644673Z","level":"error","event":"25/05/12 02:29:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.662247Z","level":"error","event":"25/05/12 02:29:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 36879, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.662991Z","level":"error","event":"25/05/12 02:29:07 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:36879 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 36879, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.663875Z","level":"error","event":"25/05/12 02:29:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 36879, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:07.664478Z","level":"error","event":"25/05/12 02:29:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 36879, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:08.441776Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:29:09.406238Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:29:10.406395Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:29:10.517517Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:29:25.744614","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:29:30.073842Z","level":"error","event":"25/05/12 02:29:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:30.635330Z","level":"error","event":"25/05/12 02:29:30 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:30.661793Z","level":"error","event":"25/05/12 02:29:30 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:30.662636Z","level":"error","event":"25/05/12 02:29:30 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:30.663615Z","level":"error","event":"25/05/12 02:29:30 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:30.934540Z","level":"error","event":"25/05/12 02:29:30 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:30.955107Z","level":"error","event":"25/05/12 02:29:30 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:30.955985Z","level":"error","event":"25/05/12 02:29:30 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:31.025269Z","level":"error","event":"25/05/12 02:29:31 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:31.048888Z","level":"error","event":"25/05/12 02:29:31 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:31.049892Z","level":"error","event":"25/05/12 02:29:31 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:31.050706Z","level":"error","event":"25/05/12 02:29:31 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:31.097081Z","level":"error","event":"25/05/12 02:29:31 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:31.119193Z","level":"error","event":"25/05/12 02:29:31 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:31.137500Z","level":"error","event":"25/05/12 02:29:31 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:31.212777Z","level":"error","event":"25/05/12 02:29:31 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:31.232197Z","level":"error","event":"25/05/12 02:29:31 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:31.233014Z","level":"error","event":"25/05/12 02:29:31 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:31.233847Z","level":"error","event":"25/05/12 02:29:31 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:31.234700Z","level":"error","event":"25/05/12 02:29:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:31.915281Z","level":"error","event":"25/05/12 02:29:31 INFO Utils: Successfully started service 'sparkDriver' on port 45587.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:31.987720Z","level":"error","event":"25/05/12 02:29:31 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.070824Z","level":"error","event":"25/05/12 02:29:32 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.117711Z","level":"error","event":"25/05/12 02:29:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.140130Z","level":"error","event":"25/05/12 02:29:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.140974Z","level":"error","event":"25/05/12 02:29:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.214243Z","level":"error","event":"25/05/12 02:29:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5fe48f4c-d7c2-4d1d-aa5f-d8f297a9ffb3","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.265262Z","level":"error","event":"25/05/12 02:29:32 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.332006Z","level":"error","event":"25/05/12 02:29:32 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.683287Z","level":"error","event":"25/05/12 02:29:32 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.859703Z","level":"error","event":"25/05/12 02:29:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.962273Z","level":"error","event":"25/05/12 02:29:32 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.962889Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.963618Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.964244Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.964903Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.965490Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.966016Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.966588Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.967174Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.967850Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.968479Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.969058Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.969758Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.970713Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.971622Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.972409Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.973213Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.973919Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.974651Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.975308Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.975957Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.976743Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.977461Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.978070Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.997030Z","level":"error","event":"25/05/12 02:29:32 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.997883Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.998672Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:32.999747Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.001544Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.002553Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.003504Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.004189Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.004762Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.005388Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.006002Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.006774Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.007750Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.008655Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.009616Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.010381Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.011079Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.011779Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.012511Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.013909Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.014873Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.015661Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.016705Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.017432Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.018235Z","level":"error","event":"25/05/12 02:29:32 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.019046Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.019764Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.020401Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.020991Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.021513Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.022080Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.022663Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.023637Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.024592Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.025443Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.026192Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.027352Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.028259Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.029426Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.030316Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.031130Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.032026Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.032962Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.033892Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.034713Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.035491Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.036245Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.036931Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.037621Z","level":"error","event":"25/05/12 02:29:32 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.038258Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.038878Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.060525Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.062157Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.063129Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.063894Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.064679Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.065515Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.066979Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.068026Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.068955Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.070620Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.071318Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.071961Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.072946Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.073872Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.074944Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.075889Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.076729Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.077691Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.078777Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.080026Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.080804Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.111048Z","level":"error","event":"25/05/12 02:29:33 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.128065Z","level":"error","event":"25/05/12 02:29:33 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.128812Z","level":"error","event":"25/05/12 02:29:33 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.146059Z","level":"error","event":"25/05/12 02:29:33 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.146872Z","level":"error","event":"25/05/12 02:29:33 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1f04a8db for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.176328Z","level":"error","event":"25/05/12 02:29:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37745.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.191439Z","level":"error","event":"25/05/12 02:29:33 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:37745","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.192090Z","level":"error","event":"25/05/12 02:29:33 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.192701Z","level":"error","event":"25/05/12 02:29:33 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 37745, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.209923Z","level":"error","event":"25/05/12 02:29:33 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:37745 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 37745, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.210609Z","level":"error","event":"25/05/12 02:29:33 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 37745, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.211355Z","level":"error","event":"25/05/12 02:29:33 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 37745, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:33.963287Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:29:34.777163Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:29:35.777349Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:29:35.880313Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:29:36.880524Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:29:36.957215Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:29:37.957387Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:29:38.053500Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:29:39.053714Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:29:39.135159Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:29:40.135292Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:29:40.144824Z","level":"error","event":"25/05/12 02:29:40 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:40.145290Z","level":"error","event":"25/05/12 02:29:40 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:41.371791","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:29:41.832285Z","level":"error","event":"25/05/12 02:29:41 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:41.832744Z","level":"error","event":"25/05/12 02:29:41 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:41.842557Z","level":"error","event":"25/05/12 02:29:41 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:41.852715Z","level":"error","event":"25/05/12 02:29:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:41.866201Z","level":"error","event":"25/05/12 02:29:41 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:41.874128Z","level":"error","event":"25/05/12 02:29:41 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:41.874762Z","level":"error","event":"25/05/12 02:29:41 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:41.883408Z","level":"error","event":"25/05/12 02:29:41 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:41.891633Z","level":"error","event":"25/05/12 02:29:41 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:41.891953Z","level":"error","event":"25/05/12 02:29:41 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:41.892215Z","level":"error","event":"25/05/12 02:29:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-133e9c37-af6a-4e93-a305-833478e5deda","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:41.892462Z","level":"error","event":"25/05/12 02:29:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-7c55f8fd-9ba3-4356-901d-f781afa7a0f3","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:29:41.900581Z","level":"error","event":"25/05/12 02:29:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-7c55f8fd-9ba3-4356-901d-f781afa7a0f3/pyspark-b6d893d1-7f78-4ea9-bb31-7b73083454f9","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:12.938537","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:30:19.400011Z","level":"error","event":"25/05/12 02:30:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:19.650983Z","level":"error","event":"25/05/12 02:30:19 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:19.665159Z","level":"error","event":"25/05/12 02:30:19 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:19.665770Z","level":"error","event":"25/05/12 02:30:19 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:19.666426Z","level":"error","event":"25/05/12 02:30:19 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:19.819149Z","level":"error","event":"25/05/12 02:30:19 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:19.834164Z","level":"error","event":"25/05/12 02:30:19 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:19.834732Z","level":"error","event":"25/05/12 02:30:19 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:19.863985Z","level":"error","event":"25/05/12 02:30:19 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:19.880326Z","level":"error","event":"25/05/12 02:30:19 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:19.881015Z","level":"error","event":"25/05/12 02:30:19 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:19.881602Z","level":"error","event":"25/05/12 02:30:19 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:19.909518Z","level":"error","event":"25/05/12 02:30:19 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:19.923033Z","level":"error","event":"25/05/12 02:30:19 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:19.935435Z","level":"error","event":"25/05/12 02:30:19 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:19.990511Z","level":"error","event":"25/05/12 02:30:19 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.003465Z","level":"error","event":"25/05/12 02:30:19 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.003994Z","level":"error","event":"25/05/12 02:30:19 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.004539Z","level":"error","event":"25/05/12 02:30:19 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.004986Z","level":"error","event":"25/05/12 02:30:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.290357Z","level":"error","event":"25/05/12 02:30:20 INFO Utils: Successfully started service 'sparkDriver' on port 44117.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.337667Z","level":"error","event":"25/05/12 02:30:20 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.394644Z","level":"error","event":"25/05/12 02:30:20 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.422600Z","level":"error","event":"25/05/12 02:30:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.437917Z","level":"error","event":"25/05/12 02:30:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.438636Z","level":"error","event":"25/05/12 02:30:20 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.470178Z","level":"error","event":"25/05/12 02:30:20 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7ef325c2-05ec-46ea-8a44-0858a6d6dedf","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.495407Z","level":"error","event":"25/05/12 02:30:20 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.521845Z","level":"error","event":"25/05/12 02:30:20 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.726349Z","level":"error","event":"25/05/12 02:30:20 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.840072Z","level":"error","event":"25/05/12 02:30:20 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.907890Z","level":"error","event":"25/05/12 02:30:20 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.908793Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.909359Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.909952Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.910661Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.911326Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.911946Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.912551Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.913343Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.913900Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.914405Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.915023Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.915610Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.916324Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.916897Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.917515Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.918204Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.918998Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.920233Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.921424Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.922394Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.922979Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.923514Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.924112Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.974575Z","level":"error","event":"25/05/12 02:30:20 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.983539Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.989020Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.991312Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.993249Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.994001Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.994793Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.995580Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.996543Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:20.997844Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.002892Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.010413Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.012243Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.014128Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.015379Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.016381Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.017470Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.029667Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.031181Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.032161Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.033050Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.034300Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.036487Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.038536Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.040640Z","level":"error","event":"25/05/12 02:30:20 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.042052Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.045513Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.047444Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.048669Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.052209Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.054663Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.055872Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.059863Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.060733Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.061667Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.062616Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.065849Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.066876Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.067603Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.068498Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.069354Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.070058Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.070714Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.071472Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.072135Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.072824Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.074047Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.076806Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.077938Z","level":"error","event":"25/05/12 02:30:20 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.078889Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.079857Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.109440Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.112059Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.112873Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.113511Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.114182Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.114792Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.115448Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.116325Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.117239Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.117914Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.118768Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.119592Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.120626Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.121565Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.122962Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.123789Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.124526Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.126150Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.127627Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.128241Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.128824Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.147858Z","level":"error","event":"25/05/12 02:30:21 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.170680Z","level":"error","event":"25/05/12 02:30:21 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.171690Z","level":"error","event":"25/05/12 02:30:21 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.200061Z","level":"error","event":"25/05/12 02:30:21 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.200989Z","level":"error","event":"25/05/12 02:30:21 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6ff21454 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.259154Z","level":"error","event":"25/05/12 02:30:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37787.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.284238Z","level":"error","event":"25/05/12 02:30:21 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:37787","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.285064Z","level":"error","event":"25/05/12 02:30:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.315290Z","level":"error","event":"25/05/12 02:30:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 37787, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.316165Z","level":"error","event":"25/05/12 02:30:21 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:37787 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 37787, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.317052Z","level":"error","event":"25/05/12 02:30:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 37787, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:21.317953Z","level":"error","event":"25/05/12 02:30:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 37787, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:22.017853Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:30:22.722077Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:30:23.722350Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:30:23.838669Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:30:24.838813Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:30:24.908599Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:30:25.908720Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:30:26.003052Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:30:27.003317Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:30:27.237065Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:30:28.108839Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:30:28.121524Z","level":"error","event":"25/05/12 02:30:28 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:28.134159Z","level":"error","event":"25/05/12 02:30:28 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:31.206954","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:30:31.753473Z","level":"error","event":"25/05/12 02:30:31 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:31.754473Z","level":"error","event":"25/05/12 02:30:31 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:31.771077Z","level":"error","event":"25/05/12 02:30:31 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:31.803130Z","level":"error","event":"25/05/12 02:30:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:31.822324Z","level":"error","event":"25/05/12 02:30:31 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:31.833350Z","level":"error","event":"25/05/12 02:30:31 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:31.833874Z","level":"error","event":"25/05/12 02:30:31 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:31.847752Z","level":"error","event":"25/05/12 02:30:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:31.860721Z","level":"error","event":"25/05/12 02:30:31 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:31.861194Z","level":"error","event":"25/05/12 02:30:31 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:31.861628Z","level":"error","event":"25/05/12 02:30:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-6a39b53f-edc0-4ed1-93b6-b79253eb2540/pyspark-6c6ed8cc-7640-43d5-8cf3-2b50711e586b","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:31.874135Z","level":"error","event":"25/05/12 02:30:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-6a39b53f-edc0-4ed1-93b6-b79253eb2540","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:30:31.874587Z","level":"error","event":"25/05/12 02:30:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-52e86467-6396-4a51-8e4c-a1596082bca6","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:02.805219","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:31:08.463871Z","level":"error","event":"25/05/12 02:31:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:08.849056Z","level":"error","event":"25/05/12 02:31:08 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:08.866173Z","level":"error","event":"25/05/12 02:31:08 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:08.866683Z","level":"error","event":"25/05/12 02:31:08 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:08.867117Z","level":"error","event":"25/05/12 02:31:08 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:09.119898Z","level":"error","event":"25/05/12 02:31:09 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:09.158343Z","level":"error","event":"25/05/12 02:31:09 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:09.159815Z","level":"error","event":"25/05/12 02:31:09 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:09.239852Z","level":"error","event":"25/05/12 02:31:09 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:09.267734Z","level":"error","event":"25/05/12 02:31:09 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:09.268632Z","level":"error","event":"25/05/12 02:31:09 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:09.272645Z","level":"error","event":"25/05/12 02:31:09 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:09.338737Z","level":"error","event":"25/05/12 02:31:09 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:09.395687Z","level":"error","event":"25/05/12 02:31:09 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:09.399290Z","level":"error","event":"25/05/12 02:31:09 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:09.655216Z","level":"error","event":"25/05/12 02:31:09 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:09.900992Z","level":"error","event":"25/05/12 02:31:09 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:09.910526Z","level":"error","event":"25/05/12 02:31:09 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:09.986668Z","level":"error","event":"25/05/12 02:31:09 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:10.016544Z","level":"error","event":"25/05/12 02:31:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:10.540952Z","level":"error","event":"25/05/12 02:31:10 INFO Utils: Successfully started service 'sparkDriver' on port 43647.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:10.604362Z","level":"error","event":"25/05/12 02:31:10 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:10.677943Z","level":"error","event":"25/05/12 02:31:10 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:10.718321Z","level":"error","event":"25/05/12 02:31:10 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:10.741094Z","level":"error","event":"25/05/12 02:31:10 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:10.742089Z","level":"error","event":"25/05/12 02:31:10 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:10.779035Z","level":"error","event":"25/05/12 02:31:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-de438d67-acf4-4b67-94a7-68546c8d297a","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:10.813243Z","level":"error","event":"25/05/12 02:31:10 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:10.851236Z","level":"error","event":"25/05/12 02:31:10 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.151451Z","level":"error","event":"25/05/12 02:31:11 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.325117Z","level":"error","event":"25/05/12 02:31:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.467192Z","level":"error","event":"25/05/12 02:31:11 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.468763Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.469768Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.473657Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.474812Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.475762Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.476843Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.478536Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.481231Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.486188Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.489200Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.490409Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.491478Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.493479Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.495024Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.496023Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.497075Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.498190Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.501880Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.505069Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.506236Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.507208Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.508207Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.509518Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.563744Z","level":"error","event":"25/05/12 02:31:11 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.580591Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.582715Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.588691Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.594033Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.595889Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.597102Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.598361Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.599513Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.600605Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.601841Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.609623Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.613470Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.615193Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.620073Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.621292Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.626856Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.629556Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.632851Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.633861Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.634834Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.636411Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.637648Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.638550Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.639457Z","level":"error","event":"25/05/12 02:31:11 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.640542Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.641406Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.642906Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.643954Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.644872Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.645898Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.646768Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.647660Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.648478Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.649292Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.650094Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.651082Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.651841Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.652516Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.653598Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.654454Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.655252Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.656051Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.656848Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.658541Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.659369Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.660514Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.661402Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.662186Z","level":"error","event":"25/05/12 02:31:11 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.663220Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.664057Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.691269Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.692127Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.692955Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.693901Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.694852Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.695669Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.696701Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.697531Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.698297Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.699055Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.699860Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.700765Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.701584Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.702394Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.703252Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.704236Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.705157Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.706834Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.707758Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.709009Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.710129Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.735884Z","level":"error","event":"25/05/12 02:31:11 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.754105Z","level":"error","event":"25/05/12 02:31:11 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.754800Z","level":"error","event":"25/05/12 02:31:11 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.770271Z","level":"error","event":"25/05/12 02:31:11 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.771179Z","level":"error","event":"25/05/12 02:31:11 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@5c48057c for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.805028Z","level":"error","event":"25/05/12 02:31:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41203.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.818574Z","level":"error","event":"25/05/12 02:31:11 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:41203","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.819122Z","level":"error","event":"25/05/12 02:31:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.819616Z","level":"error","event":"25/05/12 02:31:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 41203, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.835418Z","level":"error","event":"25/05/12 02:31:11 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:41203 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 41203, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.836270Z","level":"error","event":"25/05/12 02:31:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 41203, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:11.836885Z","level":"error","event":"25/05/12 02:31:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 41203, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:12.592395Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:31:13.370241Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:31:14.370406Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:31:14.537690Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:31:15.537841Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:31:15.629248Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:31:16.629412Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:31:16.738852Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:31:17.739331Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:31:17.843918Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:31:18.844083Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:31:18.855977Z","level":"error","event":"25/05/12 02:31:18 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:18.856444Z","level":"error","event":"25/05/12 02:31:18 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:22.507206","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:31:22.889405Z","level":"error","event":"25/05/12 02:31:22 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:22.890250Z","level":"error","event":"25/05/12 02:31:22 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:22.907736Z","level":"error","event":"25/05/12 02:31:22 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:22.923308Z","level":"error","event":"25/05/12 02:31:22 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:22.949109Z","level":"error","event":"25/05/12 02:31:22 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:22.963559Z","level":"error","event":"25/05/12 02:31:22 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:22.964201Z","level":"error","event":"25/05/12 02:31:22 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:22.964617Z","level":"error","event":"25/05/12 02:31:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:22.976210Z","level":"error","event":"25/05/12 02:31:22 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:22.986291Z","level":"error","event":"25/05/12 02:31:22 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:22.987022Z","level":"error","event":"25/05/12 02:31:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-e8b479e6-7da2-4683-8110-8bdc5feecd3a","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:22.987443Z","level":"error","event":"25/05/12 02:31:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-c6be2345-332b-4c53-82b3-363c36785785","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:22.997646Z","level":"error","event":"25/05/12 02:31:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-c6be2345-332b-4c53-82b3-363c36785785/pyspark-88d11f22-94f1-4d05-b388-48b0f57ac545","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:53.715934","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:31:58.764676Z","level":"error","event":"25/05/12 02:31:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:59.369510Z","level":"error","event":"25/05/12 02:31:59 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:59.393697Z","level":"error","event":"25/05/12 02:31:59 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:59.394448Z","level":"error","event":"25/05/12 02:31:59 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:59.395246Z","level":"error","event":"25/05/12 02:31:59 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:59.577580Z","level":"error","event":"25/05/12 02:31:59 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:59.601798Z","level":"error","event":"25/05/12 02:31:59 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:59.602856Z","level":"error","event":"25/05/12 02:31:59 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:59.672860Z","level":"error","event":"25/05/12 02:31:59 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:59.692738Z","level":"error","event":"25/05/12 02:31:59 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:59.693401Z","level":"error","event":"25/05/12 02:31:59 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:59.694246Z","level":"error","event":"25/05/12 02:31:59 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:59.733003Z","level":"error","event":"25/05/12 02:31:59 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:59.762897Z","level":"error","event":"25/05/12 02:31:59 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:59.784393Z","level":"error","event":"25/05/12 02:31:59 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:59.896677Z","level":"error","event":"25/05/12 02:31:59 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:59.917331Z","level":"error","event":"25/05/12 02:31:59 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:59.918052Z","level":"error","event":"25/05/12 02:31:59 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:59.918617Z","level":"error","event":"25/05/12 02:31:59 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:31:59.919360Z","level":"error","event":"25/05/12 02:31:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:00.417925Z","level":"error","event":"25/05/12 02:32:00 INFO Utils: Successfully started service 'sparkDriver' on port 40069.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:00.512312Z","level":"error","event":"25/05/12 02:32:00 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:00.578889Z","level":"error","event":"25/05/12 02:32:00 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:00.605467Z","level":"error","event":"25/05/12 02:32:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:00.621427Z","level":"error","event":"25/05/12 02:32:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:00.621935Z","level":"error","event":"25/05/12 02:32:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:00.653969Z","level":"error","event":"25/05/12 02:32:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ac0ff57f-5b13-4f24-b7c5-de6214c1937d","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:00.683465Z","level":"error","event":"25/05/12 02:32:00 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:00.718723Z","level":"error","event":"25/05/12 02:32:00 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:00.932473Z","level":"error","event":"25/05/12 02:32:00 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.053258Z","level":"error","event":"25/05/12 02:32:01 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.128017Z","level":"error","event":"25/05/12 02:32:01 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.128695Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.129346Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.129878Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.130356Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.130892Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.131413Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.131963Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.132622Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.133222Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.133758Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.134322Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.134848Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.135428Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.136137Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.136776Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.137359Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.137987Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.138622Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.139171Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.139778Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.140273Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.140767Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.141366Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.156089Z","level":"error","event":"25/05/12 02:32:01 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.156858Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.157724Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.158385Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.159035Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.159851Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.160637Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.161482Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.162378Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.163126Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.164074Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.164659Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.165149Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.165697Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.166189Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.166770Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.167288Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.167747Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.168188Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.168606Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.169139Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.169686Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.170390Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.171277Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.172149Z","level":"error","event":"25/05/12 02:32:01 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.173039Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.173899Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.175042Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.176090Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.176952Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.178060Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.178908Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.179759Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.180544Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.181666Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.182674Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.183509Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.184291Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.185059Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.185912Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.186717Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.187490Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.188264Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.189183Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.190013Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.190888Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.191739Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.192668Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.193488Z","level":"error","event":"25/05/12 02:32:01 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.194161Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.194694Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.207680Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.208242Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.208695Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.209107Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.209560Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.209996Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.210418Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.210834Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.211305Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.211698Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.212130Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.212532Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.212956Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.213370Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.213901Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.214387Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.214858Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.215337Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.215834Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.216486Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.217030Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.262245Z","level":"error","event":"25/05/12 02:32:01 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.275495Z","level":"error","event":"25/05/12 02:32:01 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.276113Z","level":"error","event":"25/05/12 02:32:01 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.290125Z","level":"error","event":"25/05/12 02:32:01 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.290730Z","level":"error","event":"25/05/12 02:32:01 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@4ee52be2 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.318768Z","level":"error","event":"25/05/12 02:32:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36939.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.332049Z","level":"error","event":"25/05/12 02:32:01 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:36939","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.332737Z","level":"error","event":"25/05/12 02:32:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.347138Z","level":"error","event":"25/05/12 02:32:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 36939, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.347710Z","level":"error","event":"25/05/12 02:32:01 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:36939 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 36939, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.348342Z","level":"error","event":"25/05/12 02:32:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 36939, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.348913Z","level":"error","event":"25/05/12 02:32:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 36939, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:01.863078Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:32:02.594202Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:32:03.594383Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:32:03.698465Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:32:04.698600Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:32:04.785973Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:32:05.786091Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:32:05.905437Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:32:06.905520Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:32:06.993299Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:32:07.993424Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:32:08.005481Z","level":"error","event":"25/05/12 02:32:08 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:08.021074Z","level":"error","event":"25/05/12 02:32:08 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:09.927336","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:32:10.439695Z","level":"error","event":"25/05/12 02:32:10 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:10.440181Z","level":"error","event":"25/05/12 02:32:10 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:10.454929Z","level":"error","event":"25/05/12 02:32:10 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:10.472602Z","level":"error","event":"25/05/12 02:32:10 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:10.490507Z","level":"error","event":"25/05/12 02:32:10 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:10.505144Z","level":"error","event":"25/05/12 02:32:10 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:10.506002Z","level":"error","event":"25/05/12 02:32:10 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:10.506833Z","level":"error","event":"25/05/12 02:32:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:10.522622Z","level":"error","event":"25/05/12 02:32:10 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:10.535786Z","level":"error","event":"25/05/12 02:32:10 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:10.536538Z","level":"error","event":"25/05/12 02:32:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-9b123fb0-10f3-4b6e-b7a8-f3f78122bc6d","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:10.537163Z","level":"error","event":"25/05/12 02:32:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-da1c58ee-6c3e-476c-bfa3-e23b0f94fb17/pyspark-763d3fee-5b98-4a30-a76a-4b3aaf50bb02","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:10.548734Z","level":"error","event":"25/05/12 02:32:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-da1c58ee-6c3e-476c-bfa3-e23b0f94fb17","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:41.669653","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:32:45.934919Z","level":"error","event":"25/05/12 02:32:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:46.264121Z","level":"error","event":"25/05/12 02:32:46 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:46.284273Z","level":"error","event":"25/05/12 02:32:46 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:46.284959Z","level":"error","event":"25/05/12 02:32:46 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:46.285579Z","level":"error","event":"25/05/12 02:32:46 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:46.458875Z","level":"error","event":"25/05/12 02:32:46 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:46.473831Z","level":"error","event":"25/05/12 02:32:46 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:46.474531Z","level":"error","event":"25/05/12 02:32:46 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:46.518485Z","level":"error","event":"25/05/12 02:32:46 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:46.534407Z","level":"error","event":"25/05/12 02:32:46 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:46.535044Z","level":"error","event":"25/05/12 02:32:46 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:46.535563Z","level":"error","event":"25/05/12 02:32:46 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:46.570293Z","level":"error","event":"25/05/12 02:32:46 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:46.592455Z","level":"error","event":"25/05/12 02:32:46 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:46.593239Z","level":"error","event":"25/05/12 02:32:46 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:46.663353Z","level":"error","event":"25/05/12 02:32:46 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:46.678646Z","level":"error","event":"25/05/12 02:32:46 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:46.679379Z","level":"error","event":"25/05/12 02:32:46 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:46.680080Z","level":"error","event":"25/05/12 02:32:46 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:46.680781Z","level":"error","event":"25/05/12 02:32:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:46.981385Z","level":"error","event":"25/05/12 02:32:46 INFO Utils: Successfully started service 'sparkDriver' on port 38379.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.032797Z","level":"error","event":"25/05/12 02:32:47 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.083760Z","level":"error","event":"25/05/12 02:32:47 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.174106Z","level":"error","event":"25/05/12 02:32:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.174974Z","level":"error","event":"25/05/12 02:32:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.175996Z","level":"error","event":"25/05/12 02:32:47 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.226360Z","level":"error","event":"25/05/12 02:32:47 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-449eb8dc-c5ea-4e72-9ae6-491c70d88745","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.264124Z","level":"error","event":"25/05/12 02:32:47 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.298064Z","level":"error","event":"25/05/12 02:32:47 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.584721Z","level":"error","event":"25/05/12 02:32:47 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.714221Z","level":"error","event":"25/05/12 02:32:47 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.797095Z","level":"error","event":"25/05/12 02:32:47 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.797792Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.798482Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.799204Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.799932Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.800627Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.801195Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.801786Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.802306Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.802785Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.803338Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.803945Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.804739Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.805739Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.807137Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.808063Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.808967Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.809815Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.810743Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.811639Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.812442Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.813145Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.813885Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.814476Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.845027Z","level":"error","event":"25/05/12 02:32:47 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.846003Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.846815Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.847591Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.848362Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.849392Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.850741Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.851958Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.856199Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.858124Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.859461Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.860597Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.861403Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.862264Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.863170Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.864225Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.865128Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.865915Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.866700Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.867453Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.868249Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.869041Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.869872Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.871115Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.872080Z","level":"error","event":"25/05/12 02:32:47 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.872902Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.874025Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.874871Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.876071Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.877262Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.878111Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.878771Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.879740Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.880386Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.880917Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.881439Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.882090Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.882919Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.883910Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.884789Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.885698Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.886707Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.887588Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.888670Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.889709Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.890661Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.891577Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.892376Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.893488Z","level":"error","event":"25/05/12 02:32:47 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.894231Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.895021Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.910905Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.911692Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.912286Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.912883Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.913392Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.913870Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.914364Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.915049Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.915647Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.916451Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.917241Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.918018Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.918883Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.919651Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.920399Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.920954Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.921543Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.922189Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.922772Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.923412Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.924133Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.952043Z","level":"error","event":"25/05/12 02:32:47 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.964372Z","level":"error","event":"25/05/12 02:32:47 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.965025Z","level":"error","event":"25/05/12 02:32:47 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.965613Z","level":"error","event":"25/05/12 02:32:47 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.976290Z","level":"error","event":"25/05/12 02:32:47 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@73663445 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:47.994951Z","level":"error","event":"25/05/12 02:32:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46781.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:48.004571Z","level":"error","event":"25/05/12 02:32:47 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:46781","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:48.004981Z","level":"error","event":"25/05/12 02:32:47 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:48.005337Z","level":"error","event":"25/05/12 02:32:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 46781, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:48.019490Z","level":"error","event":"25/05/12 02:32:48 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:46781 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 46781, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:48.020048Z","level":"error","event":"25/05/12 02:32:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 46781, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:48.020445Z","level":"error","event":"25/05/12 02:32:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 46781, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:48.486568Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:32:49.204535Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:32:50.204757Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:32:50.304968Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:32:51.305112Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:32:51.411638Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:32:52.411960Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:32:52.536582Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:32:53.537006Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:32:53.625249Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:32:54.625388Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:32:54.642360Z","level":"error","event":"25/05/12 02:32:54 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:54.662480Z","level":"error","event":"25/05/12 02:32:54 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:32:59.543601","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:33:00.008876Z","level":"error","event":"25/05/12 02:33:00 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:00.009651Z","level":"error","event":"25/05/12 02:33:00 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:00.023639Z","level":"error","event":"25/05/12 02:33:00 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:00.045525Z","level":"error","event":"25/05/12 02:33:00 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:00.066745Z","level":"error","event":"25/05/12 02:33:00 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:00.081703Z","level":"error","event":"25/05/12 02:33:00 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:00.082599Z","level":"error","event":"25/05/12 02:33:00 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:00.083243Z","level":"error","event":"25/05/12 02:33:00 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:00.097881Z","level":"error","event":"25/05/12 02:33:00 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:00.108826Z","level":"error","event":"25/05/12 02:33:00 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:00.109313Z","level":"error","event":"25/05/12 02:33:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-26a74641-1309-4fed-8777-a8ec4700502c","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:00.123486Z","level":"error","event":"25/05/12 02:33:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-27ea0fbf-5a57-4efc-a61b-89a5cf9bf20e","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:00.140186Z","level":"error","event":"25/05/12 02:33:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-26a74641-1309-4fed-8777-a8ec4700502c/pyspark-ac3be1c2-a71b-467c-a8be-a729a653081c","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:31.249819","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:33:36.634912Z","level":"error","event":"25/05/12 02:33:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:37.039227Z","level":"error","event":"25/05/12 02:33:37 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:37.062201Z","level":"error","event":"25/05/12 02:33:37 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:37.063099Z","level":"error","event":"25/05/12 02:33:37 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:37.063853Z","level":"error","event":"25/05/12 02:33:37 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:37.257931Z","level":"error","event":"25/05/12 02:33:37 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:37.277392Z","level":"error","event":"25/05/12 02:33:37 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:37.278171Z","level":"error","event":"25/05/12 02:33:37 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:37.330427Z","level":"error","event":"25/05/12 02:33:37 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:37.349334Z","level":"error","event":"25/05/12 02:33:37 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:37.350291Z","level":"error","event":"25/05/12 02:33:37 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:37.351258Z","level":"error","event":"25/05/12 02:33:37 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:37.402224Z","level":"error","event":"25/05/12 02:33:37 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:37.423046Z","level":"error","event":"25/05/12 02:33:37 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:37.441114Z","level":"error","event":"25/05/12 02:33:37 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:37.505712Z","level":"error","event":"25/05/12 02:33:37 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:37.522913Z","level":"error","event":"25/05/12 02:33:37 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:37.523756Z","level":"error","event":"25/05/12 02:33:37 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:37.524474Z","level":"error","event":"25/05/12 02:33:37 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:37.525256Z","level":"error","event":"25/05/12 02:33:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:37.955617Z","level":"error","event":"25/05/12 02:33:37 INFO Utils: Successfully started service 'sparkDriver' on port 44221.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.012013Z","level":"error","event":"25/05/12 02:33:38 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.074553Z","level":"error","event":"25/05/12 02:33:38 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.104342Z","level":"error","event":"25/05/12 02:33:38 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.122357Z","level":"error","event":"25/05/12 02:33:38 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.122913Z","level":"error","event":"25/05/12 02:33:38 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.160090Z","level":"error","event":"25/05/12 02:33:38 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2d4c9948-2672-450d-a326-205abadc46c0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.207361Z","level":"error","event":"25/05/12 02:33:38 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.237854Z","level":"error","event":"25/05/12 02:33:38 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.453862Z","level":"error","event":"25/05/12 02:33:38 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.539775Z","level":"error","event":"25/05/12 02:33:38 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.600972Z","level":"error","event":"25/05/12 02:33:38 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.601642Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.602190Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.602871Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.603490Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.603876Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.604270Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.604700Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.605359Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.605996Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.606526Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.607259Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.608050Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.608886Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.609661Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.610411Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.611128Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.611820Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.612514Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.613415Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.614210Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.614980Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.615611Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.616161Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.633496Z","level":"error","event":"25/05/12 02:33:38 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.634770Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.635594Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.636262Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.637364Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.638591Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.640286Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.641120Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.642497Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.643388Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.645204Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.646421Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.649138Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.650734Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.651576Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.652192Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.652717Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.653268Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.653923Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.654555Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.655230Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.655875Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.656559Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.657239Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.657854Z","level":"error","event":"25/05/12 02:33:38 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.658519Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.659219Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.660023Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.660885Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.661622Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.662365Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.663558Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.664436Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.665212Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.665990Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.666711Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.667451Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.668300Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.669326Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.670057Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.670700Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.671380Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.672036Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.672693Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.673340Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.674006Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.674620Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.675404Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.676111Z","level":"error","event":"25/05/12 02:33:38 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.677418Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.678494Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.693095Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.693838Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.694639Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.695604Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.696529Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.697479Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.698253Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.699044Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.700041Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.701275Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.702445Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.703297Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.704076Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.704863Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.705627Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.706562Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.707461Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.711797Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.712938Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.713538Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.714100Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.735696Z","level":"error","event":"25/05/12 02:33:38 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.748742Z","level":"error","event":"25/05/12 02:33:38 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.749428Z","level":"error","event":"25/05/12 02:33:38 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.749919Z","level":"error","event":"25/05/12 02:33:38 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.750317Z","level":"error","event":"25/05/12 02:33:38 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@73663445 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.777686Z","level":"error","event":"25/05/12 02:33:38 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40691.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.789855Z","level":"error","event":"25/05/12 02:33:38 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:40691","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.790370Z","level":"error","event":"25/05/12 02:33:38 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.790905Z","level":"error","event":"25/05/12 02:33:38 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 40691, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.803126Z","level":"error","event":"25/05/12 02:33:38 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:40691 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 40691, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.803636Z","level":"error","event":"25/05/12 02:33:38 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 40691, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:38.804108Z","level":"error","event":"25/05/12 02:33:38 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 40691, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:39.273219Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:33:39.979996Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:33:40.980374Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:33:41.128801Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:33:42.128910Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:33:42.201800Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:33:43.201978Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:33:43.288113Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:33:44.288216Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:33:44.381740Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:33:45.381911Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:33:45.411454Z","level":"error","event":"25/05/12 02:33:45 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:45.425443Z","level":"error","event":"25/05/12 02:33:45 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:47.437991","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:33:47.987477Z","level":"error","event":"25/05/12 02:33:47 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:48.006332Z","level":"error","event":"25/05/12 02:33:47 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:48.025486Z","level":"error","event":"25/05/12 02:33:48 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:48.067188Z","level":"error","event":"25/05/12 02:33:48 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:48.108121Z","level":"error","event":"25/05/12 02:33:48 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:48.127428Z","level":"error","event":"25/05/12 02:33:48 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:48.128146Z","level":"error","event":"25/05/12 02:33:48 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:48.157364Z","level":"error","event":"25/05/12 02:33:48 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:48.175079Z","level":"error","event":"25/05/12 02:33:48 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:48.175678Z","level":"error","event":"25/05/12 02:33:48 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:48.176417Z","level":"error","event":"25/05/12 02:33:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-97d2b852-f8c9-4aff-88c8-0e94d8d39f96/pyspark-d4783d03-54fc-4012-8a9a-e8f136a3614d","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:48.177418Z","level":"error","event":"25/05/12 02:33:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-97d2b852-f8c9-4aff-88c8-0e94d8d39f96","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:33:48.203709Z","level":"error","event":"25/05/12 02:33:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-f4d8b843-124c-406c-803c-1fc6e820f2b0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:18.868771","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:34:23.758145Z","level":"error","event":"25/05/12 02:34:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:24.078078Z","level":"error","event":"25/05/12 02:34:24 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:24.102076Z","level":"error","event":"25/05/12 02:34:24 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:24.102970Z","level":"error","event":"25/05/12 02:34:24 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:24.103837Z","level":"error","event":"25/05/12 02:34:24 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:24.369283Z","level":"error","event":"25/05/12 02:34:24 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:24.383308Z","level":"error","event":"25/05/12 02:34:24 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:24.384153Z","level":"error","event":"25/05/12 02:34:24 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:24.422968Z","level":"error","event":"25/05/12 02:34:24 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:24.437649Z","level":"error","event":"25/05/12 02:34:24 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:24.438233Z","level":"error","event":"25/05/12 02:34:24 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:24.438689Z","level":"error","event":"25/05/12 02:34:24 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:24.462016Z","level":"error","event":"25/05/12 02:34:24 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:24.474819Z","level":"error","event":"25/05/12 02:34:24 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:24.487117Z","level":"error","event":"25/05/12 02:34:24 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:24.544897Z","level":"error","event":"25/05/12 02:34:24 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:24.557772Z","level":"error","event":"25/05/12 02:34:24 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:24.558269Z","level":"error","event":"25/05/12 02:34:24 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:24.558679Z","level":"error","event":"25/05/12 02:34:24 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:24.559058Z","level":"error","event":"25/05/12 02:34:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:24.898647Z","level":"error","event":"25/05/12 02:34:24 INFO Utils: Successfully started service 'sparkDriver' on port 44857.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:24.947993Z","level":"error","event":"25/05/12 02:34:24 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.011370Z","level":"error","event":"25/05/12 02:34:25 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.047470Z","level":"error","event":"25/05/12 02:34:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.068224Z","level":"error","event":"25/05/12 02:34:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.069009Z","level":"error","event":"25/05/12 02:34:25 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.096467Z","level":"error","event":"25/05/12 02:34:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-88ee22a4-84cf-42fa-9276-d804da588b34","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.132211Z","level":"error","event":"25/05/12 02:34:25 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.176789Z","level":"error","event":"25/05/12 02:34:25 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.618664Z","level":"error","event":"25/05/12 02:34:25 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.750484Z","level":"error","event":"25/05/12 02:34:25 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.864626Z","level":"error","event":"25/05/12 02:34:25 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.865373Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.865957Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.866582Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.867239Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.867811Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.868334Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.868877Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.869442Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.870023Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.870554Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.871249Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.871944Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.872855Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.873652Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.874314Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.874947Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.875838Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.876795Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.877696Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.878531Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.879412Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.880093Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.880975Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.904175Z","level":"error","event":"25/05/12 02:34:25 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.906663Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.908366Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.909223Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.909973Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.910761Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.911658Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.912475Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.913502Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.914352Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.915153Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.915919Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.916685Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.917445Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.918308Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.919426Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.920419Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.921580Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.922708Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.923691Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.924783Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.925771Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.926637Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.927874Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.928835Z","level":"error","event":"25/05/12 02:34:25 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.929835Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.931649Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.932773Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.933709Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.934617Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.935334Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.936157Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.937000Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.938040Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.938811Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.939629Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.940398Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.941313Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.942180Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.943093Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.944092Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.945012Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.945813Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.946566Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.947359Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.948238Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.949127Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.950101Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.951032Z","level":"error","event":"25/05/12 02:34:25 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.951860Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.952564Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.972537Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.973481Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.974258Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.975216Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.976043Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.976794Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.977542Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.978371Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.980012Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.981052Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.981966Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.983298Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.984717Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.985664Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.986494Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.987426Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.988772Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.991510Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.993633Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.994907Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:25.996285Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:26.078459Z","level":"error","event":"25/05/12 02:34:26 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:26.144678Z","level":"error","event":"25/05/12 02:34:26 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:26.145734Z","level":"error","event":"25/05/12 02:34:26 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:26.146502Z","level":"error","event":"25/05/12 02:34:26 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:26.147357Z","level":"error","event":"25/05/12 02:34:26 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1f04a8db for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:26.269173Z","level":"error","event":"25/05/12 02:34:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37723.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:26.293554Z","level":"error","event":"25/05/12 02:34:26 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:37723","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:26.294400Z","level":"error","event":"25/05/12 02:34:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:26.295390Z","level":"error","event":"25/05/12 02:34:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 37723, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:26.319090Z","level":"error","event":"25/05/12 02:34:26 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:37723 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 37723, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:26.320071Z","level":"error","event":"25/05/12 02:34:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 37723, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:26.321032Z","level":"error","event":"25/05/12 02:34:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 37723, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:27.083814Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:34:27.833054Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:34:28.833254Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:34:28.953833Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:34:29.954053Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:34:30.186583Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:34:31.046388Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:34:31.137766Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:34:34.284421Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:34:34.377621Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:34:35.377855Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:34:35.388990Z","level":"error","event":"25/05/12 02:34:35 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:35.399659Z","level":"error","event":"25/05/12 02:34:35 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:37.307495","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:34:37.787178Z","level":"error","event":"25/05/12 02:34:37 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:37.788158Z","level":"error","event":"25/05/12 02:34:37 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:37.800949Z","level":"error","event":"25/05/12 02:34:37 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:37.818398Z","level":"error","event":"25/05/12 02:34:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:37.838924Z","level":"error","event":"25/05/12 02:34:37 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:37.847595Z","level":"error","event":"25/05/12 02:34:37 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:37.848546Z","level":"error","event":"25/05/12 02:34:37 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:37.858616Z","level":"error","event":"25/05/12 02:34:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:37.867414Z","level":"error","event":"25/05/12 02:34:37 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:37.867999Z","level":"error","event":"25/05/12 02:34:37 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:37.868452Z","level":"error","event":"25/05/12 02:34:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-5ed4c5e8-08e1-4362-ae1c-25e6d15cbd52","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:37.876469Z","level":"error","event":"25/05/12 02:34:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-f8ed1f2c-6d08-406a-aa5f-08ce5b5ebe60","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:34:37.885270Z","level":"error","event":"25/05/12 02:34:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-f8ed1f2c-6d08-406a-aa5f-08ce5b5ebe60/pyspark-7548220b-8346-46fd-a7f5-e1b293e1cb47","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:08.786363","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:35:12.118408Z","level":"error","event":"25/05/12 02:35:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:12.367231Z","level":"error","event":"25/05/12 02:35:12 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:12.383089Z","level":"error","event":"25/05/12 02:35:12 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:12.383482Z","level":"error","event":"25/05/12 02:35:12 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:12.383789Z","level":"error","event":"25/05/12 02:35:12 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:12.519236Z","level":"error","event":"25/05/12 02:35:12 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:12.535310Z","level":"error","event":"25/05/12 02:35:12 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:12.535661Z","level":"error","event":"25/05/12 02:35:12 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:12.560548Z","level":"error","event":"25/05/12 02:35:12 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:12.578845Z","level":"error","event":"25/05/12 02:35:12 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:12.579540Z","level":"error","event":"25/05/12 02:35:12 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:12.580011Z","level":"error","event":"25/05/12 02:35:12 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:12.598558Z","level":"error","event":"25/05/12 02:35:12 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:12.617511Z","level":"error","event":"25/05/12 02:35:12 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:12.618066Z","level":"error","event":"25/05/12 02:35:12 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:12.666767Z","level":"error","event":"25/05/12 02:35:12 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:12.682062Z","level":"error","event":"25/05/12 02:35:12 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:12.682497Z","level":"error","event":"25/05/12 02:35:12 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:12.682850Z","level":"error","event":"25/05/12 02:35:12 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:12.683149Z","level":"error","event":"25/05/12 02:35:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.054093Z","level":"error","event":"25/05/12 02:35:13 INFO Utils: Successfully started service 'sparkDriver' on port 42039.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.096584Z","level":"error","event":"25/05/12 02:35:13 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.134310Z","level":"error","event":"25/05/12 02:35:13 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.164117Z","level":"error","event":"25/05/12 02:35:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.182998Z","level":"error","event":"25/05/12 02:35:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.183531Z","level":"error","event":"25/05/12 02:35:13 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.218110Z","level":"error","event":"25/05/12 02:35:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ad4f09da-d73f-4de4-88cf-d1604bd725b7","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.236162Z","level":"error","event":"25/05/12 02:35:13 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.265238Z","level":"error","event":"25/05/12 02:35:13 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.441288Z","level":"error","event":"25/05/12 02:35:13 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.553643Z","level":"error","event":"25/05/12 02:35:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.642691Z","level":"error","event":"25/05/12 02:35:13 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.643661Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.644637Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.645943Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.646732Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.647452Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.648170Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.649041Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.649799Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.650329Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.650863Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.651385Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.651824Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.652332Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.652962Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.653593Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.654133Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.654576Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.655181Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.655642Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.656525Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.657752Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.658665Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.659630Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.682692Z","level":"error","event":"25/05/12 02:35:13 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.683338Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.683890Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.684722Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.686072Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.687592Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.688585Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.689677Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.690537Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.691546Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.692327Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.692982Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.693725Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.694543Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.695294Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.696016Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.696701Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.697312Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.697967Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.698613Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.699285Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.700116Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.700832Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.701485Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.702406Z","level":"error","event":"25/05/12 02:35:13 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.704257Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.705260Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.706052Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.707171Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.708569Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.709443Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.710310Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.711532Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.712582Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.713517Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.714409Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.715121Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.715779Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.716311Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.716759Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.717205Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.717659Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.718314Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.718997Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.719789Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.720491Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.721541Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.722108Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.723114Z","level":"error","event":"25/05/12 02:35:13 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.724103Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.724674Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.742536Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.743275Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.744417Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.745528Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.749276Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.750447Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.751502Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.752846Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.754258Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.755152Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.756035Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.757176Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.757918Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.758820Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.759675Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.760402Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.761312Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.762248Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.762951Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.763736Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.765259Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.814280Z","level":"error","event":"25/05/12 02:35:13 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.836425Z","level":"error","event":"25/05/12 02:35:13 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.837129Z","level":"error","event":"25/05/12 02:35:13 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.838325Z","level":"error","event":"25/05/12 02:35:13 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.839229Z","level":"error","event":"25/05/12 02:35:13 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@73663445 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.892652Z","level":"error","event":"25/05/12 02:35:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43747.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.907167Z","level":"error","event":"25/05/12 02:35:13 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:43747","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.907558Z","level":"error","event":"25/05/12 02:35:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.907976Z","level":"error","event":"25/05/12 02:35:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 43747, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.908387Z","level":"error","event":"25/05/12 02:35:13 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:43747 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 43747, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.924228Z","level":"error","event":"25/05/12 02:35:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 43747, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:13.924779Z","level":"error","event":"25/05/12 02:35:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 43747, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:14.527455Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:35:15.136610Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:35:16.136766Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:35:16.248414Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:35:17.248558Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:35:17.321220Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:35:18.321395Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:35:18.404882Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:35:19.405027Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:35:19.515497Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:35:20.515666Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:35:20.522801Z","level":"error","event":"25/05/12 02:35:20 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:20.532084Z","level":"error","event":"25/05/12 02:35:20 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:21.890133","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:35:22.395382Z","level":"error","event":"25/05/12 02:35:22 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:22.395972Z","level":"error","event":"25/05/12 02:35:22 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:22.410267Z","level":"error","event":"25/05/12 02:35:22 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:22.430217Z","level":"error","event":"25/05/12 02:35:22 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:22.451062Z","level":"error","event":"25/05/12 02:35:22 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:22.460691Z","level":"error","event":"25/05/12 02:35:22 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:22.461383Z","level":"error","event":"25/05/12 02:35:22 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:22.473737Z","level":"error","event":"25/05/12 02:35:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:22.484504Z","level":"error","event":"25/05/12 02:35:22 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:22.485215Z","level":"error","event":"25/05/12 02:35:22 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:22.485895Z","level":"error","event":"25/05/12 02:35:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-8bc2ea2e-a14f-4891-8eb1-6ef780396803","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:22.501106Z","level":"error","event":"25/05/12 02:35:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-8bc2ea2e-a14f-4891-8eb1-6ef780396803/pyspark-1d757f3a-5116-4f8f-8ec9-f947bd1090c8","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:22.502409Z","level":"error","event":"25/05/12 02:35:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-33266439-fda0-4e7f-8e6c-40a489bccb61","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:35:54.445463","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:36:00.310006Z","level":"error","event":"25/05/12 02:36:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:00.615730Z","level":"error","event":"25/05/12 02:36:00 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:00.641859Z","level":"error","event":"25/05/12 02:36:00 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:00.642437Z","level":"error","event":"25/05/12 02:36:00 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:00.642913Z","level":"error","event":"25/05/12 02:36:00 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:00.803345Z","level":"error","event":"25/05/12 02:36:00 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:00.829561Z","level":"error","event":"25/05/12 02:36:00 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:00.830141Z","level":"error","event":"25/05/12 02:36:00 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:00.872651Z","level":"error","event":"25/05/12 02:36:00 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:00.894234Z","level":"error","event":"25/05/12 02:36:00 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:00.895345Z","level":"error","event":"25/05/12 02:36:00 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:00.896029Z","level":"error","event":"25/05/12 02:36:00 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:00.919986Z","level":"error","event":"25/05/12 02:36:00 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:00.942089Z","level":"error","event":"25/05/12 02:36:00 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:00.943250Z","level":"error","event":"25/05/12 02:36:00 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:01.021837Z","level":"error","event":"25/05/12 02:36:01 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:01.045454Z","level":"error","event":"25/05/12 02:36:01 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:01.046135Z","level":"error","event":"25/05/12 02:36:01 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:01.046831Z","level":"error","event":"25/05/12 02:36:01 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:01.047510Z","level":"error","event":"25/05/12 02:36:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:01.486696Z","level":"error","event":"25/05/12 02:36:01 INFO Utils: Successfully started service 'sparkDriver' on port 46113.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:01.541121Z","level":"error","event":"25/05/12 02:36:01 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:01.611287Z","level":"error","event":"25/05/12 02:36:01 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:01.662825Z","level":"error","event":"25/05/12 02:36:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:01.686191Z","level":"error","event":"25/05/12 02:36:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:01.687005Z","level":"error","event":"25/05/12 02:36:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:01.718743Z","level":"error","event":"25/05/12 02:36:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f1175f85-6a43-45f6-a60f-a106ed2e8f70","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:01.762555Z","level":"error","event":"25/05/12 02:36:01 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:01.792595Z","level":"error","event":"25/05/12 02:36:01 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.029232Z","level":"error","event":"25/05/12 02:36:02 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.114054Z","level":"error","event":"25/05/12 02:36:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.194851Z","level":"error","event":"25/05/12 02:36:02 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.195515Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.196062Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.196673Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.197350Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.197913Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.198473Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.199010Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.199493Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.199975Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.200472Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.201013Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.201608Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.202099Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.202585Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.203132Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.203647Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.204179Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.204760Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.205288Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.205736Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.206201Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.206728Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.207223Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.228357Z","level":"error","event":"25/05/12 02:36:02 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.229074Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.229958Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.230923Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.233145Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.234057Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.234732Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.235245Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.235739Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.236229Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.236744Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.237280Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.237777Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.238403Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.239026Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.239728Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.240410Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.241095Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.241739Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.242266Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.242798Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.243217Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.243604Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.243968Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.244395Z","level":"error","event":"25/05/12 02:36:02 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.244852Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.245316Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.245775Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.246228Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.246692Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.247144Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.247546Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.247950Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.248348Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.248768Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.249158Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.249617Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.250027Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.250406Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.250941Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.251623Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.252294Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.252965Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.254022Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.254862Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.255502Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.256065Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.256694Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.257191Z","level":"error","event":"25/05/12 02:36:02 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.257818Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.258427Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.276860Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.277443Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.278101Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.278769Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.279458Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.280051Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.280745Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.281448Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.282316Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.283019Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.283802Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.284488Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.285092Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.285744Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.286283Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.286747Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.287157Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.287708Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.288260Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.289129Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.289747Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.324051Z","level":"error","event":"25/05/12 02:36:02 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.324652Z","level":"error","event":"25/05/12 02:36:02 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.325245Z","level":"error","event":"25/05/12 02:36:02 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.346602Z","level":"error","event":"25/05/12 02:36:02 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.347353Z","level":"error","event":"25/05/12 02:36:02 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6ff21454 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.370103Z","level":"error","event":"25/05/12 02:36:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40727.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.387253Z","level":"error","event":"25/05/12 02:36:02 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:40727","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.387893Z","level":"error","event":"25/05/12 02:36:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.388324Z","level":"error","event":"25/05/12 02:36:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 40727, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.406467Z","level":"error","event":"25/05/12 02:36:02 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:40727 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 40727, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.407029Z","level":"error","event":"25/05/12 02:36:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 40727, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.407523Z","level":"error","event":"25/05/12 02:36:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 40727, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:02.932941Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:36:03.453353Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:36:04.453726Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:36:04.573906Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:36:05.574084Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:36:05.654474Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:36:06.654643Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:36:06.758591Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:36:07.758736Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:36:07.872370Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:36:10.946608Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:36:10.973154Z","level":"error","event":"25/05/12 02:36:10 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:10.973572Z","level":"error","event":"25/05/12 02:36:10 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:12.235565","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:36:12.714351Z","level":"error","event":"25/05/12 02:36:12 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:12.714736Z","level":"error","event":"25/05/12 02:36:12 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:12.729748Z","level":"error","event":"25/05/12 02:36:12 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:12.742363Z","level":"error","event":"25/05/12 02:36:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:12.756333Z","level":"error","event":"25/05/12 02:36:12 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:12.756855Z","level":"error","event":"25/05/12 02:36:12 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:12.757262Z","level":"error","event":"25/05/12 02:36:12 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:12.772998Z","level":"error","event":"25/05/12 02:36:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:12.773439Z","level":"error","event":"25/05/12 02:36:12 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:12.773724Z","level":"error","event":"25/05/12 02:36:12 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:12.773977Z","level":"error","event":"25/05/12 02:36:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-37902937-bdd2-4bee-a450-5eebb2029a95/pyspark-690c80a8-7ae4-47be-a247-f9b99724ce9e","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:12.786394Z","level":"error","event":"25/05/12 02:36:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-08b66445-5867-4b88-b5bf-c56b5e5ed4cd","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:12.809657Z","level":"error","event":"25/05/12 02:36:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-37902937-bdd2-4bee-a450-5eebb2029a95","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:44.223512","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:36:48.879902Z","level":"error","event":"25/05/12 02:36:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:49.322577Z","level":"error","event":"25/05/12 02:36:49 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:49.356997Z","level":"error","event":"25/05/12 02:36:49 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:49.357840Z","level":"error","event":"25/05/12 02:36:49 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:49.359436Z","level":"error","event":"25/05/12 02:36:49 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:49.859756Z","level":"error","event":"25/05/12 02:36:49 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:49.883926Z","level":"error","event":"25/05/12 02:36:49 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:49.884741Z","level":"error","event":"25/05/12 02:36:49 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:49.918483Z","level":"error","event":"25/05/12 02:36:49 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:49.937368Z","level":"error","event":"25/05/12 02:36:49 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:49.938091Z","level":"error","event":"25/05/12 02:36:49 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:49.938727Z","level":"error","event":"25/05/12 02:36:49 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:49.963474Z","level":"error","event":"25/05/12 02:36:49 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:49.979144Z","level":"error","event":"25/05/12 02:36:49 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:49.995700Z","level":"error","event":"25/05/12 02:36:49 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:50.050824Z","level":"error","event":"25/05/12 02:36:50 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:50.070255Z","level":"error","event":"25/05/12 02:36:50 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:50.071078Z","level":"error","event":"25/05/12 02:36:50 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:50.071988Z","level":"error","event":"25/05/12 02:36:50 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:50.072993Z","level":"error","event":"25/05/12 02:36:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:50.653894Z","level":"error","event":"25/05/12 02:36:50 INFO Utils: Successfully started service 'sparkDriver' on port 34365.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:50.821104Z","level":"error","event":"25/05/12 02:36:50 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:50.889392Z","level":"error","event":"25/05/12 02:36:50 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:50.927134Z","level":"error","event":"25/05/12 02:36:50 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:50.951593Z","level":"error","event":"25/05/12 02:36:50 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:50.952281Z","level":"error","event":"25/05/12 02:36:50 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:50.981501Z","level":"error","event":"25/05/12 02:36:50 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8ebfeb4b-9b9c-4958-8ec2-01ac90cf435b","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.006314Z","level":"error","event":"25/05/12 02:36:51 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.036320Z","level":"error","event":"25/05/12 02:36:51 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.248563Z","level":"error","event":"25/05/12 02:36:51 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.353732Z","level":"error","event":"25/05/12 02:36:51 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.438800Z","level":"error","event":"25/05/12 02:36:51 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.439401Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.439979Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.440584Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.441136Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.441721Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.442356Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.443314Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.443959Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.444769Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.445639Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.446626Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.447393Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.448159Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.448732Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.449540Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.450087Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.450635Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.451353Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.451920Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.452567Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.453179Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.453785Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.454420Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.481936Z","level":"error","event":"25/05/12 02:36:51 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.494072Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.495045Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.496150Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.496962Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.497834Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.498782Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.499737Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.500658Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.501448Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.502511Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.503305Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.504184Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.505125Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.506268Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.507384Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.508222Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.509392Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.510672Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.511444Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.512321Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.513315Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.514176Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.514938Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.515641Z","level":"error","event":"25/05/12 02:36:51 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.516217Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.516832Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.517602Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.518489Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.519240Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.519865Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.520466Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.521040Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.521848Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.522586Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.523546Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.524604Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.525474Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.526859Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.527547Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.528119Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.528669Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.529205Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.529882Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.530797Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.531646Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.532298Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.533011Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.533615Z","level":"error","event":"25/05/12 02:36:51 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.534119Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.534831Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.565989Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.566639Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.567223Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.567813Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.568459Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.569461Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.571597Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.572745Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.573452Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.574446Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.575310Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.576189Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.577005Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.577711Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.578387Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.579142Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.580080Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.580844Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.581536Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.582633Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.584093Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.634079Z","level":"error","event":"25/05/12 02:36:51 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.635112Z","level":"error","event":"25/05/12 02:36:51 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.637345Z","level":"error","event":"25/05/12 02:36:51 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.671716Z","level":"error","event":"25/05/12 02:36:51 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.672532Z","level":"error","event":"25/05/12 02:36:51 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@73663445 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.720875Z","level":"error","event":"25/05/12 02:36:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33341.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.758184Z","level":"error","event":"25/05/12 02:36:51 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:33341","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.758805Z","level":"error","event":"25/05/12 02:36:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.759471Z","level":"error","event":"25/05/12 02:36:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 33341, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.760115Z","level":"error","event":"25/05/12 02:36:51 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:33341 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 33341, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.783023Z","level":"error","event":"25/05/12 02:36:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 33341, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:51.783717Z","level":"error","event":"25/05/12 02:36:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 33341, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:52.376639Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:36:53.166476Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:36:54.166642Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:36:54.297142Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:36:55.297571Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:36:55.386371Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:36:56.386662Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:36:56.658617Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:36:57.541348Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:36:57.643991Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:36:58.644188Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:36:58.667684Z","level":"error","event":"25/05/12 02:36:58 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:36:58.668402Z","level":"error","event":"25/05/12 02:36:58 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:01.800564","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:37:02.353357Z","level":"error","event":"25/05/12 02:37:02 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:02.354092Z","level":"error","event":"25/05/12 02:37:02 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:02.377611Z","level":"error","event":"25/05/12 02:37:02 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:02.405269Z","level":"error","event":"25/05/12 02:37:02 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:02.431649Z","level":"error","event":"25/05/12 02:37:02 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:02.432489Z","level":"error","event":"25/05/12 02:37:02 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:02.461227Z","level":"error","event":"25/05/12 02:37:02 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:02.462667Z","level":"error","event":"25/05/12 02:37:02 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:02.493863Z","level":"error","event":"25/05/12 02:37:02 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:02.494496Z","level":"error","event":"25/05/12 02:37:02 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:02.495274Z","level":"error","event":"25/05/12 02:37:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-cb667f0b-51b4-47af-8eac-4a2c499b0d14/pyspark-32e46c7c-0272-424f-88f1-cf7c62d467f4","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:02.495928Z","level":"error","event":"25/05/12 02:37:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-1474d322-eac3-434f-bfda-5b29e27c43ed","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:02.523404Z","level":"error","event":"25/05/12 02:37:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-cb667f0b-51b4-47af-8eac-4a2c499b0d14","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:36.749076","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:37:41.956418Z","level":"error","event":"25/05/12 02:37:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:42.268772Z","level":"error","event":"25/05/12 02:37:42 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:42.291091Z","level":"error","event":"25/05/12 02:37:42 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:42.291587Z","level":"error","event":"25/05/12 02:37:42 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:42.292021Z","level":"error","event":"25/05/12 02:37:42 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:42.489116Z","level":"error","event":"25/05/12 02:37:42 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:42.511300Z","level":"error","event":"25/05/12 02:37:42 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:42.511865Z","level":"error","event":"25/05/12 02:37:42 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:42.551514Z","level":"error","event":"25/05/12 02:37:42 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:42.569881Z","level":"error","event":"25/05/12 02:37:42 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:42.570479Z","level":"error","event":"25/05/12 02:37:42 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:42.571198Z","level":"error","event":"25/05/12 02:37:42 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:42.593705Z","level":"error","event":"25/05/12 02:37:42 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:42.612590Z","level":"error","event":"25/05/12 02:37:42 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:42.613240Z","level":"error","event":"25/05/12 02:37:42 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:42.684680Z","level":"error","event":"25/05/12 02:37:42 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:42.707868Z","level":"error","event":"25/05/12 02:37:42 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:42.708578Z","level":"error","event":"25/05/12 02:37:42 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:42.709393Z","level":"error","event":"25/05/12 02:37:42 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:42.710083Z","level":"error","event":"25/05/12 02:37:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.265683Z","level":"error","event":"25/05/12 02:37:43 INFO Utils: Successfully started service 'sparkDriver' on port 37237.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.326819Z","level":"error","event":"25/05/12 02:37:43 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.395679Z","level":"error","event":"25/05/12 02:37:43 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.425009Z","level":"error","event":"25/05/12 02:37:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.443734Z","level":"error","event":"25/05/12 02:37:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.444444Z","level":"error","event":"25/05/12 02:37:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.475209Z","level":"error","event":"25/05/12 02:37:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4784e1d0-8e6a-4f8e-961a-10bd7122f4d7","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.498021Z","level":"error","event":"25/05/12 02:37:43 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.521596Z","level":"error","event":"25/05/12 02:37:43 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.722440Z","level":"error","event":"25/05/12 02:37:43 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.817792Z","level":"error","event":"25/05/12 02:37:43 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.908606Z","level":"error","event":"25/05/12 02:37:43 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.909273Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.909954Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.910601Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.911223Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.911896Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.912523Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.913165Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.913806Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.914487Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.915402Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.916016Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.919364Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.920264Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.921157Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.921893Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.922599Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.923230Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.923869Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.924768Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.926037Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.926806Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.927387Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.928084Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.950411Z","level":"error","event":"25/05/12 02:37:43 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.951030Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.951726Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.952321Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.952869Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.953491Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.953990Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.954497Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.955270Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.956228Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.957100Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.957703Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.958291Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.958812Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.959315Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.959988Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.960653Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.961322Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.962052Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.962741Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.963442Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.964124Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.964769Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.965286Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.965910Z","level":"error","event":"25/05/12 02:37:43 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.966470Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.967089Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.967762Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.968360Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.969465Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.983166Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.983858Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.984364Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.984855Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.985249Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.985630Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.986009Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.986380Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.986789Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.987306Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.987766Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.988218Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.988722Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.989131Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.989581Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.990008Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.990659Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.991193Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.991649Z","level":"error","event":"25/05/12 02:37:43 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.992055Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:43.992434Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.012691Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.013422Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.014179Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.014979Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.015708Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.016314Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.016789Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.017292Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.017745Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.018197Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.018599Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.019267Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.019921Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.020698Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.021682Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.023214Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.024086Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.024852Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.025429Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.026050Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.026693Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.052216Z","level":"error","event":"25/05/12 02:37:44 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.052783Z","level":"error","event":"25/05/12 02:37:44 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.053247Z","level":"error","event":"25/05/12 02:37:44 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.073111Z","level":"error","event":"25/05/12 02:37:44 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.073594Z","level":"error","event":"25/05/12 02:37:44 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6ff21454 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.116094Z","level":"error","event":"25/05/12 02:37:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44271.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.134119Z","level":"error","event":"25/05/12 02:37:44 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:44271","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.134600Z","level":"error","event":"25/05/12 02:37:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.135087Z","level":"error","event":"25/05/12 02:37:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 44271, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.135564Z","level":"error","event":"25/05/12 02:37:44 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:44271 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 44271, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.152327Z","level":"error","event":"25/05/12 02:37:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 44271, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:44.152893Z","level":"error","event":"25/05/12 02:37:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 44271, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:46.985037Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:37:47.769308Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:37:48.827527Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:37:48.926888Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:37:49.927096Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:37:50.022577Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:37:51.022928Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:37:51.180662Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:37:52.181491Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:37:52.387472Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:37:53.387735Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:37:53.415683Z","level":"error","event":"25/05/12 02:37:53 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:53.436530Z","level":"error","event":"25/05/12 02:37:53 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:56.974594","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:37:57.459698Z","level":"error","event":"25/05/12 02:37:57 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:57.460562Z","level":"error","event":"25/05/12 02:37:57 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:57.494037Z","level":"error","event":"25/05/12 02:37:57 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:57.529902Z","level":"error","event":"25/05/12 02:37:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:57.584272Z","level":"error","event":"25/05/12 02:37:57 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:57.595028Z","level":"error","event":"25/05/12 02:37:57 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:57.611491Z","level":"error","event":"25/05/12 02:37:57 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:57.612504Z","level":"error","event":"25/05/12 02:37:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:57.630985Z","level":"error","event":"25/05/12 02:37:57 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:57.646338Z","level":"error","event":"25/05/12 02:37:57 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:57.647276Z","level":"error","event":"25/05/12 02:37:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-a68b9ab3-9bb5-4601-957d-7eeb1182215c","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:57.662279Z","level":"error","event":"25/05/12 02:37:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-60176228-b2e2-43d7-99d7-a78f16bb31e4/pyspark-2aa86506-e306-42cd-ab77-03ded1883afa","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:37:57.663070Z","level":"error","event":"25/05/12 02:37:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-60176228-b2e2-43d7-99d7-a78f16bb31e4","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:28.700375","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:38:32.463605Z","level":"error","event":"25/05/12 02:38:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:32.795925Z","level":"error","event":"25/05/12 02:38:32 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:32.816068Z","level":"error","event":"25/05/12 02:38:32 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:32.816667Z","level":"error","event":"25/05/12 02:38:32 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:32.817316Z","level":"error","event":"25/05/12 02:38:32 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:33.016963Z","level":"error","event":"25/05/12 02:38:33 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:33.120220Z","level":"error","event":"25/05/12 02:38:33 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:33.121650Z","level":"error","event":"25/05/12 02:38:33 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:33.180110Z","level":"error","event":"25/05/12 02:38:33 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:33.183077Z","level":"error","event":"25/05/12 02:38:33 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:33.184064Z","level":"error","event":"25/05/12 02:38:33 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:33.184932Z","level":"error","event":"25/05/12 02:38:33 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:33.234382Z","level":"error","event":"25/05/12 02:38:33 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:33.277485Z","level":"error","event":"25/05/12 02:38:33 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:33.278411Z","level":"error","event":"25/05/12 02:38:33 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:33.439109Z","level":"error","event":"25/05/12 02:38:33 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:33.502068Z","level":"error","event":"25/05/12 02:38:33 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:33.503115Z","level":"error","event":"25/05/12 02:38:33 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:33.503926Z","level":"error","event":"25/05/12 02:38:33 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:33.504862Z","level":"error","event":"25/05/12 02:38:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:34.023004Z","level":"error","event":"25/05/12 02:38:34 INFO Utils: Successfully started service 'sparkDriver' on port 41399.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:34.124774Z","level":"error","event":"25/05/12 02:38:34 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:34.232708Z","level":"error","event":"25/05/12 02:38:34 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:34.285651Z","level":"error","event":"25/05/12 02:38:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:34.313236Z","level":"error","event":"25/05/12 02:38:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:34.314055Z","level":"error","event":"25/05/12 02:38:34 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:34.358989Z","level":"error","event":"25/05/12 02:38:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f5e86c0a-6b25-484e-ac5c-998e32a46ea3","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:34.406346Z","level":"error","event":"25/05/12 02:38:34 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:34.451347Z","level":"error","event":"25/05/12 02:38:34 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:34.789300Z","level":"error","event":"25/05/12 02:38:34 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.136535Z","level":"error","event":"25/05/12 02:38:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.242324Z","level":"error","event":"25/05/12 02:38:35 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.243047Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.243702Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.244364Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.245062Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.245745Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.246380Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.246991Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.247572Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.248970Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.249896Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.252004Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.252832Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.253532Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.254417Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.255349Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.256140Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.256910Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.257795Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.258803Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.259667Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.260501Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.261346Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.262014Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.290930Z","level":"error","event":"25/05/12 02:38:35 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.291694Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.292461Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.293407Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.294301Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.295236Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.295923Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.296711Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.297499Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.298318Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.299213Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.299898Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.300748Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.301630Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.302394Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.303092Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.303838Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.304594Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.305308Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.305998Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.306748Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.307671Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.308551Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.309306Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.310050Z","level":"error","event":"25/05/12 02:38:35 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.310807Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.311532Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.312343Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.313354Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.314320Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.315028Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.315734Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.316474Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.317754Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.318770Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.320223Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.321204Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.322018Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.322724Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.323465Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.324187Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.324926Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.325716Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.326492Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.327365Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.328213Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.328939Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.329657Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.330350Z","level":"error","event":"25/05/12 02:38:35 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.330992Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.331650Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.355278Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.356414Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.357154Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.357872Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.358575Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.359221Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.359863Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.360530Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.361191Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.361884Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.362646Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.363347Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.364062Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.364749Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.365462Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.366201Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.366935Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.367645Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.368489Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.369177Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.370041Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.413834Z","level":"error","event":"25/05/12 02:38:35 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.437695Z","level":"error","event":"25/05/12 02:38:35 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.438522Z","level":"error","event":"25/05/12 02:38:35 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.439241Z","level":"error","event":"25/05/12 02:38:35 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.439891Z","level":"error","event":"25/05/12 02:38:35 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@67055e93 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.488899Z","level":"error","event":"25/05/12 02:38:35 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33287.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.513222Z","level":"error","event":"25/05/12 02:38:35 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:33287","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.514600Z","level":"error","event":"25/05/12 02:38:35 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.515402Z","level":"error","event":"25/05/12 02:38:35 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 33287, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.551494Z","level":"error","event":"25/05/12 02:38:35 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:33287 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 33287, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.552258Z","level":"error","event":"25/05/12 02:38:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 33287, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:35.553098Z","level":"error","event":"25/05/12 02:38:35 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 33287, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:36.224970Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:38:37.025977Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:38:38.001190Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:38:38.116018Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:38:39.101279Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:38:39.166725Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:38:40.166969Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:38:40.265452Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:38:41.265654Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:38:41.346014Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:38:42.346159Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:38:42.360723Z","level":"error","event":"25/05/12 02:38:42 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:42.361076Z","level":"error","event":"25/05/12 02:38:42 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:43.896220","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:38:44.295357Z","level":"error","event":"25/05/12 02:38:44 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:44.295710Z","level":"error","event":"25/05/12 02:38:44 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:44.310819Z","level":"error","event":"25/05/12 02:38:44 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:44.325450Z","level":"error","event":"25/05/12 02:38:44 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:44.338961Z","level":"error","event":"25/05/12 02:38:44 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:44.339392Z","level":"error","event":"25/05/12 02:38:44 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:44.353105Z","level":"error","event":"25/05/12 02:38:44 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:44.353703Z","level":"error","event":"25/05/12 02:38:44 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:44.369007Z","level":"error","event":"25/05/12 02:38:44 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:44.369739Z","level":"error","event":"25/05/12 02:38:44 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:44.370392Z","level":"error","event":"25/05/12 02:38:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-b75028a8-0b73-4fae-a115-618f14726a9e/pyspark-00b5070e-c8d0-45e6-927d-1d2a396003ff","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:44.370845Z","level":"error","event":"25/05/12 02:38:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-868b8928-b9f9-44bc-b6eb-aeb5bbb2a9d6","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:38:44.371279Z","level":"error","event":"25/05/12 02:38:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-b75028a8-0b73-4fae-a115-618f14726a9e","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:15.213369","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:39:19.752652Z","level":"error","event":"25/05/12 02:39:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:20.138624Z","level":"error","event":"25/05/12 02:39:20 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:20.176557Z","level":"error","event":"25/05/12 02:39:20 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:20.177528Z","level":"error","event":"25/05/12 02:39:20 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:20.178675Z","level":"error","event":"25/05/12 02:39:20 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:20.397447Z","level":"error","event":"25/05/12 02:39:20 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:20.422057Z","level":"error","event":"25/05/12 02:39:20 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:20.422717Z","level":"error","event":"25/05/12 02:39:20 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:20.522517Z","level":"error","event":"25/05/12 02:39:20 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:20.558011Z","level":"error","event":"25/05/12 02:39:20 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:20.558885Z","level":"error","event":"25/05/12 02:39:20 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:20.559651Z","level":"error","event":"25/05/12 02:39:20 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:20.616318Z","level":"error","event":"25/05/12 02:39:20 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:20.636499Z","level":"error","event":"25/05/12 02:39:20 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:20.637301Z","level":"error","event":"25/05/12 02:39:20 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:20.709490Z","level":"error","event":"25/05/12 02:39:20 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:20.733318Z","level":"error","event":"25/05/12 02:39:20 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:20.734001Z","level":"error","event":"25/05/12 02:39:20 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:20.734775Z","level":"error","event":"25/05/12 02:39:20 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:20.735463Z","level":"error","event":"25/05/12 02:39:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:21.975679Z","level":"error","event":"25/05/12 02:39:21 INFO Utils: Successfully started service 'sparkDriver' on port 45731.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:23.341178Z","level":"error","event":"25/05/12 02:39:23 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:23.427279Z","level":"error","event":"25/05/12 02:39:23 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:23.454342Z","level":"error","event":"25/05/12 02:39:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:23.477701Z","level":"error","event":"25/05/12 02:39:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:23.478390Z","level":"error","event":"25/05/12 02:39:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:23.514798Z","level":"error","event":"25/05/12 02:39:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-41a3509e-ffe6-464e-9373-9d38cd1f814a","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:23.573389Z","level":"error","event":"25/05/12 02:39:23 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:23.622424Z","level":"error","event":"25/05/12 02:39:23 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:23.858603Z","level":"error","event":"25/05/12 02:39:23 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:23.987013Z","level":"error","event":"25/05/12 02:39:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.046146Z","level":"error","event":"25/05/12 02:39:24 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.046910Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.047591Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.048336Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.049030Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.049580Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.050480Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.051286Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.052069Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.052754Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.053513Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.054388Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.055501Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.056318Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.057184Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.058120Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.059005Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.059764Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.060445Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.061299Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.062055Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.063259Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.064093Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.064766Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.100256Z","level":"error","event":"25/05/12 02:39:24 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.101039Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.101764Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.102493Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.103151Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.103700Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.104319Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.105004Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.105637Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.106214Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.106811Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.107466Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.108081Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.108754Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.109417Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.110004Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.110627Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.111304Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.111981Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.112654Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.113366Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.113988Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.114548Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.115060Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.115689Z","level":"error","event":"25/05/12 02:39:24 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.116368Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.117161Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.117959Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.118597Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.119170Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.119630Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.120038Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.120394Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.120773Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.121143Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.121447Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.121726Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.122015Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.122330Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.122612Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.122914Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.123182Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.123544Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.123816Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.124097Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.124372Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.124657Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.124927Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.125215Z","level":"error","event":"25/05/12 02:39:24 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.125500Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.125913Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.140133Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.140699Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.141190Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.141866Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.142345Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.142700Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.143060Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.143430Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.143754Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.144139Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.144592Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.144971Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.145497Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.145895Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.146256Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.146746Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.147293Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.147810Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.148264Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.148636Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.149105Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.170495Z","level":"error","event":"25/05/12 02:39:24 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.186583Z","level":"error","event":"25/05/12 02:39:24 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.187075Z","level":"error","event":"25/05/12 02:39:24 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.187500Z","level":"error","event":"25/05/12 02:39:24 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.187898Z","level":"error","event":"25/05/12 02:39:24 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6ff21454 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.211976Z","level":"error","event":"25/05/12 02:39:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41907.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.227925Z","level":"error","event":"25/05/12 02:39:24 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:41907","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.228336Z","level":"error","event":"25/05/12 02:39:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.228728Z","level":"error","event":"25/05/12 02:39:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 41907, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.229109Z","level":"error","event":"25/05/12 02:39:24 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:41907 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 41907, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.243539Z","level":"error","event":"25/05/12 02:39:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 41907, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.243950Z","level":"error","event":"25/05/12 02:39:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 41907, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:24.697188Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:39:25.862578Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:39:26.862728Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:39:26.990363Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:39:27.990583Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:39:28.064877Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:39:29.065127Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:39:29.160181Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:39:30.160376Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:39:30.269152Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:39:31.269258Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:39:31.290439Z","level":"error","event":"25/05/12 02:39:31 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:31.291114Z","level":"error","event":"25/05/12 02:39:31 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:33.891641","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:39:34.410122Z","level":"error","event":"25/05/12 02:39:34 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:34.410788Z","level":"error","event":"25/05/12 02:39:34 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:34.441292Z","level":"error","event":"25/05/12 02:39:34 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:34.460066Z","level":"error","event":"25/05/12 02:39:34 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:34.479579Z","level":"error","event":"25/05/12 02:39:34 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:34.480117Z","level":"error","event":"25/05/12 02:39:34 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:34.480920Z","level":"error","event":"25/05/12 02:39:34 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:34.481472Z","level":"error","event":"25/05/12 02:39:34 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:34.496276Z","level":"error","event":"25/05/12 02:39:34 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:34.496713Z","level":"error","event":"25/05/12 02:39:34 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:34.497071Z","level":"error","event":"25/05/12 02:39:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-fc186905-ee02-42fd-8a60-421f425b6186/pyspark-07715b78-45db-4d34-a6d8-bbb20e835690","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:34.512469Z","level":"error","event":"25/05/12 02:39:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-d050f053-2792-489a-9dc1-7a20544467e3","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:39:34.513019Z","level":"error","event":"25/05/12 02:39:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-fc186905-ee02-42fd-8a60-421f425b6186","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:05.020946","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:40:08.818475Z","level":"error","event":"25/05/12 02:40:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:09.275304Z","level":"error","event":"25/05/12 02:40:09 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:09.308713Z","level":"error","event":"25/05/12 02:40:09 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:09.309466Z","level":"error","event":"25/05/12 02:40:09 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:09.310288Z","level":"error","event":"25/05/12 02:40:09 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:09.642899Z","level":"error","event":"25/05/12 02:40:09 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:09.665247Z","level":"error","event":"25/05/12 02:40:09 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:09.665785Z","level":"error","event":"25/05/12 02:40:09 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:09.699785Z","level":"error","event":"25/05/12 02:40:09 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:09.720754Z","level":"error","event":"25/05/12 02:40:09 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:09.721322Z","level":"error","event":"25/05/12 02:40:09 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:09.721856Z","level":"error","event":"25/05/12 02:40:09 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:09.760138Z","level":"error","event":"25/05/12 02:40:09 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:09.782799Z","level":"error","event":"25/05/12 02:40:09 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:09.783488Z","level":"error","event":"25/05/12 02:40:09 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:09.857664Z","level":"error","event":"25/05/12 02:40:09 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:09.879250Z","level":"error","event":"25/05/12 02:40:09 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:09.879973Z","level":"error","event":"25/05/12 02:40:09 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:09.880657Z","level":"error","event":"25/05/12 02:40:09 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:09.881215Z","level":"error","event":"25/05/12 02:40:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:10.340801Z","level":"error","event":"25/05/12 02:40:10 INFO Utils: Successfully started service 'sparkDriver' on port 37397.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:10.393724Z","level":"error","event":"25/05/12 02:40:10 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:10.457963Z","level":"error","event":"25/05/12 02:40:10 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:10.498229Z","level":"error","event":"25/05/12 02:40:10 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:10.525757Z","level":"error","event":"25/05/12 02:40:10 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:10.526465Z","level":"error","event":"25/05/12 02:40:10 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:10.557819Z","level":"error","event":"25/05/12 02:40:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c61eb1c8-7f9a-4728-adc1-1500e0b74116","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:10.591663Z","level":"error","event":"25/05/12 02:40:10 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:10.635907Z","level":"error","event":"25/05/12 02:40:10 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.003903Z","level":"error","event":"25/05/12 02:40:11 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.154593Z","level":"error","event":"25/05/12 02:40:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.251125Z","level":"error","event":"25/05/12 02:40:11 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.251941Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.252669Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.253398Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.254078Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.254614Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.255181Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.255934Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.256718Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.257478Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.258344Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.259400Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.260282Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.260964Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.261670Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.262348Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.262974Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.263526Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.264180Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.264839Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.265594Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.266171Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.266738Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.267683Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.294988Z","level":"error","event":"25/05/12 02:40:11 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.295856Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.296659Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.297603Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.299566Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.300710Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.301500Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.302407Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.303335Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.304661Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.305610Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.306457Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.307564Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.308411Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.309617Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.310711Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.311525Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.312464Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.314614Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.315778Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.316949Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.317952Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.318808Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.319637Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.320742Z","level":"error","event":"25/05/12 02:40:11 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.322941Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.330869Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.337774Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.339976Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.343050Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.344587Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.345341Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.346812Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.348171Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.349258Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.351542Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.352279Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.352966Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.354151Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.355178Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.356068Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.358251Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.359720Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.361496Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.362392Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.363111Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.363957Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.364806Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.365561Z","level":"error","event":"25/05/12 02:40:11 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.366229Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.367002Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.391008Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.391691Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.392337Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.392991Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.393754Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.394473Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.395266Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.395980Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.396569Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.397030Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.397546Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.398059Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.398866Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.399341Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.399767Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.400296Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.400845Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.401321Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.401853Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.402450Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.403059Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.426175Z","level":"error","event":"25/05/12 02:40:11 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.427028Z","level":"error","event":"25/05/12 02:40:11 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.427724Z","level":"error","event":"25/05/12 02:40:11 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.450770Z","level":"error","event":"25/05/12 02:40:11 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.451411Z","level":"error","event":"25/05/12 02:40:11 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1f04a8db for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.476207Z","level":"error","event":"25/05/12 02:40:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35381.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.500213Z","level":"error","event":"25/05/12 02:40:11 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:35381","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.500914Z","level":"error","event":"25/05/12 02:40:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.501652Z","level":"error","event":"25/05/12 02:40:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 35381, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.526972Z","level":"error","event":"25/05/12 02:40:11 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:35381 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 35381, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.527860Z","level":"error","event":"25/05/12 02:40:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 35381, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:11.528837Z","level":"error","event":"25/05/12 02:40:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 35381, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:12.382017Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:40:13.205379Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:40:14.205601Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:40:14.381495Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:40:15.381603Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:40:15.474171Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:40:16.474382Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:40:16.649642Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:40:17.649739Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:40:17.776136Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:40:18.776302Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:40:18.785135Z","level":"error","event":"25/05/12 02:40:18 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:18.794339Z","level":"error","event":"25/05/12 02:40:18 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:20.802432","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:40:21.163416Z","level":"error","event":"25/05/12 02:40:21 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:21.163996Z","level":"error","event":"25/05/12 02:40:21 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:21.177524Z","level":"error","event":"25/05/12 02:40:21 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:21.194126Z","level":"error","event":"25/05/12 02:40:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:21.213211Z","level":"error","event":"25/05/12 02:40:21 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:21.221119Z","level":"error","event":"25/05/12 02:40:21 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:21.221745Z","level":"error","event":"25/05/12 02:40:21 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:21.230443Z","level":"error","event":"25/05/12 02:40:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:21.239567Z","level":"error","event":"25/05/12 02:40:21 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:21.247801Z","level":"error","event":"25/05/12 02:40:21 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:21.248406Z","level":"error","event":"25/05/12 02:40:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-a23aae62-3889-49e2-b829-3e3c4086805c","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:21.248866Z","level":"error","event":"25/05/12 02:40:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-c53a4ecf-9ee9-4aff-a2d4-80d46c8b466c","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:21.258049Z","level":"error","event":"25/05/12 02:40:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-a23aae62-3889-49e2-b829-3e3c4086805c/pyspark-407ad47b-7980-48d3-a0e9-9806477470f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:51.823455","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:40:56.479418Z","level":"error","event":"25/05/12 02:40:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:56.811360Z","level":"error","event":"25/05/12 02:40:56 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:56.832885Z","level":"error","event":"25/05/12 02:40:56 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:56.833563Z","level":"error","event":"25/05/12 02:40:56 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:56.834214Z","level":"error","event":"25/05/12 02:40:56 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:57.029681Z","level":"error","event":"25/05/12 02:40:57 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:57.053577Z","level":"error","event":"25/05/12 02:40:57 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:57.054248Z","level":"error","event":"25/05/12 02:40:57 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:57.094950Z","level":"error","event":"25/05/12 02:40:57 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:57.116370Z","level":"error","event":"25/05/12 02:40:57 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:57.117075Z","level":"error","event":"25/05/12 02:40:57 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:57.117680Z","level":"error","event":"25/05/12 02:40:57 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:57.157192Z","level":"error","event":"25/05/12 02:40:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:57.183267Z","level":"error","event":"25/05/12 02:40:57 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:57.203778Z","level":"error","event":"25/05/12 02:40:57 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:58.123063Z","level":"error","event":"25/05/12 02:40:58 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:58.144737Z","level":"error","event":"25/05/12 02:40:58 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:58.145369Z","level":"error","event":"25/05/12 02:40:58 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:58.146249Z","level":"error","event":"25/05/12 02:40:58 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:58.147092Z","level":"error","event":"25/05/12 02:40:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:59.824554Z","level":"error","event":"25/05/12 02:40:59 INFO Utils: Successfully started service 'sparkDriver' on port 38843.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:59.889250Z","level":"error","event":"25/05/12 02:40:59 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:59.951803Z","level":"error","event":"25/05/12 02:40:59 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:40:59.994304Z","level":"error","event":"25/05/12 02:40:59 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.016537Z","level":"error","event":"25/05/12 02:40:59 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.017242Z","level":"error","event":"25/05/12 02:41:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.043431Z","level":"error","event":"25/05/12 02:41:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-21ce110d-e55e-453c-9a9e-3f9e408b060f","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.073351Z","level":"error","event":"25/05/12 02:41:00 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.158918Z","level":"error","event":"25/05/12 02:41:00 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.449664Z","level":"error","event":"25/05/12 02:41:00 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.642992Z","level":"error","event":"25/05/12 02:41:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.758374Z","level":"error","event":"25/05/12 02:41:00 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.759332Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.759983Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.760645Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.761296Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.761901Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.762575Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.763220Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.763772Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.764372Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.764911Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.766745Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.780032Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.781936Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.798684Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.801362Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.802356Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.821979Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.823181Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.824084Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.825523Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.826485Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.827470Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.828130Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.852082Z","level":"error","event":"25/05/12 02:41:00 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.853182Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.853978Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.856862Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.857940Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.858752Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.859468Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.860264Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.861229Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.862178Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.862895Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.863638Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.864405Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.865308Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.866055Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.866749Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.867487Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.868262Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.869024Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.869721Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.870584Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.871482Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.872168Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.872908Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.874164Z","level":"error","event":"25/05/12 02:41:00 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.874917Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.875822Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.876574Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.877549Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.878286Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.878932Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.879546Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.880195Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.880909Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.881551Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.882512Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.883252Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.883972Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.884649Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.885285Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.885949Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.886651Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.887410Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.888080Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.888645Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.889309Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.889926Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.890616Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.891209Z","level":"error","event":"25/05/12 02:41:00 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.891813Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.892481Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.918448Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.919185Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.920049Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.921016Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.921745Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.922593Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.923480Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.924372Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.925054Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.925819Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.926689Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.927477Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.928123Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.928749Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.929532Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.930519Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.931326Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.932091Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.932831Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.933662Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.936154Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.960198Z","level":"error","event":"25/05/12 02:41:00 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.960972Z","level":"error","event":"25/05/12 02:41:00 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.961679Z","level":"error","event":"25/05/12 02:41:00 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.995306Z","level":"error","event":"25/05/12 02:41:00 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:00.996074Z","level":"error","event":"25/05/12 02:41:00 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1f04a8db for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:01.032797Z","level":"error","event":"25/05/12 02:41:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45279.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:01.057443Z","level":"error","event":"25/05/12 02:41:01 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:45279","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:01.058140Z","level":"error","event":"25/05/12 02:41:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:01.058736Z","level":"error","event":"25/05/12 02:41:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 45279, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:01.059270Z","level":"error","event":"25/05/12 02:41:01 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:45279 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 45279, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:01.080232Z","level":"error","event":"25/05/12 02:41:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 45279, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:01.081371Z","level":"error","event":"25/05/12 02:41:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 45279, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:01.938676Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:41:03.484185Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:41:04.484491Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:41:04.656621Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:41:05.656808Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:41:05.783411Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:41:06.781673Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:41:06.895087Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:41:07.895242Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:41:07.989370Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:41:08.989772Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:41:09.008483Z","level":"error","event":"25/05/12 02:41:09 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:09.008785Z","level":"error","event":"25/05/12 02:41:09 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:10.771358","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:41:11.151207Z","level":"error","event":"25/05/12 02:41:11 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:11.159646Z","level":"error","event":"25/05/12 02:41:11 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:11.169822Z","level":"error","event":"25/05/12 02:41:11 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:11.190597Z","level":"error","event":"25/05/12 02:41:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:11.210493Z","level":"error","event":"25/05/12 02:41:11 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:11.219148Z","level":"error","event":"25/05/12 02:41:11 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:11.228757Z","level":"error","event":"25/05/12 02:41:11 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:11.229449Z","level":"error","event":"25/05/12 02:41:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:11.242779Z","level":"error","event":"25/05/12 02:41:11 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:11.251135Z","level":"error","event":"25/05/12 02:41:11 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:11.251730Z","level":"error","event":"25/05/12 02:41:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-337052d5-517b-45ec-906f-ba889fe40acf","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:11.252285Z","level":"error","event":"25/05/12 02:41:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-535c9485-7b88-4cc1-a5d3-2d5986d45a36","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:11.260896Z","level":"error","event":"25/05/12 02:41:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-535c9485-7b88-4cc1-a5d3-2d5986d45a36/pyspark-acaf26ee-6f1b-40b8-9905-c694b31cf5d6","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:41.752931","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:41:46.146216Z","level":"error","event":"25/05/12 02:41:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:46.408006Z","level":"error","event":"25/05/12 02:41:46 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:46.426187Z","level":"error","event":"25/05/12 02:41:46 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:46.426592Z","level":"error","event":"25/05/12 02:41:46 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:46.426970Z","level":"error","event":"25/05/12 02:41:46 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:46.635692Z","level":"error","event":"25/05/12 02:41:46 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:46.658832Z","level":"error","event":"25/05/12 02:41:46 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:46.659406Z","level":"error","event":"25/05/12 02:41:46 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:46.684589Z","level":"error","event":"25/05/12 02:41:46 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:46.702810Z","level":"error","event":"25/05/12 02:41:46 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:46.703522Z","level":"error","event":"25/05/12 02:41:46 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:46.704249Z","level":"error","event":"25/05/12 02:41:46 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:46.726249Z","level":"error","event":"25/05/12 02:41:46 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:46.743605Z","level":"error","event":"25/05/12 02:41:46 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:46.744158Z","level":"error","event":"25/05/12 02:41:46 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:46.800238Z","level":"error","event":"25/05/12 02:41:46 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:46.818398Z","level":"error","event":"25/05/12 02:41:46 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:46.818925Z","level":"error","event":"25/05/12 02:41:46 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:46.819346Z","level":"error","event":"25/05/12 02:41:46 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:46.819796Z","level":"error","event":"25/05/12 02:41:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.111370Z","level":"error","event":"25/05/12 02:41:47 INFO Utils: Successfully started service 'sparkDriver' on port 39329.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.147470Z","level":"error","event":"25/05/12 02:41:47 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.193514Z","level":"error","event":"25/05/12 02:41:47 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.219805Z","level":"error","event":"25/05/12 02:41:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.238959Z","level":"error","event":"25/05/12 02:41:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.239459Z","level":"error","event":"25/05/12 02:41:47 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.259836Z","level":"error","event":"25/05/12 02:41:47 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3562e866-d302-435e-bb88-b6ccdc59c1b3","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.284201Z","level":"error","event":"25/05/12 02:41:47 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.319615Z","level":"error","event":"25/05/12 02:41:47 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.483397Z","level":"error","event":"25/05/12 02:41:47 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.570007Z","level":"error","event":"25/05/12 02:41:47 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.622673Z","level":"error","event":"25/05/12 02:41:47 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.623171Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.623637Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.624069Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.624607Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.625045Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.625560Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.626004Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.626471Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.626950Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.627345Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.627690Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.628039Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.628452Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.629205Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.629851Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.630444Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.630965Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.631401Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.631819Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.632273Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.632799Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.633287Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.633846Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.652794Z","level":"error","event":"25/05/12 02:41:47 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.653438Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.654058Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.654638Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.655252Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.655948Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.656612Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.657195Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.657844Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.658516Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.659197Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.659963Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.660718Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.661367Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.662069Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.662724Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.663361Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.663992Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.664544Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.665068Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.665595Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.666260Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.666916Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.667545Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.668172Z","level":"error","event":"25/05/12 02:41:47 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.668693Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.669251Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.669820Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.670324Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.670935Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.671528Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.672011Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.672477Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.672931Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.673311Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.673669Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.674071Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.674607Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.675073Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.675647Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.676164Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.676614Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.677027Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.677449Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.677823Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.678266Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.678815Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.679425Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.679996Z","level":"error","event":"25/05/12 02:41:47 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.680537Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.681027Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.697390Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.697876Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.698304Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.698684Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.699143Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.699566Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.700081Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.700508Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.700921Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.701335Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.701746Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.702153Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.702543Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.702892Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.703264Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.703623Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.704005Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.704435Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.705017Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.705582Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.706077Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.722008Z","level":"error","event":"25/05/12 02:41:47 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.722691Z","level":"error","event":"25/05/12 02:41:47 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.737831Z","level":"error","event":"25/05/12 02:41:47 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.738258Z","level":"error","event":"25/05/12 02:41:47 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.738734Z","level":"error","event":"25/05/12 02:41:47 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2c9036a8 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.764655Z","level":"error","event":"25/05/12 02:41:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46191.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.778755Z","level":"error","event":"25/05/12 02:41:47 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:46191","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.779232Z","level":"error","event":"25/05/12 02:41:47 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.779717Z","level":"error","event":"25/05/12 02:41:47 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 46191, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.780164Z","level":"error","event":"25/05/12 02:41:47 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:46191 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 46191, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.794912Z","level":"error","event":"25/05/12 02:41:47 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 46191, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:47.795336Z","level":"error","event":"25/05/12 02:41:47 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 46191, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:48.239662Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:41:48.884802Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:41:49.885139Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:41:50.088277Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:41:51.088732Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:41:51.170222Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:41:52.170611Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:41:52.255556Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:41:53.255716Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:41:53.350077Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:41:54.350329Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:41:54.372296Z","level":"error","event":"25/05/12 02:41:54 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:54.373042Z","level":"error","event":"25/05/12 02:41:54 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:55.665252","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:41:56.112904Z","level":"error","event":"25/05/12 02:41:56 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:56.113428Z","level":"error","event":"25/05/12 02:41:56 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:56.134481Z","level":"error","event":"25/05/12 02:41:56 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:56.151438Z","level":"error","event":"25/05/12 02:41:56 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:56.168172Z","level":"error","event":"25/05/12 02:41:56 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:56.168913Z","level":"error","event":"25/05/12 02:41:56 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:56.186154Z","level":"error","event":"25/05/12 02:41:56 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:56.186624Z","level":"error","event":"25/05/12 02:41:56 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:56.202981Z","level":"error","event":"25/05/12 02:41:56 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:56.203496Z","level":"error","event":"25/05/12 02:41:56 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:56.203925Z","level":"error","event":"25/05/12 02:41:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-f414e6c2-1391-464d-b359-64e3b8d146b2","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:56.204414Z","level":"error","event":"25/05/12 02:41:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-f414e6c2-1391-464d-b359-64e3b8d146b2/pyspark-72c43c66-4a28-4d10-a864-5d613aaebe70","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:41:56.221502Z","level":"error","event":"25/05/12 02:41:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-eee069ea-63b0-4738-95ee-c5c3611c1079","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:26.998862","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:42:30.467831Z","level":"error","event":"25/05/12 02:42:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:30.761008Z","level":"error","event":"25/05/12 02:42:30 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:30.787961Z","level":"error","event":"25/05/12 02:42:30 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:30.788630Z","level":"error","event":"25/05/12 02:42:30 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:30.789296Z","level":"error","event":"25/05/12 02:42:30 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:30.971485Z","level":"error","event":"25/05/12 02:42:30 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:31.005098Z","level":"error","event":"25/05/12 02:42:30 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:31.005719Z","level":"error","event":"25/05/12 02:42:30 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:31.047820Z","level":"error","event":"25/05/12 02:42:31 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:31.069044Z","level":"error","event":"25/05/12 02:42:31 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:31.069899Z","level":"error","event":"25/05/12 02:42:31 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:31.070780Z","level":"error","event":"25/05/12 02:42:31 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:31.098283Z","level":"error","event":"25/05/12 02:42:31 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:31.117421Z","level":"error","event":"25/05/12 02:42:31 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:31.117964Z","level":"error","event":"25/05/12 02:42:31 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:31.174618Z","level":"error","event":"25/05/12 02:42:31 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:31.193866Z","level":"error","event":"25/05/12 02:42:31 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:31.194514Z","level":"error","event":"25/05/12 02:42:31 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:31.195048Z","level":"error","event":"25/05/12 02:42:31 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:31.195636Z","level":"error","event":"25/05/12 02:42:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:31.534781Z","level":"error","event":"25/05/12 02:42:31 INFO Utils: Successfully started service 'sparkDriver' on port 41753.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:31.586993Z","level":"error","event":"25/05/12 02:42:31 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:31.655228Z","level":"error","event":"25/05/12 02:42:31 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:31.690277Z","level":"error","event":"25/05/12 02:42:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:31.709234Z","level":"error","event":"25/05/12 02:42:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:31.709677Z","level":"error","event":"25/05/12 02:42:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:31.735467Z","level":"error","event":"25/05/12 02:42:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-49b05172-5d2e-4926-a9b7-9d25c1318365","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:31.758332Z","level":"error","event":"25/05/12 02:42:31 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:31.781962Z","level":"error","event":"25/05/12 02:42:31 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:31.969107Z","level":"error","event":"25/05/12 02:42:31 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.081498Z","level":"error","event":"25/05/12 02:42:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.139469Z","level":"error","event":"25/05/12 02:42:32 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.139909Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.140306Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.140666Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.141039Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.141400Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.141707Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.142049Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.142413Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.142869Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.143305Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.143791Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.144280Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.144808Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.145327Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.145851Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.146380Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.146885Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.147417Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.147942Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.148466Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.149071Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.149749Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.150506Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.171207Z","level":"error","event":"25/05/12 02:42:32 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.172020Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.172843Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.173525Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.174478Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.175272Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.176000Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.176706Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.177339Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.178054Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.178799Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.179548Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.180244Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.180832Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.181437Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.182233Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.182799Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.183266Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.183897Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.184396Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.184921Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.185582Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.186141Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.186697Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.187223Z","level":"error","event":"25/05/12 02:42:32 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.187760Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.188295Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.189055Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.189768Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.190330Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.191003Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.191631Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.192152Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.192619Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.193096Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.193737Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.194337Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.194858Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.195468Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.195964Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.196312Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.196594Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.196868Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.197163Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.197527Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.197991Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.198470Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.198837Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.199381Z","level":"error","event":"25/05/12 02:42:32 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.199795Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.200216Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.214155Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.214837Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.215432Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.216018Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.216618Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.217380Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.219073Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.219774Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.220333Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.220984Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.221529Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.222187Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.222800Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.223387Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.224062Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.224645Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.225191Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.225632Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.226067Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.226451Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.226822Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.248214Z","level":"error","event":"25/05/12 02:42:32 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.264854Z","level":"error","event":"25/05/12 02:42:32 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.265262Z","level":"error","event":"25/05/12 02:42:32 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.265650Z","level":"error","event":"25/05/12 02:42:32 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.266074Z","level":"error","event":"25/05/12 02:42:32 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6ff21454 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.291306Z","level":"error","event":"25/05/12 02:42:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39559.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.305699Z","level":"error","event":"25/05/12 02:42:32 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:39559","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.306163Z","level":"error","event":"25/05/12 02:42:32 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.306609Z","level":"error","event":"25/05/12 02:42:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 39559, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.307040Z","level":"error","event":"25/05/12 02:42:32 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:39559 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 39559, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.319937Z","level":"error","event":"25/05/12 02:42:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 39559, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.320321Z","level":"error","event":"25/05/12 02:42:32 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 39559, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:32.713689Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:42:33.375035Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:42:36.480127Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:42:36.590875Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:42:37.591072Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:42:37.672931Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:42:38.751300Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:42:38.815613Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:42:39.815976Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:42:39.934876Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:42:40.934992Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:42:40.948303Z","level":"error","event":"25/05/12 02:42:40 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:40.948668Z","level":"error","event":"25/05/12 02:42:40 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:42.674248","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:42:43.166917Z","level":"error","event":"25/05/12 02:42:43 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:43.167832Z","level":"error","event":"25/05/12 02:42:43 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:43.191051Z","level":"error","event":"25/05/12 02:42:43 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:43.232594Z","level":"error","event":"25/05/12 02:42:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:43.259632Z","level":"error","event":"25/05/12 02:42:43 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:43.260398Z","level":"error","event":"25/05/12 02:42:43 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:43.286283Z","level":"error","event":"25/05/12 02:42:43 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:43.286863Z","level":"error","event":"25/05/12 02:42:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:43.310312Z","level":"error","event":"25/05/12 02:42:43 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:43.311308Z","level":"error","event":"25/05/12 02:42:43 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:43.312711Z","level":"error","event":"25/05/12 02:42:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-d7c2316d-69a3-4495-9705-a2a4c388809f","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:43.351165Z","level":"error","event":"25/05/12 02:42:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-485252ee-278b-4f0d-b784-92f5ef61ff9f/pyspark-3521f994-a03f-4f26-8e97-609e0b21a0ba","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:42:43.352082Z","level":"error","event":"25/05/12 02:42:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-485252ee-278b-4f0d-b784-92f5ef61ff9f","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:14.639637","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:43:19.193882Z","level":"error","event":"25/05/12 02:43:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:19.630439Z","level":"error","event":"25/05/12 02:43:19 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:19.659981Z","level":"error","event":"25/05/12 02:43:19 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:19.660727Z","level":"error","event":"25/05/12 02:43:19 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:19.661343Z","level":"error","event":"25/05/12 02:43:19 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:19.841397Z","level":"error","event":"25/05/12 02:43:19 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:19.861577Z","level":"error","event":"25/05/12 02:43:19 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:19.862210Z","level":"error","event":"25/05/12 02:43:19 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:19.911886Z","level":"error","event":"25/05/12 02:43:19 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:19.932931Z","level":"error","event":"25/05/12 02:43:19 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:19.933569Z","level":"error","event":"25/05/12 02:43:19 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:19.934165Z","level":"error","event":"25/05/12 02:43:19 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:19.964052Z","level":"error","event":"25/05/12 02:43:19 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:19.983638Z","level":"error","event":"25/05/12 02:43:19 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:19.984235Z","level":"error","event":"25/05/12 02:43:19 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:20.059759Z","level":"error","event":"25/05/12 02:43:20 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:20.081991Z","level":"error","event":"25/05/12 02:43:20 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:20.082713Z","level":"error","event":"25/05/12 02:43:20 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:20.083365Z","level":"error","event":"25/05/12 02:43:20 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:20.084038Z","level":"error","event":"25/05/12 02:43:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:20.671085Z","level":"error","event":"25/05/12 02:43:20 INFO Utils: Successfully started service 'sparkDriver' on port 39891.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:20.739125Z","level":"error","event":"25/05/12 02:43:20 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:20.819441Z","level":"error","event":"25/05/12 02:43:20 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:20.863149Z","level":"error","event":"25/05/12 02:43:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:20.889533Z","level":"error","event":"25/05/12 02:43:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:20.890187Z","level":"error","event":"25/05/12 02:43:20 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:20.930037Z","level":"error","event":"25/05/12 02:43:20 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ef961c9b-10ca-4c72-b840-5adb6554e353","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:20.958532Z","level":"error","event":"25/05/12 02:43:20 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:20.990449Z","level":"error","event":"25/05/12 02:43:20 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.246902Z","level":"error","event":"25/05/12 02:43:21 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.401884Z","level":"error","event":"25/05/12 02:43:21 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.474495Z","level":"error","event":"25/05/12 02:43:21 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.475111Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.475583Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.476015Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.476459Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.477057Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.477554Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.478149Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.479089Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.479849Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.480682Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.481446Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.482141Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.482728Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.483372Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.484063Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.484794Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.485774Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.486470Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.487169Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.487891Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.488645Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.489297Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.489760Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.528274Z","level":"error","event":"25/05/12 02:43:21 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.529174Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.529998Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.530744Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.531492Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.532227Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.532945Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.533628Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.534297Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.535092Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.535821Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.536421Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.537028Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.537528Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.537959Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.538449Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.538911Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.539778Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.540361Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.540903Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.541468Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.541950Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.542551Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.543263Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.543930Z","level":"error","event":"25/05/12 02:43:21 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.544588Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.545357Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.546148Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.546883Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.548022Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.548592Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.549184Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.550193Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.550948Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.551707Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.552425Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.552873Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.553349Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.553787Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.554512Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.555142Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.555763Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.556296Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.556802Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.557242Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.558024Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.559162Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.559962Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.560569Z","level":"error","event":"25/05/12 02:43:21 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.561379Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.562076Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.594786Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.595876Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.596645Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.597673Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.598784Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.599758Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.600525Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.601372Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.602191Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.602904Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.603524Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.604137Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.604896Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.605632Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.606722Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.608442Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.609436Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.610706Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.611593Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.612425Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.613107Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.638726Z","level":"error","event":"25/05/12 02:43:21 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.639296Z","level":"error","event":"25/05/12 02:43:21 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.639803Z","level":"error","event":"25/05/12 02:43:21 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.657837Z","level":"error","event":"25/05/12 02:43:21 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.658295Z","level":"error","event":"25/05/12 02:43:21 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@73663445 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.692218Z","level":"error","event":"25/05/12 02:43:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42359.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.712781Z","level":"error","event":"25/05/12 02:43:21 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:42359","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.713574Z","level":"error","event":"25/05/12 02:43:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.714187Z","level":"error","event":"25/05/12 02:43:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 42359, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.734184Z","level":"error","event":"25/05/12 02:43:21 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:42359 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 42359, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.734661Z","level":"error","event":"25/05/12 02:43:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 42359, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:21.735089Z","level":"error","event":"25/05/12 02:43:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 42359, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:22.591034Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:43:23.170309Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:43:24.170507Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:43:24.284952Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:43:25.285182Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:43:25.374247Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:43:26.374517Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:43:26.528594Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:43:27.528882Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:43:27.671826Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:43:28.671926Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:43:28.696586Z","level":"error","event":"25/05/12 02:43:28 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:28.715167Z","level":"error","event":"25/05/12 02:43:28 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:30.761058","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:43:31.227982Z","level":"error","event":"25/05/12 02:43:31 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:31.228583Z","level":"error","event":"25/05/12 02:43:31 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:31.251174Z","level":"error","event":"25/05/12 02:43:31 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:31.272415Z","level":"error","event":"25/05/12 02:43:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:31.293860Z","level":"error","event":"25/05/12 02:43:31 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:31.294316Z","level":"error","event":"25/05/12 02:43:31 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:31.314048Z","level":"error","event":"25/05/12 02:43:31 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:31.314655Z","level":"error","event":"25/05/12 02:43:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:31.334399Z","level":"error","event":"25/05/12 02:43:31 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:31.334895Z","level":"error","event":"25/05/12 02:43:31 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:31.335384Z","level":"error","event":"25/05/12 02:43:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-24767b78-63cf-42f5-bfb4-e8c146ee04c5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:31.356089Z","level":"error","event":"25/05/12 02:43:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-24767b78-63cf-42f5-bfb4-e8c146ee04c5/pyspark-5e43a8c4-a308-4138-8ef5-6481f17373e0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:43:31.356700Z","level":"error","event":"25/05/12 02:43:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-638e5217-1e82-4e5c-8cfb-83575dd55e82","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:26.772821","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:44:32.296073Z","level":"error","event":"25/05/12 02:44:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:32.662427Z","level":"error","event":"25/05/12 02:44:32 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:32.684897Z","level":"error","event":"25/05/12 02:44:32 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:32.685802Z","level":"error","event":"25/05/12 02:44:32 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:32.686456Z","level":"error","event":"25/05/12 02:44:32 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:32.880727Z","level":"error","event":"25/05/12 02:44:32 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:32.899444Z","level":"error","event":"25/05/12 02:44:32 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:32.899934Z","level":"error","event":"25/05/12 02:44:32 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:32.933415Z","level":"error","event":"25/05/12 02:44:32 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:32.957224Z","level":"error","event":"25/05/12 02:44:32 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:32.957878Z","level":"error","event":"25/05/12 02:44:32 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:32.958830Z","level":"error","event":"25/05/12 02:44:32 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:32.980690Z","level":"error","event":"25/05/12 02:44:32 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:32.999754Z","level":"error","event":"25/05/12 02:44:32 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:33.000362Z","level":"error","event":"25/05/12 02:44:32 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:33.074326Z","level":"error","event":"25/05/12 02:44:33 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:33.089268Z","level":"error","event":"25/05/12 02:44:33 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:33.089777Z","level":"error","event":"25/05/12 02:44:33 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:33.090153Z","level":"error","event":"25/05/12 02:44:33 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:33.090487Z","level":"error","event":"25/05/12 02:44:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:33.434738Z","level":"error","event":"25/05/12 02:44:33 INFO Utils: Successfully started service 'sparkDriver' on port 41343.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:33.484508Z","level":"error","event":"25/05/12 02:44:33 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:33.542191Z","level":"error","event":"25/05/12 02:44:33 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:33.571015Z","level":"error","event":"25/05/12 02:44:33 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:33.591619Z","level":"error","event":"25/05/12 02:44:33 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:33.592257Z","level":"error","event":"25/05/12 02:44:33 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:33.616112Z","level":"error","event":"25/05/12 02:44:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3c824a19-ede7-4f6b-a2db-137cf550b52e","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:33.641072Z","level":"error","event":"25/05/12 02:44:33 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:33.665624Z","level":"error","event":"25/05/12 02:44:33 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:33.872932Z","level":"error","event":"25/05/12 02:44:33 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:33.986355Z","level":"error","event":"25/05/12 02:44:33 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.063838Z","level":"error","event":"25/05/12 02:44:34 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.064538Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.065149Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.065802Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.066831Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.067593Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.068255Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.068952Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.069589Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.070217Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.070864Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.071427Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.071964Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.072544Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.073078Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.073621Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.074204Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.074902Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.075665Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.076366Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.077073Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.077811Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.078556Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.079271Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.102149Z","level":"error","event":"25/05/12 02:44:34 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.102781Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.103410Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.104034Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.104603Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.105361Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.106377Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.107545Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.108443Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.109246Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.110037Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.111013Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.111714Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.112403Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.113025Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.113636Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.114129Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.114712Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.115381Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.115926Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.116450Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.116902Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.117334Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.117853Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.118279Z","level":"error","event":"25/05/12 02:44:34 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.118777Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.119208Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.119597Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.120211Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.121162Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.122049Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.122851Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.123740Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.124743Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.125609Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.126393Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.127051Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.127578Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.128044Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.128440Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.128832Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.129215Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.129639Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.130111Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.130548Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.130994Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.131371Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.131735Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.132094Z","level":"error","event":"25/05/12 02:44:34 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.132504Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.132961Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.150712Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.151410Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.152043Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.152823Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.153595Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.154191Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.154832Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.155487Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.156215Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.156898Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.157585Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.158199Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.158697Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.159270Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.159842Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.160435Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.161079Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.161749Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.162351Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.162933Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.163468Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.187942Z","level":"error","event":"25/05/12 02:44:34 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.209755Z","level":"error","event":"25/05/12 02:44:34 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.210670Z","level":"error","event":"25/05/12 02:44:34 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.211361Z","level":"error","event":"25/05/12 02:44:34 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.212044Z","level":"error","event":"25/05/12 02:44:34 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@73663445 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.244208Z","level":"error","event":"25/05/12 02:44:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41189.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.266604Z","level":"error","event":"25/05/12 02:44:34 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:41189","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.267283Z","level":"error","event":"25/05/12 02:44:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.267855Z","level":"error","event":"25/05/12 02:44:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 41189, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.268389Z","level":"error","event":"25/05/12 02:44:34 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:41189 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 41189, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.287876Z","level":"error","event":"25/05/12 02:44:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 41189, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:34.288473Z","level":"error","event":"25/05/12 02:44:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 41189, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:35.017139Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:44:35.704166Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:44:36.704414Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:44:36.828504Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:44:37.828625Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:44:37.915143Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:44:38.915343Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:44:39.111396Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:44:40.111610Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:44:40.210137Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:44:41.210433Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:44:41.251659Z","level":"error","event":"25/05/12 02:44:41 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:41.252552Z","level":"error","event":"25/05/12 02:44:41 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:46.751818","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:44:47.260079Z","level":"error","event":"25/05/12 02:44:47 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:47.261498Z","level":"error","event":"25/05/12 02:44:47 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:47.295126Z","level":"error","event":"25/05/12 02:44:47 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:47.324815Z","level":"error","event":"25/05/12 02:44:47 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:47.354189Z","level":"error","event":"25/05/12 02:44:47 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:47.384557Z","level":"error","event":"25/05/12 02:44:47 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:47.387133Z","level":"error","event":"25/05/12 02:44:47 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:47.388401Z","level":"error","event":"25/05/12 02:44:47 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:47.428642Z","level":"error","event":"25/05/12 02:44:47 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:47.451357Z","level":"error","event":"25/05/12 02:44:47 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:47.452095Z","level":"error","event":"25/05/12 02:44:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-2c14d240-387d-41f3-8dd0-764c51d7d37a","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:47.452731Z","level":"error","event":"25/05/12 02:44:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-75d4869d-e0a9-44b6-86ab-b9fb3706bcbf/pyspark-284d509e-efd9-4679-aef1-838c76f16ea5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:44:47.480666Z","level":"error","event":"25/05/12 02:44:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-75d4869d-e0a9-44b6-86ab-b9fb3706bcbf","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:18.451872","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:45:26.200250Z","level":"error","event":"25/05/12 02:45:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:26.918381Z","level":"error","event":"25/05/12 02:45:26 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:27.007066Z","level":"error","event":"25/05/12 02:45:26 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:27.011030Z","level":"error","event":"25/05/12 02:45:26 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:27.011918Z","level":"error","event":"25/05/12 02:45:26 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:27.533572Z","level":"error","event":"25/05/12 02:45:27 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:27.554436Z","level":"error","event":"25/05/12 02:45:27 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:27.555588Z","level":"error","event":"25/05/12 02:45:27 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:27.690189Z","level":"error","event":"25/05/12 02:45:27 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:27.738831Z","level":"error","event":"25/05/12 02:45:27 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:27.739660Z","level":"error","event":"25/05/12 02:45:27 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:27.740665Z","level":"error","event":"25/05/12 02:45:27 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:27.775236Z","level":"error","event":"25/05/12 02:45:27 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:27.814079Z","level":"error","event":"25/05/12 02:45:27 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:27.826884Z","level":"error","event":"25/05/12 02:45:27 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:28.092314Z","level":"error","event":"25/05/12 02:45:28 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:28.114007Z","level":"error","event":"25/05/12 02:45:28 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:28.114701Z","level":"error","event":"25/05/12 02:45:28 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:28.115423Z","level":"error","event":"25/05/12 02:45:28 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:28.116207Z","level":"error","event":"25/05/12 02:45:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:29.104964Z","level":"error","event":"25/05/12 02:45:29 INFO Utils: Successfully started service 'sparkDriver' on port 37259.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:29.190524Z","level":"error","event":"25/05/12 02:45:29 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:29.274268Z","level":"error","event":"25/05/12 02:45:29 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:29.305065Z","level":"error","event":"25/05/12 02:45:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:29.319000Z","level":"error","event":"25/05/12 02:45:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:29.319743Z","level":"error","event":"25/05/12 02:45:29 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:29.367220Z","level":"error","event":"25/05/12 02:45:29 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cbc6c46c-5675-420a-a24f-3450fa95aecb","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:29.430020Z","level":"error","event":"25/05/12 02:45:29 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:29.482391Z","level":"error","event":"25/05/12 02:45:29 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:29.831815Z","level":"error","event":"25/05/12 02:45:29 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.070970Z","level":"error","event":"25/05/12 02:45:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.179074Z","level":"error","event":"25/05/12 02:45:30 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.180434Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.185823Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.186731Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.187449Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.188106Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.188893Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.189697Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.190568Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.192907Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.193795Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.194646Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.202463Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.203736Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.204790Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.205518Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.206273Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.207114Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.207878Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.208720Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.209569Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.210383Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.211116Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.211834Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.225468Z","level":"error","event":"25/05/12 02:45:30 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.226698Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.227706Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.228883Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.229749Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.230883Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.231885Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.233407Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.234355Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.235310Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.236254Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.237094Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.237923Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.238659Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.239380Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.240166Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.240877Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.241569Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.242277Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.243031Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.243628Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.244309Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.245070Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.245774Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.246551Z","level":"error","event":"25/05/12 02:45:30 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.250021Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.251241Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.252192Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.253016Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.253896Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.254884Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.255764Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.256575Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.261023Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.262018Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.264056Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.265800Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.269495Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.270399Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.271261Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.272303Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.273377Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.274324Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.275839Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.278087Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.279504Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.283736Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.293164Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.298307Z","level":"error","event":"25/05/12 02:45:30 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.299197Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.300167Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.384187Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.395281Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.396579Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.397510Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.402071Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.417193Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.418068Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.418859Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.419624Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.420391Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.422353Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.423306Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.424246Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.428123Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.441308Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.442198Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.451862Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.466463Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.481262Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.495866Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.510302Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.719033Z","level":"error","event":"25/05/12 02:45:30 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.775094Z","level":"error","event":"25/05/12 02:45:30 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.776100Z","level":"error","event":"25/05/12 02:45:30 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.833698Z","level":"error","event":"25/05/12 02:45:30 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.834676Z","level":"error","event":"25/05/12 02:45:30 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1cc616da for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.922373Z","level":"error","event":"25/05/12 02:45:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39129.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.924156Z","level":"error","event":"25/05/12 02:45:30 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:39129","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.925306Z","level":"error","event":"25/05/12 02:45:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.941087Z","level":"error","event":"25/05/12 02:45:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 39129, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.957863Z","level":"error","event":"25/05/12 02:45:30 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:39129 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 39129, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.971244Z","level":"error","event":"25/05/12 02:45:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 39129, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:30.972455Z","level":"error","event":"25/05/12 02:45:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 39129, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:32.751351Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:45:33.863342Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:45:34.863713Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:45:35.023454Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:45:36.023507Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:45:36.175670Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:45:37.176564Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:45:37.317153Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:45:38.323231Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:45:38.456160Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:45:39.456324Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:45:39.492347Z","level":"error","event":"25/05/12 02:45:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:39.514649Z","level":"error","event":"25/05/12 02:45:39 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:43.245357","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:45:43.624626Z","level":"error","event":"25/05/12 02:45:43 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:43.642571Z","level":"error","event":"25/05/12 02:45:43 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:43.668406Z","level":"error","event":"25/05/12 02:45:43 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:43.721544Z","level":"error","event":"25/05/12 02:45:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:43.824161Z","level":"error","event":"25/05/12 02:45:43 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:43.838405Z","level":"error","event":"25/05/12 02:45:43 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:43.887395Z","level":"error","event":"25/05/12 02:45:43 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:43.911603Z","level":"error","event":"25/05/12 02:45:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:44.010134Z","level":"error","event":"25/05/12 02:45:44 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:44.042848Z","level":"error","event":"25/05/12 02:45:44 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:44.043712Z","level":"error","event":"25/05/12 02:45:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-c65e36eb-ec7b-4541-8327-96e33e6f8e0d","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:44.044493Z","level":"error","event":"25/05/12 02:45:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-a859f7aa-db1e-44f3-904c-01d06b59acbd","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:45:44.062683Z","level":"error","event":"25/05/12 02:45:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-a859f7aa-db1e-44f3-904c-01d06b59acbd/pyspark-d76bf34a-8e14-4ffe-9ec8-f6e164946503","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:14.737650","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:46:18.517895Z","level":"error","event":"25/05/12 02:46:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:19.609910Z","level":"error","event":"25/05/12 02:46:19 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:19.628171Z","level":"error","event":"25/05/12 02:46:19 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:19.628520Z","level":"error","event":"25/05/12 02:46:19 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:19.628840Z","level":"error","event":"25/05/12 02:46:19 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:21.116614Z","level":"error","event":"25/05/12 02:46:21 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:21.181201Z","level":"error","event":"25/05/12 02:46:21 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:21.182609Z","level":"error","event":"25/05/12 02:46:21 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:21.214687Z","level":"error","event":"25/05/12 02:46:21 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:21.215550Z","level":"error","event":"25/05/12 02:46:21 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:21.216387Z","level":"error","event":"25/05/12 02:46:21 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:21.232352Z","level":"error","event":"25/05/12 02:46:21 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:21.271204Z","level":"error","event":"25/05/12 02:46:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:21.289685Z","level":"error","event":"25/05/12 02:46:21 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:21.301767Z","level":"error","event":"25/05/12 02:46:21 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:21.373993Z","level":"error","event":"25/05/12 02:46:21 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:21.384135Z","level":"error","event":"25/05/12 02:46:21 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:21.384907Z","level":"error","event":"25/05/12 02:46:21 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:21.386619Z","level":"error","event":"25/05/12 02:46:21 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:21.387664Z","level":"error","event":"25/05/12 02:46:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:22.163837Z","level":"error","event":"25/05/12 02:46:22 INFO Utils: Successfully started service 'sparkDriver' on port 41313.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:22.241568Z","level":"error","event":"25/05/12 02:46:22 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:22.333624Z","level":"error","event":"25/05/12 02:46:22 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:22.365710Z","level":"error","event":"25/05/12 02:46:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:22.378283Z","level":"error","event":"25/05/12 02:46:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:22.389518Z","level":"error","event":"25/05/12 02:46:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:22.420448Z","level":"error","event":"25/05/12 02:46:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cb9a50d3-c4bd-438c-af93-8390c7fbf6f4","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:22.447544Z","level":"error","event":"25/05/12 02:46:22 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:22.481012Z","level":"error","event":"25/05/12 02:46:22 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:22.872770Z","level":"error","event":"25/05/12 02:46:22 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.073911Z","level":"error","event":"25/05/12 02:46:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.188348Z","level":"error","event":"25/05/12 02:46:23 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.189791Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.191033Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.192143Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.193972Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.196529Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.197381Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.199047Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.202207Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.204171Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.205510Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.206473Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.207327Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.208114Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.208828Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.209474Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.212020Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.214432Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.218944Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.221020Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.222729Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.223597Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.224382Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.225141Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.248629Z","level":"error","event":"25/05/12 02:46:23 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.249971Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.252762Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.253693Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.254506Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.259965Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.260984Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.262622Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.264089Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.264882Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.265705Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.266548Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.267183Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.267861Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.268486Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.269035Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.269662Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.280073Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.280955Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.282254Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.284368Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.285213Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.286292Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.287193Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.288038Z","level":"error","event":"25/05/12 02:46:23 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.288829Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.301955Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.302744Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.303478Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.304347Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.305147Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.311298Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.312046Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.313091Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.313865Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.317233Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.318430Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.319674Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.320695Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.322020Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.323496Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.324298Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.325190Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.327406Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.328972Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.333658Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.335833Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.337263Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.338119Z","level":"error","event":"25/05/12 02:46:23 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.339260Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.341310Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.359910Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.360682Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.362145Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.363898Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.365773Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.366690Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.367733Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.368704Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.369521Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.370228Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.371010Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.371760Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.372576Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.387535Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.389590Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.390314Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.394279Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.395417Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.399151Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.401238Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.405339Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.461213Z","level":"error","event":"25/05/12 02:46:23 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.478799Z","level":"error","event":"25/05/12 02:46:23 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.479521Z","level":"error","event":"25/05/12 02:46:23 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.495827Z","level":"error","event":"25/05/12 02:46:23 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.496627Z","level":"error","event":"25/05/12 02:46:23 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@67055e93 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.556975Z","level":"error","event":"25/05/12 02:46:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34083.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.573462Z","level":"error","event":"25/05/12 02:46:23 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:34083","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.574350Z","level":"error","event":"25/05/12 02:46:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.604074Z","level":"error","event":"25/05/12 02:46:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 34083, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.605109Z","level":"error","event":"25/05/12 02:46:23 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:34083 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 34083, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.606499Z","level":"error","event":"25/05/12 02:46:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 34083, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:23.607404Z","level":"error","event":"25/05/12 02:46:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 34083, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:24.386419Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:46:24.978779Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:46:25.978981Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:46:26.076907Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:46:27.076989Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:46:27.162417Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:46:28.162520Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:46:28.258386Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:46:29.258640Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:46:29.345242Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:46:30.345368Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:46:30.354768Z","level":"error","event":"25/05/12 02:46:30 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:30.365210Z","level":"error","event":"25/05/12 02:46:30 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:32.113560","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:46:32.647164Z","level":"error","event":"25/05/12 02:46:32 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:32.647701Z","level":"error","event":"25/05/12 02:46:32 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:32.661242Z","level":"error","event":"25/05/12 02:46:32 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:32.682229Z","level":"error","event":"25/05/12 02:46:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:32.707787Z","level":"error","event":"25/05/12 02:46:32 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:32.716402Z","level":"error","event":"25/05/12 02:46:32 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:32.725800Z","level":"error","event":"25/05/12 02:46:32 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:32.726542Z","level":"error","event":"25/05/12 02:46:32 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:32.738564Z","level":"error","event":"25/05/12 02:46:32 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:32.749071Z","level":"error","event":"25/05/12 02:46:32 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:32.749785Z","level":"error","event":"25/05/12 02:46:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-297c4142-a80d-49de-b268-fa81e537c5a8/pyspark-34555f38-e9a9-4a12-b4f4-552721cc869c","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:32.759999Z","level":"error","event":"25/05/12 02:46:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-ae9fe4ff-e05c-435e-889f-0d45e22a2891","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:46:32.760673Z","level":"error","event":"25/05/12 02:46:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-297c4142-a80d-49de-b268-fa81e537c5a8","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:03.530645","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:47:08.621854Z","level":"error","event":"25/05/12 02:47:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:08.967690Z","level":"error","event":"25/05/12 02:47:08 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:08.993333Z","level":"error","event":"25/05/12 02:47:08 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:08.994140Z","level":"error","event":"25/05/12 02:47:08 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:08.994708Z","level":"error","event":"25/05/12 02:47:08 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:09.201192Z","level":"error","event":"25/05/12 02:47:09 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:09.224613Z","level":"error","event":"25/05/12 02:47:09 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:09.225372Z","level":"error","event":"25/05/12 02:47:09 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:09.271583Z","level":"error","event":"25/05/12 02:47:09 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:09.305592Z","level":"error","event":"25/05/12 02:47:09 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:09.306932Z","level":"error","event":"25/05/12 02:47:09 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:09.307735Z","level":"error","event":"25/05/12 02:47:09 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:09.337041Z","level":"error","event":"25/05/12 02:47:09 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:09.365450Z","level":"error","event":"25/05/12 02:47:09 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:09.366222Z","level":"error","event":"25/05/12 02:47:09 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:09.451888Z","level":"error","event":"25/05/12 02:47:09 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:09.478489Z","level":"error","event":"25/05/12 02:47:09 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:09.479381Z","level":"error","event":"25/05/12 02:47:09 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:09.480426Z","level":"error","event":"25/05/12 02:47:09 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:09.481367Z","level":"error","event":"25/05/12 02:47:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:09.895364Z","level":"error","event":"25/05/12 02:47:09 INFO Utils: Successfully started service 'sparkDriver' on port 41399.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:09.950978Z","level":"error","event":"25/05/12 02:47:09 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.021450Z","level":"error","event":"25/05/12 02:47:10 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.076506Z","level":"error","event":"25/05/12 02:47:10 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.099958Z","level":"error","event":"25/05/12 02:47:10 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.100703Z","level":"error","event":"25/05/12 02:47:10 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.127492Z","level":"error","event":"25/05/12 02:47:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4b779ef6-12e1-41a8-8860-652ac5ae3b79","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.158203Z","level":"error","event":"25/05/12 02:47:10 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.192558Z","level":"error","event":"25/05/12 02:47:10 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.551873Z","level":"error","event":"25/05/12 02:47:10 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.709811Z","level":"error","event":"25/05/12 02:47:10 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.800477Z","level":"error","event":"25/05/12 02:47:10 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.801150Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.801836Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.802595Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.803270Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.803980Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.804659Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.805580Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.806223Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.807182Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.808152Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.808880Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.809595Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.810267Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.810907Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.811552Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.812260Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.813021Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.813960Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.814810Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.815446Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.816067Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.816737Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.817497Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.844499Z","level":"error","event":"25/05/12 02:47:10 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.845390Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.846877Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.847513Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.848195Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.848851Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.849466Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.850084Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.850704Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.851382Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.852068Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.852991Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.853717Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.854540Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.855273Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.856060Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.856851Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.857647Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.858802Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.859944Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.861595Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.862528Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.863221Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.863953Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.864566Z","level":"error","event":"25/05/12 02:47:10 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.865251Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.866265Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.866968Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.867822Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.868592Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.869246Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.870296Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.870978Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.871624Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.872233Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.873273Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.874375Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.875897Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.876752Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.877540Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.878200Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.878765Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.879358Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.879924Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.880846Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.881804Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.882797Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.883661Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.884536Z","level":"error","event":"25/05/12 02:47:10 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.885370Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.886109Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.917277Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.918094Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.918852Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.919640Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.920409Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.921354Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.922394Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.923790Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.924592Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.925264Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.926089Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.926753Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.927361Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.928241Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.928957Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.930220Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.931416Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.932706Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.933581Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.934442Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.935222Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.964377Z","level":"error","event":"25/05/12 02:47:10 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.990318Z","level":"error","event":"25/05/12 02:47:10 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.991037Z","level":"error","event":"25/05/12 02:47:10 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.991624Z","level":"error","event":"25/05/12 02:47:10 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:10.992521Z","level":"error","event":"25/05/12 02:47:10 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@240f8e49 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:11.037226Z","level":"error","event":"25/05/12 02:47:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34797.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:11.058473Z","level":"error","event":"25/05/12 02:47:11 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:34797","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:11.059103Z","level":"error","event":"25/05/12 02:47:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:11.059928Z","level":"error","event":"25/05/12 02:47:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 34797, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:11.060763Z","level":"error","event":"25/05/12 02:47:11 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:34797 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 34797, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:11.084498Z","level":"error","event":"25/05/12 02:47:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 34797, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:11.085204Z","level":"error","event":"25/05/12 02:47:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 34797, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:11.809969Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:47:12.402356Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:47:13.402772Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:47:13.552214Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:47:14.552363Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:47:14.651193Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:47:15.651481Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:47:15.753051Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:47:16.753274Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:47:16.840290Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:47:17.840453Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:47:17.849052Z","level":"error","event":"25/05/12 02:47:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:17.856936Z","level":"error","event":"25/05/12 02:47:17 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:19.571136","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:47:20.074946Z","level":"error","event":"25/05/12 02:47:20 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:20.075565Z","level":"error","event":"25/05/12 02:47:20 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:20.091597Z","level":"error","event":"25/05/12 02:47:20 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:20.109857Z","level":"error","event":"25/05/12 02:47:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:20.128576Z","level":"error","event":"25/05/12 02:47:20 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:20.138933Z","level":"error","event":"25/05/12 02:47:20 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:20.139681Z","level":"error","event":"25/05/12 02:47:20 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:20.150119Z","level":"error","event":"25/05/12 02:47:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:20.158217Z","level":"error","event":"25/05/12 02:47:20 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:20.158876Z","level":"error","event":"25/05/12 02:47:20 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:20.159442Z","level":"error","event":"25/05/12 02:47:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-48300bf8-b542-4dbc-bc92-cfd5ec05d78f","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:20.168651Z","level":"error","event":"25/05/12 02:47:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-137b1408-27d3-4a7f-b448-b04f32f16b4b","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:20.177042Z","level":"error","event":"25/05/12 02:47:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-137b1408-27d3-4a7f-b448-b04f32f16b4b/pyspark-6e55e8d1-d576-4674-b2cb-93a0b099f472","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:51.481838","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:47:57.960103Z","level":"error","event":"25/05/12 02:47:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:58.199914Z","level":"error","event":"25/05/12 02:47:58 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:58.217451Z","level":"error","event":"25/05/12 02:47:58 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:58.217894Z","level":"error","event":"25/05/12 02:47:58 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:58.218323Z","level":"error","event":"25/05/12 02:47:58 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:58.365337Z","level":"error","event":"25/05/12 02:47:58 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:58.384806Z","level":"error","event":"25/05/12 02:47:58 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:58.385360Z","level":"error","event":"25/05/12 02:47:58 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:58.414871Z","level":"error","event":"25/05/12 02:47:58 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:58.432755Z","level":"error","event":"25/05/12 02:47:58 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:58.433459Z","level":"error","event":"25/05/12 02:47:58 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:58.434012Z","level":"error","event":"25/05/12 02:47:58 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:58.453808Z","level":"error","event":"25/05/12 02:47:58 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:58.471311Z","level":"error","event":"25/05/12 02:47:58 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:58.471906Z","level":"error","event":"25/05/12 02:47:58 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:58.524344Z","level":"error","event":"25/05/12 02:47:58 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:58.542284Z","level":"error","event":"25/05/12 02:47:58 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:58.542847Z","level":"error","event":"25/05/12 02:47:58 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:58.543479Z","level":"error","event":"25/05/12 02:47:58 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:58.544009Z","level":"error","event":"25/05/12 02:47:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:58.793598Z","level":"error","event":"25/05/12 02:47:58 INFO Utils: Successfully started service 'sparkDriver' on port 35781.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:58.837735Z","level":"error","event":"25/05/12 02:47:58 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:58.884595Z","level":"error","event":"25/05/12 02:47:58 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:58.909237Z","level":"error","event":"25/05/12 02:47:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:58.928363Z","level":"error","event":"25/05/12 02:47:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:58.929020Z","level":"error","event":"25/05/12 02:47:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:58.949649Z","level":"error","event":"25/05/12 02:47:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cc1cb2c3-00a4-4cc2-8ab1-881ec15c2de8","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:58.971267Z","level":"error","event":"25/05/12 02:47:58 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.008423Z","level":"error","event":"25/05/12 02:47:59 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.160534Z","level":"error","event":"25/05/12 02:47:59 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.263520Z","level":"error","event":"25/05/12 02:47:59 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.330597Z","level":"error","event":"25/05/12 02:47:59 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.331469Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.331974Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.332479Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.333127Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.333613Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.334088Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.334545Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.335006Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.335662Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.336418Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.337180Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.337934Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.338575Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.339197Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.339836Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.340408Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.340972Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.341567Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.342159Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.342855Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.343496Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.344065Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.344838Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.376029Z","level":"error","event":"25/05/12 02:47:59 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.376844Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.377581Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.378251Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.379011Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.379898Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.380620Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.381204Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.381835Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.382444Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.383180Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.383886Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.384433Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.385293Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.386063Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.386732Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.387382Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.388049Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.388778Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.389454Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.390154Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.390824Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.391461Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.392229Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.392889Z","level":"error","event":"25/05/12 02:47:59 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.393598Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.394398Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.395064Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.395705Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.396238Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.396722Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.397346Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.397987Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.398711Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.399426Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.399954Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.400639Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.401359Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.402019Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.402576Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.403072Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.403512Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.403975Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.404470Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.405049Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.405536Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.406037Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.406791Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.407534Z","level":"error","event":"25/05/12 02:47:59 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.408305Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.408888Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.430716Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.431306Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.432085Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.432656Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.433619Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.434446Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.435583Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.436321Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.437567Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.438404Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.438850Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.439246Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.439588Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.440010Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.440589Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.441192Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.441758Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.442282Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.442788Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.443252Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.443782Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.463943Z","level":"error","event":"25/05/12 02:47:59 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.464429Z","level":"error","event":"25/05/12 02:47:59 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.464873Z","level":"error","event":"25/05/12 02:47:59 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.488781Z","level":"error","event":"25/05/12 02:47:59 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.489479Z","level":"error","event":"25/05/12 02:47:59 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2c9036a8 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.521194Z","level":"error","event":"25/05/12 02:47:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37783.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.538366Z","level":"error","event":"25/05/12 02:47:59 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:37783","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.538819Z","level":"error","event":"25/05/12 02:47:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.539368Z","level":"error","event":"25/05/12 02:47:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 37783, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.539962Z","level":"error","event":"25/05/12 02:47:59 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:37783 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 37783, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.558132Z","level":"error","event":"25/05/12 02:47:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 37783, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:47:59.558847Z","level":"error","event":"25/05/12 02:47:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 37783, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:00.065250Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:48:00.912804Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:48:01.912987Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:48:02.030990Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:48:03.025424Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:48:03.112618Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:48:04.114880Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:48:04.244411Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:48:05.244578Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:48:05.330111Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:48:06.330257Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:48:06.347793Z","level":"error","event":"25/05/12 02:48:06 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:06.348712Z","level":"error","event":"25/05/12 02:48:06 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:08.460821","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:48:08.998139Z","level":"error","event":"25/05/12 02:48:08 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:08.998920Z","level":"error","event":"25/05/12 02:48:08 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:09.020280Z","level":"error","event":"25/05/12 02:48:09 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:09.039684Z","level":"error","event":"25/05/12 02:48:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:09.057614Z","level":"error","event":"25/05/12 02:48:09 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:09.058251Z","level":"error","event":"25/05/12 02:48:09 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:09.058789Z","level":"error","event":"25/05/12 02:48:09 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:09.059227Z","level":"error","event":"25/05/12 02:48:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:09.078958Z","level":"error","event":"25/05/12 02:48:09 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:09.079557Z","level":"error","event":"25/05/12 02:48:09 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:09.080200Z","level":"error","event":"25/05/12 02:48:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-0ab618a7-7123-4465-ae83-bad888bf7e5d/pyspark-ddea0557-3c87-48ef-9fb4-959cdea5b0c4","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:09.080828Z","level":"error","event":"25/05/12 02:48:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-0ab618a7-7123-4465-ae83-bad888bf7e5d","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:09.099428Z","level":"error","event":"25/05/12 02:48:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-8b5ac7e2-8305-4540-b411-fd04f0f5c71d","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:40.296281","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:48:44.176314Z","level":"error","event":"25/05/12 02:48:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:44.411219Z","level":"error","event":"25/05/12 02:48:44 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:44.426268Z","level":"error","event":"25/05/12 02:48:44 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:44.426670Z","level":"error","event":"25/05/12 02:48:44 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:44.426930Z","level":"error","event":"25/05/12 02:48:44 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:44.566980Z","level":"error","event":"25/05/12 02:48:44 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:44.591844Z","level":"error","event":"25/05/12 02:48:44 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:44.592483Z","level":"error","event":"25/05/12 02:48:44 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:44.661414Z","level":"error","event":"25/05/12 02:48:44 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:44.686551Z","level":"error","event":"25/05/12 02:48:44 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:44.687154Z","level":"error","event":"25/05/12 02:48:44 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:44.687678Z","level":"error","event":"25/05/12 02:48:44 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:44.726682Z","level":"error","event":"25/05/12 02:48:44 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:44.748356Z","level":"error","event":"25/05/12 02:48:44 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:44.749078Z","level":"error","event":"25/05/12 02:48:44 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:44.807463Z","level":"error","event":"25/05/12 02:48:44 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:44.826153Z","level":"error","event":"25/05/12 02:48:44 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:44.826706Z","level":"error","event":"25/05/12 02:48:44 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:44.827173Z","level":"error","event":"25/05/12 02:48:44 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:44.827766Z","level":"error","event":"25/05/12 02:48:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.152725Z","level":"error","event":"25/05/12 02:48:45 INFO Utils: Successfully started service 'sparkDriver' on port 41237.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.196355Z","level":"error","event":"25/05/12 02:48:45 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.252256Z","level":"error","event":"25/05/12 02:48:45 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.280814Z","level":"error","event":"25/05/12 02:48:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.304638Z","level":"error","event":"25/05/12 02:48:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.305163Z","level":"error","event":"25/05/12 02:48:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.332314Z","level":"error","event":"25/05/12 02:48:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d1807a72-e818-4514-b6fe-610a6c0bd07f","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.354134Z","level":"error","event":"25/05/12 02:48:45 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.376387Z","level":"error","event":"25/05/12 02:48:45 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.579245Z","level":"error","event":"25/05/12 02:48:45 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.686160Z","level":"error","event":"25/05/12 02:48:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.748286Z","level":"error","event":"25/05/12 02:48:45 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.748861Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.749221Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.749565Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.749929Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.750295Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.750711Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.751205Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.751784Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.752358Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.752855Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.753398Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.753980Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.754546Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.755172Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.755764Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.756208Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.756647Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.757247Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.757932Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.758579Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.759245Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.759859Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.760470Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.779958Z","level":"error","event":"25/05/12 02:48:45 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.780610Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.781196Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.781764Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.782352Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.783069Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.783769Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.784379Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.785299Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.786723Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.787702Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.788504Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.789310Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.790033Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.790869Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.791554Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.792235Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.792814Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.793386Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.793973Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.794648Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.795272Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.795769Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.796264Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.796810Z","level":"error","event":"25/05/12 02:48:45 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.797390Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.797994Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.798500Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.798971Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.799480Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.800108Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.800766Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.801688Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.802263Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.802834Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.803301Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.803650Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.803956Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.804333Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.804723Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.805116Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.805490Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.806007Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.806541Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.807114Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.807711Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.808348Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.808853Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.809291Z","level":"error","event":"25/05/12 02:48:45 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.809781Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.810217Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.826669Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.827314Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.828506Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.829380Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.830041Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.830750Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.831395Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.832014Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.832701Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.833276Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.833853Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.834518Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.835116Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.835726Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.836349Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.837238Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.837870Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.838409Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.839109Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.839693Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.840138Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.871622Z","level":"error","event":"25/05/12 02:48:45 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.889597Z","level":"error","event":"25/05/12 02:48:45 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.890091Z","level":"error","event":"25/05/12 02:48:45 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.890460Z","level":"error","event":"25/05/12 02:48:45 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.890791Z","level":"error","event":"25/05/12 02:48:45 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@73663445 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.919318Z","level":"error","event":"25/05/12 02:48:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38239.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.934616Z","level":"error","event":"25/05/12 02:48:45 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:38239","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.935084Z","level":"error","event":"25/05/12 02:48:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.935542Z","level":"error","event":"25/05/12 02:48:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 38239, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.936032Z","level":"error","event":"25/05/12 02:48:45 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:38239 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 38239, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.952250Z","level":"error","event":"25/05/12 02:48:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 38239, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:45.952688Z","level":"error","event":"25/05/12 02:48:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 38239, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:46.488221Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:48:47.227925Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:48:48.228027Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:48:48.337507Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:48:49.338054Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:48:49.406615Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:48:50.406767Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:48:50.513879Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:48:51.514108Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:48:51.648352Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:48:52.648426Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:48:52.658067Z","level":"error","event":"25/05/12 02:48:52 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:52.668774Z","level":"error","event":"25/05/12 02:48:52 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:54.145848","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:48:54.683425Z","level":"error","event":"25/05/12 02:48:54 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:54.684007Z","level":"error","event":"25/05/12 02:48:54 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:54.699229Z","level":"error","event":"25/05/12 02:48:54 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:54.722397Z","level":"error","event":"25/05/12 02:48:54 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:54.757179Z","level":"error","event":"25/05/12 02:48:54 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:54.773523Z","level":"error","event":"25/05/12 02:48:54 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:54.787707Z","level":"error","event":"25/05/12 02:48:54 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:54.788334Z","level":"error","event":"25/05/12 02:48:54 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:54.806622Z","level":"error","event":"25/05/12 02:48:54 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:54.816627Z","level":"error","event":"25/05/12 02:48:54 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:54.817272Z","level":"error","event":"25/05/12 02:48:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-5af6de08-b412-43ab-85ab-15dd0ea0cb84","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:54.827055Z","level":"error","event":"25/05/12 02:48:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-d342f837-31c8-40db-bd32-a0eda9bf9e6b","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:48:54.827787Z","level":"error","event":"25/05/12 02:48:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-d342f837-31c8-40db-bd32-a0eda9bf9e6b/pyspark-bae73c10-f469-4d1f-9706-077c1c2a132f","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:25.977056","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:49:29.301598Z","level":"error","event":"25/05/12 02:49:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:29.501694Z","level":"error","event":"25/05/12 02:49:29 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:29.517917Z","level":"error","event":"25/05/12 02:49:29 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:29.518280Z","level":"error","event":"25/05/12 02:49:29 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:29.518585Z","level":"error","event":"25/05/12 02:49:29 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:29.663047Z","level":"error","event":"25/05/12 02:49:29 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:29.682507Z","level":"error","event":"25/05/12 02:49:29 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:29.683218Z","level":"error","event":"25/05/12 02:49:29 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:29.717685Z","level":"error","event":"25/05/12 02:49:29 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:29.741351Z","level":"error","event":"25/05/12 02:49:29 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:29.742420Z","level":"error","event":"25/05/12 02:49:29 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:29.743365Z","level":"error","event":"25/05/12 02:49:29 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:29.763229Z","level":"error","event":"25/05/12 02:49:29 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:29.777322Z","level":"error","event":"25/05/12 02:49:29 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:29.777808Z","level":"error","event":"25/05/12 02:49:29 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:29.832791Z","level":"error","event":"25/05/12 02:49:29 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:29.845849Z","level":"error","event":"25/05/12 02:49:29 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:29.846266Z","level":"error","event":"25/05/12 02:49:29 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:29.846571Z","level":"error","event":"25/05/12 02:49:29 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:29.846880Z","level":"error","event":"25/05/12 02:49:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.041046Z","level":"error","event":"25/05/12 02:49:30 INFO Utils: Successfully started service 'sparkDriver' on port 33143.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.078384Z","level":"error","event":"25/05/12 02:49:30 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.122619Z","level":"error","event":"25/05/12 02:49:30 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.145795Z","level":"error","event":"25/05/12 02:49:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.168245Z","level":"error","event":"25/05/12 02:49:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.169024Z","level":"error","event":"25/05/12 02:49:30 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.186127Z","level":"error","event":"25/05/12 02:49:30 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-42ba454c-d230-4750-9213-d5bfc8e2518a","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.205733Z","level":"error","event":"25/05/12 02:49:30 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.228062Z","level":"error","event":"25/05/12 02:49:30 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.398483Z","level":"error","event":"25/05/12 02:49:30 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.492589Z","level":"error","event":"25/05/12 02:49:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.546810Z","level":"error","event":"25/05/12 02:49:30 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.547405Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.547851Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.548340Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.548814Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.549322Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.549811Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.550222Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.550667Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.551120Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.551591Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.552122Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.552669Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.553125Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.553617Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.554041Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.554469Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.554936Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.555376Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.555800Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.556141Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.556536Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.556939Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.557286Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.577134Z","level":"error","event":"25/05/12 02:49:30 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.577911Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.578651Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.579413Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.580165Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.580817Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.581535Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.582324Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.583056Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.583725Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.584431Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.585207Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.586003Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.586714Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.587386Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.588005Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.588590Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.589251Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.590096Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.590877Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.591581Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.592456Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.593277Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.594012Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.595155Z","level":"error","event":"25/05/12 02:49:30 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.595716Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.596204Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.596710Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.597231Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.597701Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.598187Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.598666Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.599168Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.599682Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.600202Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.600644Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.601058Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.601589Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.602130Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.602670Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.603332Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.604331Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.604867Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.605441Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.605948Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.606638Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.607189Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.607754Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.608358Z","level":"error","event":"25/05/12 02:49:30 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.609001Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.609517Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.629838Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.630622Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.631408Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.632119Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.632983Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.633776Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.634615Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.635463Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.636126Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.636790Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.637381Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.637942Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.638518Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.639207Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.640172Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.640988Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.641986Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.642752Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.643419Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.644109Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.644876Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.673169Z","level":"error","event":"25/05/12 02:49:30 INFO Executor: Starting executor ID driver on host 7b3d6efb52f0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.696902Z","level":"error","event":"25/05/12 02:49:30 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.697679Z","level":"error","event":"25/05/12 02:49:30 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.698392Z","level":"error","event":"25/05/12 02:49:30 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.699037Z","level":"error","event":"25/05/12 02:49:30 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6ff21454 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.736824Z","level":"error","event":"25/05/12 02:49:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34699.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.768880Z","level":"error","event":"25/05/12 02:49:30 INFO NettyBlockTransferService: Server created on 7b3d6efb52f0:34699","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.769886Z","level":"error","event":"25/05/12 02:49:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.771428Z","level":"error","event":"25/05/12 02:49:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7b3d6efb52f0, 34699, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.793234Z","level":"error","event":"25/05/12 02:49:30 INFO BlockManagerMasterEndpoint: Registering block manager 7b3d6efb52f0:34699 with 434.4 MiB RAM, BlockManagerId(driver, 7b3d6efb52f0, 34699, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.793683Z","level":"error","event":"25/05/12 02:49:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7b3d6efb52f0, 34699, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:30.794068Z","level":"error","event":"25/05/12 02:49:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7b3d6efb52f0, 34699, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:31.203279Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:49:32.378160Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:49:34.675955Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:49:34.775337Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:49:35.775875Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:49:35.869646Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:49:36.869769Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:49:36.996338Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:49:37.996506Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:49:38.099003Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:49:39.099305Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:49:39.127903Z","level":"error","event":"25/05/12 02:49:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:39.128393Z","level":"error","event":"25/05/12 02:49:39 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:41.678164","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/YFinance.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/YFinance.py","lineno":64,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:49:42.180155Z","level":"error","event":"25/05/12 02:49:42 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:42.180794Z","level":"error","event":"25/05/12 02:49:42 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:42.204765Z","level":"error","event":"25/05/12 02:49:42 INFO SparkUI: Stopped Spark web UI at http://7b3d6efb52f0:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:42.226977Z","level":"error","event":"25/05/12 02:49:42 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:42.252267Z","level":"error","event":"25/05/12 02:49:42 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:42.272342Z","level":"error","event":"25/05/12 02:49:42 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:42.273112Z","level":"error","event":"25/05/12 02:49:42 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:42.273703Z","level":"error","event":"25/05/12 02:49:42 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:42.292829Z","level":"error","event":"25/05/12 02:49:42 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:42.293473Z","level":"error","event":"25/05/12 02:49:42 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:42.293981Z","level":"error","event":"25/05/12 02:49:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-37678086-5387-4661-9716-1a6f51db00a6","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:42.311306Z","level":"error","event":"25/05/12 02:49:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-23ddefa6-f436-4b79-aca8-65af0b8413da","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:49:42.311830Z","level":"error","event":"25/05/12 02:49:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-23ddefa6-f436-4b79-aca8-65af0b8413da/pyspark-d4dce93c-3e0b-497b-9662-10c713307260","chan":"stderr","logger":"processor"}
