{"timestamp":"2025-05-12T02:45:03.296419","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-05-12T02:45:03.297770","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/yfinance_DAG.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:45:13.574431Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T02:45:13.575088Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T02:45:13.575893Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T02:45:13.576273Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T02:45:13.576686Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T02:45:13.577043Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T02:45:13.577332Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T02:45:13.577679Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T02:45:13.578032Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T02:45:13.578403Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T02:45:13.578724Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T02:45:19.121799Z","level":"error","event":"25/05/12 02:45:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T02:45:19.637350Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T02:45:19.638415Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T02:45:45.948773","level":"info","event":"Done. Returned value was: None","logger":"airflow.task.operators.airflow.providers.standard.operators.python.PythonOperator"}
{"timestamp":"2025-05-12T02:45:47.407879Z","level":"error","event":"\r[Stage 0:>                                                          (0 + 0) / 1]\r\r[Stage 0:>                                                          (0 + 1) / 1]\r\r                                                                                \r\r[Stage 1:>                                                          (0 + 1) / 1]\r\r[Stage 3:>                                                          (0 + 1) / 1]","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T03:05:08.318806","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-05-12T03:05:08.319863","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/yfinance_daily.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T03:05:12.057255Z","level":"error","event":"python: can't open file '/opt/airflow/jobs/YFinance/daily.py': [Errno 2] No such file or directory","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T03:05:08.459290","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"CalledProcessError","exc_value":"Command '['python', 'jobs/YFinance/daily.py']' returned non-zero exit status 2.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":825,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1088,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":408,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":212,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":235,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/dags/yfinance_daily.py","lineno":7,"name":"collect_data"},{"filename":"/usr/local/lib/python3.12/subprocess.py","lineno":571,"name":"run"}]}]}
{"timestamp":"2025-05-12T03:26:13.561564","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-05-12T03:26:13.562327","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/yfinance_daily.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T03:26:17.606427Z","level":"error","event":"python: can't open file '/opt/airflow/dags/jobs/YFinance/_scrap.py': [Errno 2] No such file or directory","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T03:26:17.607073Z","level":"error","event":"Traceback (most recent call last):","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T03:26:17.607729Z","level":"error","event":"  File \"/opt/airflow/jobs/YFinance/daily.py\", line 12, in <module>","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T03:26:17.608258Z","level":"error","event":"    subprocess.run([\"python\", \"/opt/airflow/dags/jobs/YFinance/_scrap.py\", \"--period\", \"1d\"], check=True)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T03:26:17.608869Z","level":"error","event":"  File \"/usr/local/lib/python3.12/subprocess.py\", line 571, in run","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T03:26:17.609399Z","level":"error","event":"    raise CalledProcessError(retcode, process.args,","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T03:26:17.610238Z","level":"error","event":"subprocess.CalledProcessError: Command '['python', '/opt/airflow/dags/jobs/YFinance/_scrap.py', '--period', '1d']' returned non-zero exit status 2.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T03:26:15.713067","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"CalledProcessError","exc_value":"Command '['python', 'jobs/YFinance/daily.py']' returned non-zero exit status 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":825,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1088,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":408,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":212,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":235,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/dags/yfinance_daily.py","lineno":7,"name":"collect_data"},{"filename":"/usr/local/lib/python3.12/subprocess.py","lineno":571,"name":"run"}]}]}
{"timestamp":"2025-05-12T07:53:24.420057","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-05-12T07:53:24.420911","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/yfinance_daily.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T07:53:25.734886","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"CalledProcessError","exc_value":"Command '['python', 'jobs/YFinance/daily.py']' returned non-zero exit status 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":825,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1088,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":408,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":212,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":235,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/dags/yfinance_daily.py","lineno":7,"name":"collect_data"},{"filename":"/usr/local/lib/python3.12/subprocess.py","lineno":571,"name":"run"}]}]}
{"timestamp":"2025-05-12T07:53:27.702555Z","level":"error","event":"python: can't open file '/opt/airflow/dags/jobs/YFinance/_scrap.py': [Errno 2] No such file or directory","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T07:53:27.703108Z","level":"error","event":"Traceback (most recent call last):","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T07:53:27.703612Z","level":"error","event":"  File \"/opt/airflow/jobs/YFinance/daily.py\", line 12, in <module>","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T07:53:27.704201Z","level":"error","event":"    subprocess.run([\"python\", \"/opt/airflow/dags/jobs/YFinance/_scrap.py\", \"--period\", \"1d\"], check=True)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T07:53:27.704770Z","level":"error","event":"  File \"/usr/local/lib/python3.12/subprocess.py\", line 571, in run","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T07:53:27.705358Z","level":"error","event":"    raise CalledProcessError(retcode, process.args,","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T07:53:27.706015Z","level":"error","event":"subprocess.CalledProcessError: Command '['python', '/opt/airflow/dags/jobs/YFinance/_scrap.py', '--period', '1d']' returned non-zero exit status 2.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T08:59:48.991012","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-05-12T08:59:48.994893","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/yfinance_daily.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T08:59:55.143184Z","level":"error","event":"python: can't open file '/opt/airflow/dags/jobs/YFinance/_scrap.py': [Errno 2] No such file or directory","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T08:59:55.145021Z","level":"error","event":"Traceback (most recent call last):","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T08:59:55.147116Z","level":"error","event":"  File \"/opt/airflow/jobs/YFinance/daily.py\", line 12, in <module>","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T08:59:55.149920Z","level":"error","event":"    subprocess.run([\"python\", \"/opt/airflow/dags/jobs/YFinance/_scrap.py\", \"--period\", \"1d\"], check=True)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T08:59:55.151703Z","level":"error","event":"  File \"/usr/local/lib/python3.12/subprocess.py\", line 571, in run","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T08:59:55.153632Z","level":"error","event":"    raise CalledProcessError(retcode, process.args,","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T08:59:55.155550Z","level":"error","event":"subprocess.CalledProcessError: Command '['python', '/opt/airflow/dags/jobs/YFinance/_scrap.py', '--period', '1d']' returned non-zero exit status 2.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T08:59:51.544445","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"CalledProcessError","exc_value":"Command '['python', 'jobs/YFinance/daily.py']' returned non-zero exit status 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":825,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1088,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":408,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":212,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":235,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/dags/yfinance_daily.py","lineno":7,"name":"collect_data"},{"filename":"/usr/local/lib/python3.12/subprocess.py","lineno":571,"name":"run"}]}]}
{"timestamp":"2025-05-12T09:35:57.914192","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-05-12T09:35:57.916232","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/yfinance_daily.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T09:36:27.179677Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T09:36:27.183520Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T09:36:27.188679Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T09:36:27.190839Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T09:36:27.192638Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T09:36:27.194515Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T09:36:27.196623Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T09:36:27.198933Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T09:36:27.202081Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T09:36:27.204354Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T09:36:27.206499Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T09:36:32.784685Z","level":"error","event":"25/05/12 09:36:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:33.177523Z","level":"error","event":"25/05/12 09:36:33 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:33.178485Z","level":"error","event":"25/05/12 09:36:33 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:33.179518Z","level":"error","event":"25/05/12 09:36:33 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:33.180336Z","level":"error","event":"25/05/12 09:36:33 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:33.402515Z","level":"error","event":"25/05/12 09:36:33 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:33.403535Z","level":"error","event":"25/05/12 09:36:33 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:33.404412Z","level":"error","event":"25/05/12 09:36:33 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:33.468792Z","level":"error","event":"25/05/12 09:36:33 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:33.469890Z","level":"error","event":"25/05/12 09:36:33 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:33.471100Z","level":"error","event":"25/05/12 09:36:33 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:33.472361Z","level":"error","event":"25/05/12 09:36:33 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:33.533179Z","level":"error","event":"25/05/12 09:36:33 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:33.553231Z","level":"error","event":"25/05/12 09:36:33 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:33.556203Z","level":"error","event":"25/05/12 09:36:33 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:33.662901Z","level":"error","event":"25/05/12 09:36:33 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:33.663860Z","level":"error","event":"25/05/12 09:36:33 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:33.665236Z","level":"error","event":"25/05/12 09:36:33 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:33.666831Z","level":"error","event":"25/05/12 09:36:33 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:33.668383Z","level":"error","event":"25/05/12 09:36:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:34.227162Z","level":"error","event":"25/05/12 09:36:34 INFO Utils: Successfully started service 'sparkDriver' on port 46011.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:34.294330Z","level":"error","event":"25/05/12 09:36:34 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:34.376862Z","level":"error","event":"25/05/12 09:36:34 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:34.414555Z","level":"error","event":"25/05/12 09:36:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:34.416630Z","level":"error","event":"25/05/12 09:36:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:34.429205Z","level":"error","event":"25/05/12 09:36:34 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:34.520500Z","level":"error","event":"25/05/12 09:36:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1bd86bff-1394-4b5f-8930-73f1a6bce1e9","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:34.577929Z","level":"error","event":"25/05/12 09:36:34 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:34.622581Z","level":"error","event":"25/05/12 09:36:34 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.074967Z","level":"error","event":"25/05/12 09:36:35 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.241700Z","level":"error","event":"25/05/12 09:36:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.336864Z","level":"error","event":"25/05/12 09:36:35 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.337661Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.338222Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.338871Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.339512Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.340133Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.340845Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.341655Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.342530Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.343177Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.343695Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.344601Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.345729Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.346691Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.347576Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.348632Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.349526Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.350388Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.351193Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.352674Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.354308Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.355735Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.356683Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.357423Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.358513Z","level":"error","event":"25/05/12 09:36:35 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.359337Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.360368Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.361309Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.362164Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.362950Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.363690Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.364317Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.364886Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.365442Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.366170Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.367197Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.367922Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.368833Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.369634Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.370483Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.371390Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.372399Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.374015Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.375111Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.376218Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.377155Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.377951Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.378709Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.379429Z","level":"error","event":"25/05/12 09:36:35 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.380568Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.383128Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.387454Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.388472Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.389306Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.390207Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.391102Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.392029Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.392752Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.393674Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.394476Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.395202Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.395992Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.396848Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.397745Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.398769Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.399602Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.400731Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.401629Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.402520Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.403377Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.404384Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.405786Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.407060Z","level":"error","event":"25/05/12 09:36:35 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.407951Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.408745Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.409690Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.410407Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.411460Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.412431Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.413331Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.414221Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.415095Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.415937Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.416947Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.417894Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.419163Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.420143Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.421197Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.422301Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.423582Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.424471Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.425373Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.426357Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.427530Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.428444Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.429591Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.528178Z","level":"error","event":"25/05/12 09:36:35 INFO Executor: Starting executor ID driver on host e32d8dd2660d","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.529492Z","level":"error","event":"25/05/12 09:36:35 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.530795Z","level":"error","event":"25/05/12 09:36:35 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.550145Z","level":"error","event":"25/05/12 09:36:35 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.552873Z","level":"error","event":"25/05/12 09:36:35 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@7e56a3df for default.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.607076Z","level":"error","event":"25/05/12 09:36:35 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40929.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.608171Z","level":"error","event":"25/05/12 09:36:35 INFO NettyBlockTransferService: Server created on e32d8dd2660d:40929","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.613495Z","level":"error","event":"25/05/12 09:36:35 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.628561Z","level":"error","event":"25/05/12 09:36:35 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, e32d8dd2660d, 40929, None)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.635860Z","level":"error","event":"25/05/12 09:36:35 INFO BlockManagerMasterEndpoint: Registering block manager e32d8dd2660d:40929 with 434.4 MiB RAM, BlockManagerId(driver, e32d8dd2660d, 40929, None)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.644636Z","level":"error","event":"25/05/12 09:36:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, e32d8dd2660d, 40929, None)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:35.647703Z","level":"error","event":"25/05/12 09:36:35 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, e32d8dd2660d, 40929, None)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:36.925738Z","level":"error","event":"25/05/12 09:36:36 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.011391Z","level":"error","event":"Traceback (most recent call last):","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.012280Z","level":"error","event":"  File \"/opt/airflow/jobs/YFinance/daily.py\", line 23, in <module>","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.018241Z","level":"error","event":"    df = _spark.read.format(\"mongo\") \\","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.019042Z","level":"error","event":"         ^^^^^^^^^^^","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.019740Z","level":"error","event":"  File \"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py\", line 1706, in read","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.020663Z","level":"error","event":"    return DataFrameReader(self)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.021525Z","level":"error","event":"           ^^^^^^^^^^^^^^^^^^^^^","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.022355Z","level":"error","event":"  File \"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py\", line 70, in __init__","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.023059Z","level":"error","event":"    self._jreader = spark._jsparkSession.read()","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.023723Z","level":"error","event":"                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.024471Z","level":"error","event":"  File \"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1322, in __call__","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.025398Z","level":"error","event":"    return_value = get_return_value(","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.026206Z","level":"error","event":"                   ^^^^^^^^^^^^^^^^^","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.027004Z","level":"error","event":"  File \"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", line 185, in deco","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.027699Z","level":"error","event":"    raise converted from None","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.056892Z","level":"error","event":"25/05/12 09:36:37 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.105691Z","level":"error","event":"pyspark.errors.exceptions.captured.IllegalArgumentException: The value of property spark.mongodb.read.connection.uri must not be null","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.106523Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.107268Z","level":"error","event":"JVM stacktrace:","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.107934Z","level":"error","event":"java.lang.IllegalArgumentException: The value of property spark.mongodb.read.connection.uri must not be null","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.108531Z","level":"error","event":"\tat org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:219)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.109098Z","level":"error","event":"\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1403)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.109607Z","level":"error","event":"\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1384)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.110167Z","level":"error","event":"\tat org.apache.spark.sql.internal.SharedState.$anonfun$x$1$2(SharedState.scala:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.110772Z","level":"error","event":"\tat scala.collection.immutable.Map$Map3.foreach(Map.scala:376)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.111518Z","level":"error","event":"\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:69)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.112198Z","level":"error","event":"\tat org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.112942Z","level":"error","event":"\tat scala.Option.getOrElse(Option.scala:189)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.113613Z","level":"error","event":"\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.114171Z","level":"error","event":"\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.114702Z","level":"error","event":"\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.115317Z","level":"error","event":"\tat scala.Option.getOrElse(Option.scala:189)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.115782Z","level":"error","event":"\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.116264Z","level":"error","event":"\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.116724Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameReader.<init>(DataFrameReader.scala:699)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.117339Z","level":"error","event":"\tat org.apache.spark.sql.SparkSession.read(SparkSession.scala:783)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.117922Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.118530Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.119115Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.119869Z","level":"error","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.120533Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.121342Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.122113Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:282)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.122716Z","level":"error","event":"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.123295Z","level":"error","event":"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.123791Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.124500Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.125177Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.125854Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.231085","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"CalledProcessError","exc_value":"Command '['python', 'jobs/YFinance/daily.py']' returned non-zero exit status 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":825,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1088,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":408,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":212,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":235,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/dags/yfinance_daily.py","lineno":7,"name":"collect_data"},{"filename":"/usr/local/lib/python3.12/subprocess.py","lineno":571,"name":"run"}]}]}
{"timestamp":"2025-05-12T09:36:37.245467Z","level":"error","event":"25/05/12 09:36:37 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.248031Z","level":"error","event":"25/05/12 09:36:37 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.277418Z","level":"error","event":"25/05/12 09:36:37 INFO SparkUI: Stopped Spark web UI at http://e32d8dd2660d:4040","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.309958Z","level":"error","event":"25/05/12 09:36:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.339898Z","level":"error","event":"25/05/12 09:36:37 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.340724Z","level":"error","event":"25/05/12 09:36:37 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.361467Z","level":"error","event":"25/05/12 09:36:37 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.373273Z","level":"error","event":"25/05/12 09:36:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.411069Z","level":"error","event":"25/05/12 09:36:37 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.413301Z","level":"error","event":"25/05/12 09:36:37 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.419443Z","level":"error","event":"25/05/12 09:36:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-02ba76b5-1b16-48a4-a53c-7eba8164edce","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.431705Z","level":"error","event":"25/05/12 09:36:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-165131fd-f2ad-4189-8282-c8ec87d07644/pyspark-be658a16-e949-4de7-95c3-3441e876b145","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:36:37.442219Z","level":"error","event":"25/05/12 09:36:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-165131fd-f2ad-4189-8282-c8ec87d07644","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:06:29.170065","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-05-12T11:06:29.170844","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/yfinance_daily.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T11:06:56.466920Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:06:56.468944Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:06:56.470485Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:06:56.472044Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:06:56.473749Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:06:56.475112Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:06:56.476412Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:06:56.477823Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:06:56.479157Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:06:56.480575Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:06:56.481832Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:07:00.443528Z","level":"error","event":"25/05/12 11:07:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:00.656193Z","level":"error","event":"25/05/12 11:07:00 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:00.656750Z","level":"error","event":"25/05/12 11:07:00 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:00.657401Z","level":"error","event":"25/05/12 11:07:00 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:00.658288Z","level":"error","event":"25/05/12 11:07:00 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:00.850001Z","level":"error","event":"25/05/12 11:07:00 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:00.850465Z","level":"error","event":"25/05/12 11:07:00 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:00.851399Z","level":"error","event":"25/05/12 11:07:00 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:00.889005Z","level":"error","event":"25/05/12 11:07:00 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:00.889813Z","level":"error","event":"25/05/12 11:07:00 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:00.890426Z","level":"error","event":"25/05/12 11:07:00 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:00.891081Z","level":"error","event":"25/05/12 11:07:00 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:00.918924Z","level":"error","event":"25/05/12 11:07:00 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:00.928385Z","level":"error","event":"25/05/12 11:07:00 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:00.929523Z","level":"error","event":"25/05/12 11:07:00 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:00.996031Z","level":"error","event":"25/05/12 11:07:00 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:00.996798Z","level":"error","event":"25/05/12 11:07:00 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:00.997439Z","level":"error","event":"25/05/12 11:07:00 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:00.998431Z","level":"error","event":"25/05/12 11:07:00 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:00.999097Z","level":"error","event":"25/05/12 11:07:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.295202Z","level":"error","event":"25/05/12 11:07:01 INFO Utils: Successfully started service 'sparkDriver' on port 46803.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.363025Z","level":"error","event":"25/05/12 11:07:01 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.426034Z","level":"error","event":"25/05/12 11:07:01 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.456032Z","level":"error","event":"25/05/12 11:07:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.457438Z","level":"error","event":"25/05/12 11:07:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.467099Z","level":"error","event":"25/05/12 11:07:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.508731Z","level":"error","event":"25/05/12 11:07:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1019839d-35bb-4e10-9381-55de8a023fc5","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.532847Z","level":"error","event":"25/05/12 11:07:01 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.566312Z","level":"error","event":"25/05/12 11:07:01 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.797233Z","level":"error","event":"25/05/12 11:07:01 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.909821Z","level":"error","event":"25/05/12 11:07:01 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.960876Z","level":"error","event":"25/05/12 11:07:01 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.961594Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.962060Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.962576Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.963160Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.963646Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.964182Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.964561Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.965018Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.965588Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.966218Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.967316Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.968280Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.969332Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.970114Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.970804Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.971584Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.973184Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.974397Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.975164Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.975943Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.977190Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.978043Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.978934Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.979853Z","level":"error","event":"25/05/12 11:07:01 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.981010Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.981949Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.982946Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.983953Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.984531Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.985223Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.986315Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.987148Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.987900Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.988442Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.988946Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.989562Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.990229Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.990836Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.991647Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.994063Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.996019Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.996779Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.997520Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:01.999865Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.000712Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.001486Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.002513Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.003587Z","level":"error","event":"25/05/12 11:07:01 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.004540Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.006026Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.006591Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.007463Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.008501Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.009500Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.010215Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.010954Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.011728Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.013863Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.014578Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.015290Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.016239Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.016977Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.018086Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.019569Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.020741Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.022340Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.025156Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.026017Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.026951Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.027726Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.028832Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.029574Z","level":"error","event":"25/05/12 11:07:01 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.030191Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.030962Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.031704Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.032422Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.033076Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.033540Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.033927Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.034379Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.034830Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.035489Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.036198Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.036829Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.037370Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.038132Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.038697Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.039128Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.039663Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.040373Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.040921Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.041573Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.042128Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.042578Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.043050Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.110199Z","level":"error","event":"25/05/12 11:07:02 INFO Executor: Starting executor ID driver on host bedd0e5c3c72","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.110810Z","level":"error","event":"25/05/12 11:07:02 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.111396Z","level":"error","event":"25/05/12 11:07:02 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.120060Z","level":"error","event":"25/05/12 11:07:02 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.121130Z","level":"error","event":"25/05/12 11:07:02 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1cc616da for default.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.147224Z","level":"error","event":"25/05/12 11:07:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45731.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.147939Z","level":"error","event":"25/05/12 11:07:02 INFO NettyBlockTransferService: Server created on bedd0e5c3c72:45731","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.150226Z","level":"error","event":"25/05/12 11:07:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.157290Z","level":"error","event":"25/05/12 11:07:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, bedd0e5c3c72, 45731, None)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.160753Z","level":"error","event":"25/05/12 11:07:02 INFO BlockManagerMasterEndpoint: Registering block manager bedd0e5c3c72:45731 with 434.4 MiB RAM, BlockManagerId(driver, bedd0e5c3c72, 45731, None)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.162712Z","level":"error","event":"25/05/12 11:07:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, bedd0e5c3c72, 45731, None)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.164116Z","level":"error","event":"25/05/12 11:07:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, bedd0e5c3c72, 45731, None)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.961318Z","level":"error","event":"25/05/12 11:07:02 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:02.967331Z","level":"error","event":"25/05/12 11:07:02 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.345469Z","level":"error","event":"Traceback (most recent call last):","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.346118Z","level":"error","event":"  File \"/opt/airflow/jobs/YFinance/daily.py\", line 27, in <module>","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.369645Z","level":"error","event":"    .load().withColumn(\"date\", to_date(col(\"Date\")))","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.370406Z","level":"error","event":"     ^^^^^^","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.371117Z","level":"error","event":"  File \"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py\", line 314, in load","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.371667Z","level":"error","event":"    return self._df(self._jreader.load())","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.372195Z","level":"error","event":"                    ^^^^^^^^^^^^^^^^^^^^","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.372846Z","level":"error","event":"  File \"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1322, in __call__","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.373487Z","level":"error","event":"    return_value = get_return_value(","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.374078Z","level":"error","event":"                   ^^^^^^^^^^^^^^^^^","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.374533Z","level":"error","event":"  File \"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.375036Z","level":"error","event":"    return f(*a, **kw)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.375471Z","level":"error","event":"           ^^^^^^^^^^^","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.375903Z","level":"error","event":"  File \"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py\", line 326, in get_return_value","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.376408Z","level":"error","event":"    raise Py4JJavaError(","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.376869Z","level":"error","event":"py4j.protocol.Py4JJavaError: An error occurred while calling o34.load.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.377364Z","level":"error","event":": org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.377844Z","level":"error","event":"\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.378316Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.378791Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.379299Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.379933Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.380702Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.381663Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.382654Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.383496Z","level":"error","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.384224Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.384940Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.385975Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:282)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.386751Z","level":"error","event":"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.387564Z","level":"error","event":"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.388423Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.389322Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.390259Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.390964Z","level":"error","event":"Caused by: java.lang.ClassNotFoundException: mongo.DefaultSource","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.391819Z","level":"error","event":"\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.392511Z","level":"error","event":"\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.393415Z","level":"error","event":"\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.394418Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.395284Z","level":"error","event":"\tat scala.util.Try$.apply(Try.scala:213)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.396019Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.396641Z","level":"error","event":"\tat scala.util.Failure.orElse(Try.scala:224)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.397157Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.397608Z","level":"error","event":"\t... 15 more","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.398051Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.516341","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"CalledProcessError","exc_value":"Command '['python', 'jobs/YFinance/daily.py']' returned non-zero exit status 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":825,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1088,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":408,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":212,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":235,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/dags/yfinance_daily.py","lineno":7,"name":"collect_data"},{"filename":"/usr/local/lib/python3.12/subprocess.py","lineno":571,"name":"run"}]}]}
{"timestamp":"2025-05-12T11:07:09.532386Z","level":"error","event":"25/05/12 11:07:09 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.534414Z","level":"error","event":"25/05/12 11:07:09 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.565661Z","level":"error","event":"25/05/12 11:07:09 INFO SparkUI: Stopped Spark web UI at http://bedd0e5c3c72:4040","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.607132Z","level":"error","event":"25/05/12 11:07:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.650141Z","level":"error","event":"25/05/12 11:07:09 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.651421Z","level":"error","event":"25/05/12 11:07:09 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.663908Z","level":"error","event":"25/05/12 11:07:09 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.671350Z","level":"error","event":"25/05/12 11:07:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.689727Z","level":"error","event":"25/05/12 11:07:09 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.690575Z","level":"error","event":"25/05/12 11:07:09 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.691906Z","level":"error","event":"25/05/12 11:07:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-389fa133-0809-4963-a769-c4e98873b504","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.701812Z","level":"error","event":"25/05/12 11:07:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-d0767d83-d57f-4fdb-80fa-f2e63ff58677","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:09.710547Z","level":"error","event":"25/05/12 11:07:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-d0767d83-d57f-4fdb-80fa-f2e63ff58677/pyspark-1c28da9b-2113-4415-a0e3-51c441cb3363","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:27:31.307370","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-05-12T11:27:31.308703","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/yfinance_daily.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T11:30:30.349792Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:30:30.351238Z","level":"info","event":"ERROR: Gagal mengambil data AADI.JK: biggybiggydataluwak.gxbo3d4.mongodb.net:27017: [Errno -2] Name or service not known (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 6821db291dbb4026bd42eba2, topology_type: Unknown, servers: [<ServerDescription ('biggybiggydataluwak.gxbo3d4.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('biggybiggydataluwak.gxbo3d4.mongodb.net:27017: [Errno -2] Name or service not known (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:30:30.351840Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:30:30.352425Z","level":"info","event":"ERROR: Gagal mengambil data AALI.JK: biggybiggydataluwak.gxbo3d4.mongodb.net:27017: [Errno -2] Name or service not known (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 6821db291dbb4026bd42eba2, topology_type: Unknown, servers: [<ServerDescription ('biggybiggydataluwak.gxbo3d4.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('biggybiggydataluwak.gxbo3d4.mongodb.net:27017: [Errno -2] Name or service not known (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:30:30.353027Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:30:30.353616Z","level":"info","event":"ERROR: Gagal mengambil data ABBA.JK: biggybiggydataluwak.gxbo3d4.mongodb.net:27017: [Errno -2] Name or service not known (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 6821db291dbb4026bd42eba2, topology_type: Unknown, servers: [<ServerDescription ('biggybiggydataluwak.gxbo3d4.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('biggybiggydataluwak.gxbo3d4.mongodb.net:27017: [Errno -2] Name or service not known (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:30:30.354215Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:30:30.354800Z","level":"info","event":"ERROR: Gagal mengambil data ABDA.JK: biggybiggydataluwak.gxbo3d4.mongodb.net:27017: [Errno -2] Name or service not known (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 6821db291dbb4026bd42eba2, topology_type: Unknown, servers: [<ServerDescription ('biggybiggydataluwak.gxbo3d4.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('biggybiggydataluwak.gxbo3d4.mongodb.net:27017: [Errno -2] Name or service not known (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:30:30.355492Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:30:30.356113Z","level":"info","event":"ERROR: Gagal mengambil data ABMM.JK: biggybiggydataluwak.gxbo3d4.mongodb.net:27017: [Errno -2] Name or service not known (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 6821db291dbb4026bd42eba2, topology_type: Unknown, servers: [<ServerDescription ('biggybiggydataluwak.gxbo3d4.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('biggybiggydataluwak.gxbo3d4.mongodb.net:27017: [Errno -2] Name or service not known (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:30:30.356703Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:30:33.516797Z","level":"error","event":"25/05/12 11:30:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:30:33.811712Z","level":"error","event":"Setting default log level to \"WARN\".","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:30:33.812404Z","level":"error","event":"To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:09.995311Z","level":"error","event":"Traceback (most recent call last):","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:09.995795Z","level":"error","event":"  File \"/opt/airflow/jobs/YFinance/daily.py\", line 27, in <module>","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:09.998399Z","level":"error","event":"    .load().withColumn(\"date\", to_date(col(\"Date\")))","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:09.998709Z","level":"error","event":"     ^^^^^^","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:09.998922Z","level":"error","event":"  File \"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py\", line 314, in load","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:09.999126Z","level":"error","event":"    return self._df(self._jreader.load())","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:09.999386Z","level":"error","event":"                    ^^^^^^^^^^^^^^^^^^^^","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:09.999574Z","level":"error","event":"  File \"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1322, in __call__","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.000098Z","level":"error","event":"    return_value = get_return_value(","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.000283Z","level":"error","event":"                   ^^^^^^^^^^^^^^^^^","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.000506Z","level":"error","event":"  File \"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.000737Z","level":"error","event":"    return f(*a, **kw)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.000906Z","level":"error","event":"           ^^^^^^^^^^^","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.001087Z","level":"error","event":"  File \"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py\", line 326, in get_return_value","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.001287Z","level":"error","event":"    raise Py4JJavaError(","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.001909Z","level":"error","event":"py4j.protocol.Py4JJavaError: An error occurred while calling o34.load.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.002250Z","level":"error","event":": com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=biggybiggydataluwak.gxbo3d4.mongodb.net:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketException: biggybiggydataluwak.gxbo3d4.mongodb.net}, caused by {java.net.UnknownHostException: biggybiggydataluwak.gxbo3d4.mongodb.net}}]","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.002527Z","level":"error","event":"\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.002813Z","level":"error","event":"\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.003071Z","level":"error","event":"\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.003274Z","level":"error","event":"\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.003452Z","level":"error","event":"\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.003611Z","level":"error","event":"\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.003766Z","level":"error","event":"\tat com.mongodb.client.internal.MongoDatabaseImpl.executeCommand(MongoDatabaseImpl.java:194)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.003920Z","level":"error","event":"\tat com.mongodb.client.internal.MongoDatabaseImpl.runCommand(MongoDatabaseImpl.java:163)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.004074Z","level":"error","event":"\tat com.mongodb.client.internal.MongoDatabaseImpl.runCommand(MongoDatabaseImpl.java:158)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.004259Z","level":"error","event":"\tat com.mongodb.spark.MongoConnector.$anonfun$hasSampleAggregateOperator$1(MongoConnector.scala:234)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.004447Z","level":"error","event":"\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.004607Z","level":"error","event":"\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.004786Z","level":"error","event":"\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.004942Z","level":"error","event":"\tat com.mongodb.spark.MongoConnector.hasSampleAggregateOperator(MongoConnector.scala:234)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.005096Z","level":"error","event":"\tat com.mongodb.spark.rdd.MongoRDD.hasSampleAggregateOperator$lzycompute(MongoRDD.scala:221)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.005268Z","level":"error","event":"\tat com.mongodb.spark.rdd.MongoRDD.hasSampleAggregateOperator(MongoRDD.scala:221)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.005447Z","level":"error","event":"\tat com.mongodb.spark.sql.MongoInferSchema$.apply(MongoInferSchema.scala:68)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.005613Z","level":"error","event":"\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:97)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.005770Z","level":"error","event":"\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.005918Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.006085Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.006259Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.006431Z","level":"error","event":"\tat scala.Option.getOrElse(Option.scala:189)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.006617Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.006793Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.006972Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.007129Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.007298Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.007472Z","level":"error","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.007678Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.007860Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.008050Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:282)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.008236Z","level":"error","event":"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.008437Z","level":"error","event":"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.008644Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.008818Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.008989Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.009146Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:31:10.055698","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"CalledProcessError","exc_value":"Command '['python', 'jobs/YFinance/daily.py']' returned non-zero exit status 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":825,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1088,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":408,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":212,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":235,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/dags/yfinance_daily.py","lineno":7,"name":"collect_data"},{"filename":"/usr/local/lib/python3.12/subprocess.py","lineno":571,"name":"run"}]}]}
