{"timestamp":"2025-05-12T09:19:46.086309","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-05-12T09:19:46.087643","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/yfinance_daily.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T09:22:41.179556Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T09:22:41.181618Z","level":"info","event":"ERROR: Gagal mengambil data AADI.JK: localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 6821bd3a3b85797927644ed0, topology_type: Unknown, servers: [<ServerDescription ('localhost', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T09:22:41.194496Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T09:22:41.196110Z","level":"info","event":"ERROR: Gagal mengambil data AALI.JK: localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 6821bd3a3b85797927644ed0, topology_type: Unknown, servers: [<ServerDescription ('localhost', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T09:22:41.197969Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T09:22:41.199809Z","level":"info","event":"ERROR: Gagal mengambil data ABBA.JK: localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 6821bd3a3b85797927644ed0, topology_type: Unknown, servers: [<ServerDescription ('localhost', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T09:22:41.201429Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T09:22:41.203366Z","level":"info","event":"ERROR: Gagal mengambil data ABDA.JK: localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 6821bd3a3b85797927644ed0, topology_type: Unknown, servers: [<ServerDescription ('localhost', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T09:22:41.205193Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T09:22:41.207340Z","level":"info","event":"ERROR: Gagal mengambil data ABMM.JK: localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 6821bd3a3b85797927644ed0, topology_type: Unknown, servers: [<ServerDescription ('localhost', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('localhost:27017: [Errno 111] Connection refused (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T09:22:41.209529Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T09:22:49.667530Z","level":"error","event":"25/05/12 09:22:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:50.295246Z","level":"error","event":"25/05/12 09:22:50 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:50.296759Z","level":"error","event":"25/05/12 09:22:50 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:50.298242Z","level":"error","event":"25/05/12 09:22:50 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:50.299551Z","level":"error","event":"25/05/12 09:22:50 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:50.736799Z","level":"error","event":"25/05/12 09:22:50 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:50.738601Z","level":"error","event":"25/05/12 09:22:50 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:50.740046Z","level":"error","event":"25/05/12 09:22:50 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:50.822489Z","level":"error","event":"25/05/12 09:22:50 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:50.824299Z","level":"error","event":"25/05/12 09:22:50 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:50.825906Z","level":"error","event":"25/05/12 09:22:50 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:50.827349Z","level":"error","event":"25/05/12 09:22:50 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:50.894043Z","level":"error","event":"25/05/12 09:22:50 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:50.916362Z","level":"error","event":"25/05/12 09:22:50 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:50.918655Z","level":"error","event":"25/05/12 09:22:50 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:51.104643Z","level":"error","event":"25/05/12 09:22:51 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:51.106837Z","level":"error","event":"25/05/12 09:22:51 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:51.109907Z","level":"error","event":"25/05/12 09:22:51 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:51.112921Z","level":"error","event":"25/05/12 09:22:51 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:51.114865Z","level":"error","event":"25/05/12 09:22:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:51.709975Z","level":"error","event":"25/05/12 09:22:51 INFO Utils: Successfully started service 'sparkDriver' on port 39021.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:51.796426Z","level":"error","event":"25/05/12 09:22:51 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:51.886499Z","level":"error","event":"25/05/12 09:22:51 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:51.937563Z","level":"error","event":"25/05/12 09:22:51 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:51.939633Z","level":"error","event":"25/05/12 09:22:51 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:51.950108Z","level":"error","event":"25/05/12 09:22:51 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.025198Z","level":"error","event":"25/05/12 09:22:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-304ce0ba-4f41-47e6-9419-d84fd96fd104","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.060289Z","level":"error","event":"25/05/12 09:22:52 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.129079Z","level":"error","event":"25/05/12 09:22:52 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.637433Z","level":"error","event":"25/05/12 09:22:52 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.852094Z","level":"error","event":"25/05/12 09:22:52 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.968145Z","level":"error","event":"25/05/12 09:22:52 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.968945Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.969702Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.970571Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.971360Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.972223Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.973274Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.974107Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.974990Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.975961Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.976647Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.977407Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.978425Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.979126Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.979866Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.980723Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.981496Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.982307Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.983172Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.983929Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.984628Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.985306Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.986065Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.986859Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.987719Z","level":"error","event":"25/05/12 09:22:52 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.988460Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.989217Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.989977Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.990732Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.991442Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.992093Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.992763Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.993619Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.994500Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.995234Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.996019Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.996888Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.997770Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.998498Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.999246Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:52.999943Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.000666Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.001444Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.002239Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.003078Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.003868Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.004651Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.005788Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.007382Z","level":"error","event":"25/05/12 09:22:52 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.008497Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.010429Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.012595Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.013872Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.015511Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.017360Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.018973Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.019906Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.021321Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.022604Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.023563Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.024342Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.025420Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.026281Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.027136Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.027884Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.028610Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.029511Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.030330Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.031172Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.032046Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.032857Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.033630Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.034329Z","level":"error","event":"25/05/12 09:22:52 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.035243Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.036017Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.038381Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.041570Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.043203Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.044361Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.045228Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.046144Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.047124Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.049235Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.050208Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.051293Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.052368Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.057200Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.058879Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.059805Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.060951Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.061829Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.063432Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.064868Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.066425Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.067674Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.068673Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.200357Z","level":"error","event":"25/05/12 09:22:53 INFO Executor: Starting executor ID driver on host aeef85c9aebd","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.201958Z","level":"error","event":"25/05/12 09:22:53 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.203174Z","level":"error","event":"25/05/12 09:22:53 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.216897Z","level":"error","event":"25/05/12 09:22:53 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.218944Z","level":"error","event":"25/05/12 09:22:53 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@7e56a3df for default.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.330008Z","level":"error","event":"25/05/12 09:22:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34621.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.333938Z","level":"error","event":"25/05/12 09:22:53 INFO NettyBlockTransferService: Server created on aeef85c9aebd:34621","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.345979Z","level":"error","event":"25/05/12 09:22:53 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.378914Z","level":"error","event":"25/05/12 09:22:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, aeef85c9aebd, 34621, None)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.399605Z","level":"error","event":"25/05/12 09:22:53 INFO BlockManagerMasterEndpoint: Registering block manager aeef85c9aebd:34621 with 434.4 MiB RAM, BlockManagerId(driver, aeef85c9aebd, 34621, None)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.406642Z","level":"error","event":"25/05/12 09:22:53 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, aeef85c9aebd, 34621, None)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:53.409862Z","level":"error","event":"25/05/12 09:22:53 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, aeef85c9aebd, 34621, None)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:54.834211Z","level":"error","event":"25/05/12 09:22:54 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:54.901735Z","level":"error","event":"Traceback (most recent call last):","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:54.903511Z","level":"error","event":"  File \"/opt/airflow/jobs/YFinance/daily.py\", line 23, in <module>","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:54.909847Z","level":"error","event":"    df = _spark.read.format(\"mongo\") \\","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:54.910696Z","level":"error","event":"         ^^^^^^^^^^^","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:54.911579Z","level":"error","event":"  File \"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py\", line 1706, in read","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:54.912550Z","level":"error","event":"    return DataFrameReader(self)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:54.913393Z","level":"error","event":"           ^^^^^^^^^^^^^^^^^^^^^","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:54.914246Z","level":"error","event":"  File \"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py\", line 70, in __init__","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:54.920370Z","level":"error","event":"    self._jreader = spark._jsparkSession.read()","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:54.921507Z","level":"error","event":"                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:54.922330Z","level":"error","event":"  File \"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1322, in __call__","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:54.923486Z","level":"error","event":"    return_value = get_return_value(","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:54.924305Z","level":"error","event":"                   ^^^^^^^^^^^^^^^^^","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:54.925294Z","level":"error","event":"  File \"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", line 185, in deco","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:54.926248Z","level":"error","event":"    raise converted from None","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:54.961227Z","level":"error","event":"25/05/12 09:22:54 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.016629Z","level":"error","event":"pyspark.errors.exceptions.captured.IllegalArgumentException: The value of property spark.mongodb.read.connection.uri must not be null","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.017685Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.018585Z","level":"error","event":"JVM stacktrace:","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.019480Z","level":"error","event":"java.lang.IllegalArgumentException: The value of property spark.mongodb.read.connection.uri must not be null","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.020386Z","level":"error","event":"\tat org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:219)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.021152Z","level":"error","event":"\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1403)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.021931Z","level":"error","event":"\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1384)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.022710Z","level":"error","event":"\tat org.apache.spark.sql.internal.SharedState.$anonfun$x$1$2(SharedState.scala:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.023503Z","level":"error","event":"\tat scala.collection.immutable.Map$Map3.foreach(Map.scala:376)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.024191Z","level":"error","event":"\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:69)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.024929Z","level":"error","event":"\tat org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.025682Z","level":"error","event":"\tat scala.Option.getOrElse(Option.scala:189)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.026452Z","level":"error","event":"\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.027198Z","level":"error","event":"\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.027886Z","level":"error","event":"\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.028584Z","level":"error","event":"\tat scala.Option.getOrElse(Option.scala:189)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.029272Z","level":"error","event":"\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.030007Z","level":"error","event":"\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.030739Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameReader.<init>(DataFrameReader.scala:699)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.031413Z","level":"error","event":"\tat org.apache.spark.sql.SparkSession.read(SparkSession.scala:783)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.032220Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.033373Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.034851Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.035705Z","level":"error","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.036615Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.037544Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.038592Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:282)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.039394Z","level":"error","event":"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.040121Z","level":"error","event":"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.040891Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.041568Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.042381Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.043234Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.163442","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"CalledProcessError","exc_value":"Command '['python', 'jobs/YFinance/daily.py']' returned non-zero exit status 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":825,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1088,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":408,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":212,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":235,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/dags/yfinance_daily.py","lineno":7,"name":"collect_data"},{"filename":"/usr/local/lib/python3.12/subprocess.py","lineno":571,"name":"run"}]}]}
{"timestamp":"2025-05-12T09:22:55.175951Z","level":"error","event":"25/05/12 09:22:55 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.177356Z","level":"error","event":"25/05/12 09:22:55 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.234389Z","level":"error","event":"25/05/12 09:22:55 INFO SparkUI: Stopped Spark web UI at http://aeef85c9aebd:4040","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.289041Z","level":"error","event":"25/05/12 09:22:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.358963Z","level":"error","event":"25/05/12 09:22:55 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.360719Z","level":"error","event":"25/05/12 09:22:55 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.379801Z","level":"error","event":"25/05/12 09:22:55 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.390242Z","level":"error","event":"25/05/12 09:22:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.438416Z","level":"error","event":"25/05/12 09:22:55 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.440001Z","level":"error","event":"25/05/12 09:22:55 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.443811Z","level":"error","event":"25/05/12 09:22:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-1965d796-5da2-49b5-9e97-76434e9754cd/pyspark-c0c5fd02-c9b4-4618-b853-89130902d477","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.456019Z","level":"error","event":"25/05/12 09:22:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-f0314b59-32b1-453b-ad83-aeb35dcf89a5","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T09:22:55.466981Z","level":"error","event":"25/05/12 09:22:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-1965d796-5da2-49b5-9e97-76434e9754cd","chan":"stderr","logger":"task"}
