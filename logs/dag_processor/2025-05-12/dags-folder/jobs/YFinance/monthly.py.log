{"timestamp":"2025-05-12T02:54:17.114969","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/monthly.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:54:46.657411Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:54:46.658303Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:54:46.659419Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:54:46.671056Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:54:46.671679Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:54:46.672290Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:54:46.672882Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:54:46.673524Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:54:46.674178Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:54:46.674810Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:54:46.675391Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:54:49.299593","level":"error","event":"Process timed out, PID: 38","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-05-12T02:54:49.300623","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/monthly.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/jobs/YFinance/monthly.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 38","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/monthly.py","lineno":21,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/session.py","lineno":497,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":515,"name":"getOrCreate"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":201,"name":"__init__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/context.py","lineno":436,"name":"_ensure_initialized"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/java_gateway.py","lineno":104,"name":"launch_gateway"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-05-12T02:54:52.430898Z","level":"error","event":"25/05/12 02:54:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:53.866448Z","level":"error","event":"25/05/12 02:54:53 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:53.890134Z","level":"error","event":"25/05/12 02:54:53 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:53.891855Z","level":"error","event":"25/05/12 02:54:53 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:53.892944Z","level":"error","event":"25/05/12 02:54:53 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:55.276152Z","level":"error","event":"Exception in thread \"main\" java.nio.file.NoSuchFileException: /tmp/tmp863d7hyr/connection9786073291628618537.info","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:55.276978Z","level":"error","event":"\tat java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:55.277652Z","level":"error","event":"\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:55.278242Z","level":"error","event":"\tat java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:55.278843Z","level":"error","event":"\tat java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:218)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:55.279519Z","level":"error","event":"\tat java.base/java.nio.file.Files.newByteChannel(Files.java:380)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:55.280130Z","level":"error","event":"\tat java.base/java.nio.file.Files.createFile(Files.java:658)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:55.280724Z","level":"error","event":"\tat java.base/java.nio.file.TempFileHelper.create(TempFileHelper.java:136)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:55.281232Z","level":"error","event":"\tat java.base/java.nio.file.TempFileHelper.createTempFile(TempFileHelper.java:159)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:55.281671Z","level":"error","event":"\tat java.base/java.nio.file.Files.createTempFile(Files.java:878)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:55.282233Z","level":"error","event":"\tat org.apache.spark.api.python.PythonGatewayServer$.main(PythonGatewayServer.scala:54)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:55.282773Z","level":"error","event":"\tat org.apache.spark.api.python.PythonGatewayServer.main(PythonGatewayServer.scala)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:55.283246Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:55.283865Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:55.284490Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:55.285075Z","level":"error","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:55.285720Z","level":"error","event":"\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:55.286358Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1034)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:55.287119Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:55.287782Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:55.288481Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:55.289169Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:55.289769Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:54:55.290513Z","level":"error","event":"\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:36.688037","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/monthly.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:55:48.074180Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:55:48.074845Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:55:48.075308Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:55:48.075761Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:55:48.076216Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:55:48.076561Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:55:48.076887Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:55:48.077223Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:55:48.077580Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:55:48.077956Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:55:48.078230Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:55:52.443143Z","level":"error","event":"25/05/12 02:55:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:52.844849Z","level":"error","event":"25/05/12 02:55:52 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:52.857586Z","level":"error","event":"25/05/12 02:55:52 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:52.858510Z","level":"error","event":"25/05/12 02:55:52 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:52.871257Z","level":"error","event":"25/05/12 02:55:52 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:53.072169Z","level":"error","event":"25/05/12 02:55:53 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:53.083403Z","level":"error","event":"25/05/12 02:55:53 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:53.084033Z","level":"error","event":"25/05/12 02:55:53 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:53.126749Z","level":"error","event":"25/05/12 02:55:53 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:53.135312Z","level":"error","event":"25/05/12 02:55:53 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:53.135898Z","level":"error","event":"25/05/12 02:55:53 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:53.136374Z","level":"error","event":"25/05/12 02:55:53 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:53.169364Z","level":"error","event":"25/05/12 02:55:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:53.185968Z","level":"error","event":"25/05/12 02:55:53 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:53.196136Z","level":"error","event":"25/05/12 02:55:53 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:53.259201Z","level":"error","event":"25/05/12 02:55:53 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:53.269235Z","level":"error","event":"25/05/12 02:55:53 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:53.270023Z","level":"error","event":"25/05/12 02:55:53 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:53.270567Z","level":"error","event":"25/05/12 02:55:53 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:53.271260Z","level":"error","event":"25/05/12 02:55:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:53.692175Z","level":"error","event":"25/05/12 02:55:53 INFO Utils: Successfully started service 'sparkDriver' on port 44947.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:53.746050Z","level":"error","event":"25/05/12 02:55:53 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:53.830974Z","level":"error","event":"25/05/12 02:55:53 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:53.863170Z","level":"error","event":"25/05/12 02:55:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:53.875267Z","level":"error","event":"25/05/12 02:55:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:53.875999Z","level":"error","event":"25/05/12 02:55:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:53.912406Z","level":"error","event":"25/05/12 02:55:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0ba672cb-063d-45fd-9f25-9faae9a51de9","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:53.942405Z","level":"error","event":"25/05/12 02:55:53 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:53.983904Z","level":"error","event":"25/05/12 02:55:53 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.272254Z","level":"error","event":"25/05/12 02:55:54 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.413658Z","level":"error","event":"25/05/12 02:55:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.474072Z","level":"error","event":"25/05/12 02:55:54 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.474633Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.475047Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.475467Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.475863Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.476300Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.476692Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.477049Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.477521Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.478159Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.478745Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.479462Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.480036Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.480623Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.481185Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.481696Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.482178Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.482691Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.483155Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.483864Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.484481Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.485030Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.485511Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.486006Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.496848Z","level":"error","event":"25/05/12 02:55:54 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.497780Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.498659Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.499379Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.500119Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.500920Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.501599Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.502098Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.502564Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.503104Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.503620Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.504087Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.504692Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.505232Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.505859Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.506487Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.507095Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.507684Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.508209Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.508789Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.509421Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.510220Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.510933Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.511551Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.512249Z","level":"error","event":"25/05/12 02:55:54 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.513111Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.513811Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.514425Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.515074Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.515686Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.516425Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.517125Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.517748Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.518280Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.518838Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.519354Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.519956Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.520650Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.521263Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.521880Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.522388Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.522892Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.523339Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.523733Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.524159Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.524617Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.525021Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.525474Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.525843Z","level":"error","event":"25/05/12 02:55:54 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.526205Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.526625Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.535898Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.536418Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.536822Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.537199Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.537560Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.537958Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.538290Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.538636Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.539010Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.539404Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.539738Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.540090Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.540429Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.540781Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.541118Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.541637Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.542273Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.542730Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.543158Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.543596Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.544026Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.577835Z","level":"error","event":"25/05/12 02:55:54 INFO Executor: Starting executor ID driver on host 4ad261c1f1d7","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.586314Z","level":"error","event":"25/05/12 02:55:54 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.586867Z","level":"error","event":"25/05/12 02:55:54 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.596958Z","level":"error","event":"25/05/12 02:55:54 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.597628Z","level":"error","event":"25/05/12 02:55:54 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@73663445 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.627488Z","level":"error","event":"25/05/12 02:55:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46705.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.634475Z","level":"error","event":"25/05/12 02:55:54 INFO NettyBlockTransferService: Server created on 4ad261c1f1d7:46705","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.635011Z","level":"error","event":"25/05/12 02:55:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.642305Z","level":"error","event":"25/05/12 02:55:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4ad261c1f1d7, 46705, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.651030Z","level":"error","event":"25/05/12 02:55:54 INFO BlockManagerMasterEndpoint: Registering block manager 4ad261c1f1d7:46705 with 434.4 MiB RAM, BlockManagerId(driver, 4ad261c1f1d7, 46705, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.651623Z","level":"error","event":"25/05/12 02:55:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4ad261c1f1d7, 46705, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:54.652096Z","level":"error","event":"25/05/12 02:55:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4ad261c1f1d7, 46705, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:55.184595Z","level":"error","event":"25/05/12 02:55:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:55.194862Z","level":"error","event":"25/05/12 02:55:55 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:56.624029","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/monthly.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/monthly.py","lineno":27,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:55:56.654267Z","level":"error","event":"25/05/12 02:55:56 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:56.654778Z","level":"error","event":"25/05/12 02:55:56 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:56.664682Z","level":"error","event":"25/05/12 02:55:56 INFO SparkUI: Stopped Spark web UI at http://4ad261c1f1d7:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:56.679408Z","level":"error","event":"25/05/12 02:55:56 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:56.694108Z","level":"error","event":"25/05/12 02:55:56 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:56.700836Z","level":"error","event":"25/05/12 02:55:56 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:56.707846Z","level":"error","event":"25/05/12 02:55:56 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:56.708461Z","level":"error","event":"25/05/12 02:55:56 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:56.716595Z","level":"error","event":"25/05/12 02:55:56 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:56.723033Z","level":"error","event":"25/05/12 02:55:56 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:56.723583Z","level":"error","event":"25/05/12 02:55:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-63513795-8839-4727-9d88-dc96b3e46c86","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:56.731046Z","level":"error","event":"25/05/12 02:55:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-d5e3297b-91ec-43b5-88e7-d279e1946f30","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:56.731493Z","level":"error","event":"25/05/12 02:55:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-d5e3297b-91ec-43b5-88e7-d279e1946f30/pyspark-c3745564-f4a6-496d-b475-235233a03bf2","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:27.425113","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/monthly.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:56:39.941996Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.942514Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.942931Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.943318Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.943627Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.944036Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.944418Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.944944Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.945387Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.945829Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.946256Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:56:44.921502Z","level":"error","event":"25/05/12 02:56:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:45.431551Z","level":"error","event":"25/05/12 02:56:45 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:45.460771Z","level":"error","event":"25/05/12 02:56:45 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:45.461493Z","level":"error","event":"25/05/12 02:56:45 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:45.462157Z","level":"error","event":"25/05/12 02:56:45 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:45.650309Z","level":"error","event":"25/05/12 02:56:45 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:45.681767Z","level":"error","event":"25/05/12 02:56:45 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:45.682496Z","level":"error","event":"25/05/12 02:56:45 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:45.745208Z","level":"error","event":"25/05/12 02:56:45 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:45.770968Z","level":"error","event":"25/05/12 02:56:45 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:45.771800Z","level":"error","event":"25/05/12 02:56:45 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:45.772565Z","level":"error","event":"25/05/12 02:56:45 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:45.806495Z","level":"error","event":"25/05/12 02:56:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:45.829022Z","level":"error","event":"25/05/12 02:56:45 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:45.829707Z","level":"error","event":"25/05/12 02:56:45 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:45.922111Z","level":"error","event":"25/05/12 02:56:45 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:45.948492Z","level":"error","event":"25/05/12 02:56:45 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:45.949090Z","level":"error","event":"25/05/12 02:56:45 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:45.949652Z","level":"error","event":"25/05/12 02:56:45 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:45.950332Z","level":"error","event":"25/05/12 02:56:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:46.401442Z","level":"error","event":"25/05/12 02:56:46 INFO Utils: Successfully started service 'sparkDriver' on port 37321.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:46.505833Z","level":"error","event":"25/05/12 02:56:46 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:46.671276Z","level":"error","event":"25/05/12 02:56:46 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:46.749344Z","level":"error","event":"25/05/12 02:56:46 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:46.787261Z","level":"error","event":"25/05/12 02:56:46 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:46.788120Z","level":"error","event":"25/05/12 02:56:46 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:46.845418Z","level":"error","event":"25/05/12 02:56:46 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1ddab81b-fc43-4b09-ad32-069f275c7848","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:46.903864Z","level":"error","event":"25/05/12 02:56:46 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:46.940946Z","level":"error","event":"25/05/12 02:56:46 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.216070Z","level":"error","event":"25/05/12 02:56:47 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.423174Z","level":"error","event":"25/05/12 02:56:47 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.544279Z","level":"error","event":"25/05/12 02:56:47 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.545066Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.545989Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.546718Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.547533Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.548215Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.549291Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.550170Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.551170Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.551892Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.552567Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.553467Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.554328Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.555063Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.555985Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.556797Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.557678Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.560022Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.560852Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.561578Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.562318Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.563167Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.564117Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.565300Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.596742Z","level":"error","event":"25/05/12 02:56:47 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.598400Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.599170Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.599748Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.600396Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.601008Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.601798Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.603327Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.604109Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.604787Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.605434Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.606090Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.606740Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.607651Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.608508Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.609423Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.610402Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.611979Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.612820Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.613604Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.614372Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.615077Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.615843Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.616831Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.617741Z","level":"error","event":"25/05/12 02:56:47 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.618607Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.619475Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.620370Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.621152Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.622465Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.623439Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.624286Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.626048Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.626999Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.628264Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.629683Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.630572Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.631252Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.632028Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.632769Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.633510Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.634970Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.635836Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.637056Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.637934Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.638710Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.639455Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.640139Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.640867Z","level":"error","event":"25/05/12 02:56:47 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.641581Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.643771Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.675967Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.676691Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.677509Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.678209Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.679009Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.679792Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.684701Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.685684Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.686363Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.686992Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.687635Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.688445Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.690582Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.691304Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.692160Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.693219Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.693891Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.694575Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.695426Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.696369Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.697198Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.732300Z","level":"error","event":"25/05/12 02:56:47 INFO Executor: Starting executor ID driver on host 4ad261c1f1d7","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.763345Z","level":"error","event":"25/05/12 02:56:47 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.764066Z","level":"error","event":"25/05/12 02:56:47 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.764827Z","level":"error","event":"25/05/12 02:56:47 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.765515Z","level":"error","event":"25/05/12 02:56:47 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@5e67b91e for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.818554Z","level":"error","event":"25/05/12 02:56:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43097.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.855622Z","level":"error","event":"25/05/12 02:56:47 INFO NettyBlockTransferService: Server created on 4ad261c1f1d7:43097","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.856629Z","level":"error","event":"25/05/12 02:56:47 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.857386Z","level":"error","event":"25/05/12 02:56:47 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4ad261c1f1d7, 43097, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.858129Z","level":"error","event":"25/05/12 02:56:47 INFO BlockManagerMasterEndpoint: Registering block manager 4ad261c1f1d7:43097 with 434.4 MiB RAM, BlockManagerId(driver, 4ad261c1f1d7, 43097, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.858856Z","level":"error","event":"25/05/12 02:56:47 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4ad261c1f1d7, 43097, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:47.859617Z","level":"error","event":"25/05/12 02:56:47 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4ad261c1f1d7, 43097, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:48.628672Z","level":"error","event":"25/05/12 02:56:48 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:48.662957Z","level":"error","event":"25/05/12 02:56:48 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:51.921963","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/monthly.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/monthly.py","lineno":27,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:56:51.984911Z","level":"error","event":"25/05/12 02:56:51 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:51.985604Z","level":"error","event":"25/05/12 02:56:51 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:52.061599Z","level":"error","event":"25/05/12 02:56:52 INFO SparkUI: Stopped Spark web UI at http://4ad261c1f1d7:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:52.129137Z","level":"error","event":"25/05/12 02:56:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:52.169018Z","level":"error","event":"25/05/12 02:56:52 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:52.173674Z","level":"error","event":"25/05/12 02:56:52 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:52.207030Z","level":"error","event":"25/05/12 02:56:52 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:52.207652Z","level":"error","event":"25/05/12 02:56:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:52.234477Z","level":"error","event":"25/05/12 02:56:52 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:52.235316Z","level":"error","event":"25/05/12 02:56:52 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:52.236102Z","level":"error","event":"25/05/12 02:56:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-30615302-ff66-42c4-ac1b-00894680c9ce","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:52.236747Z","level":"error","event":"25/05/12 02:56:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-b6ff058b-cadd-4408-b798-27fa223ad970","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:52.237444Z","level":"error","event":"25/05/12 02:56:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-30615302-ff66-42c4-ac1b-00894680c9ce/pyspark-61f4840e-708d-45b5-b469-0cffb67233fa","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:23.223305","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/monthly.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:57:32.453456Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.454053Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.454503Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.454978Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.455849Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.456323Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.456776Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.457291Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.457743Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.458274Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.458830Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:57:38.717311Z","level":"error","event":"25/05/12 02:57:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:39.094231Z","level":"error","event":"25/05/12 02:57:39 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:39.109330Z","level":"error","event":"25/05/12 02:57:39 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:39.109970Z","level":"error","event":"25/05/12 02:57:39 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:39.110662Z","level":"error","event":"25/05/12 02:57:39 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:39.273358Z","level":"error","event":"25/05/12 02:57:39 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:39.287576Z","level":"error","event":"25/05/12 02:57:39 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:39.288493Z","level":"error","event":"25/05/12 02:57:39 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:39.348268Z","level":"error","event":"25/05/12 02:57:39 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:39.361706Z","level":"error","event":"25/05/12 02:57:39 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:39.363330Z","level":"error","event":"25/05/12 02:57:39 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:39.364000Z","level":"error","event":"25/05/12 02:57:39 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:39.397315Z","level":"error","event":"25/05/12 02:57:39 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:39.414447Z","level":"error","event":"25/05/12 02:57:39 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:39.424986Z","level":"error","event":"25/05/12 02:57:39 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:39.499002Z","level":"error","event":"25/05/12 02:57:39 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:39.508317Z","level":"error","event":"25/05/12 02:57:39 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:39.509017Z","level":"error","event":"25/05/12 02:57:39 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:39.509631Z","level":"error","event":"25/05/12 02:57:39 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:39.510542Z","level":"error","event":"25/05/12 02:57:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:39.943852Z","level":"error","event":"25/05/12 02:57:39 INFO Utils: Successfully started service 'sparkDriver' on port 45581.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:39.992821Z","level":"error","event":"25/05/12 02:57:39 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.063639Z","level":"error","event":"25/05/12 02:57:40 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.103479Z","level":"error","event":"25/05/12 02:57:40 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.117218Z","level":"error","event":"25/05/12 02:57:40 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.118222Z","level":"error","event":"25/05/12 02:57:40 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.157788Z","level":"error","event":"25/05/12 02:57:40 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7b737c16-9ad7-40ec-a5bc-81c704944de4","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.181992Z","level":"error","event":"25/05/12 02:57:40 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.208585Z","level":"error","event":"25/05/12 02:57:40 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.411088Z","level":"error","event":"25/05/12 02:57:40 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.504632Z","level":"error","event":"25/05/12 02:57:40 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.581742Z","level":"error","event":"25/05/12 02:57:40 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.582671Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.583230Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.583797Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.584273Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.584765Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.585245Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.585801Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.586262Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.586737Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.587399Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.587941Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.588487Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.588961Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.589527Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.590168Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.590674Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.591237Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.591736Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.592208Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.592685Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.593161Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.593686Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.594155Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.634098Z","level":"error","event":"25/05/12 02:57:40 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.634945Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.635596Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.636255Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.636931Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.637631Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.638252Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.638806Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.639414Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.640022Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.640610Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.641076Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.641656Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.642391Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.643550Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.644230Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.644935Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.645669Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.646434Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.647198Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.648126Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.648728Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.649293Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.649861Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.650417Z","level":"error","event":"25/05/12 02:57:40 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.650997Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.651517Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.651996Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.652438Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.652853Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.653398Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.654017Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.654642Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.655228Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.655766Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.656339Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.656907Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.657486Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.658320Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.659015Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.659932Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.660549Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.661253Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.661801Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.662388Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.663005Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.663622Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.664243Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.664957Z","level":"error","event":"25/05/12 02:57:40 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.665660Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.666237Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.685924Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.686566Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.687121Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.687650Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.688256Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.688973Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.689825Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.690558Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.691329Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.692019Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.692553Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.692981Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.693459Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.693948Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.694484Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.694980Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.695475Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.695993Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.696441Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.696881Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.697337Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.718312Z","level":"error","event":"25/05/12 02:57:40 INFO Executor: Starting executor ID driver on host 4ad261c1f1d7","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.718850Z","level":"error","event":"25/05/12 02:57:40 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.719440Z","level":"error","event":"25/05/12 02:57:40 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.740901Z","level":"error","event":"25/05/12 02:57:40 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.741550Z","level":"error","event":"25/05/12 02:57:40 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6ff21454 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.763047Z","level":"error","event":"25/05/12 02:57:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36741.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.763547Z","level":"error","event":"25/05/12 02:57:40 INFO NettyBlockTransferService: Server created on 4ad261c1f1d7:36741","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.763995Z","level":"error","event":"25/05/12 02:57:40 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.784091Z","level":"error","event":"25/05/12 02:57:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4ad261c1f1d7, 36741, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.784688Z","level":"error","event":"25/05/12 02:57:40 INFO BlockManagerMasterEndpoint: Registering block manager 4ad261c1f1d7:36741 with 434.4 MiB RAM, BlockManagerId(driver, 4ad261c1f1d7, 36741, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.785198Z","level":"error","event":"25/05/12 02:57:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4ad261c1f1d7, 36741, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:40.785723Z","level":"error","event":"25/05/12 02:57:40 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4ad261c1f1d7, 36741, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:41.347053Z","level":"error","event":"25/05/12 02:57:41 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:41.380292Z","level":"error","event":"25/05/12 02:57:41 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:43.428196","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/monthly.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/monthly.py","lineno":27,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:57:43.453705Z","level":"error","event":"25/05/12 02:57:43 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:43.454332Z","level":"error","event":"25/05/12 02:57:43 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:43.478175Z","level":"error","event":"25/05/12 02:57:43 INFO SparkUI: Stopped Spark web UI at http://4ad261c1f1d7:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:43.502637Z","level":"error","event":"25/05/12 02:57:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:43.525090Z","level":"error","event":"25/05/12 02:57:43 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:43.525660Z","level":"error","event":"25/05/12 02:57:43 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:43.547677Z","level":"error","event":"25/05/12 02:57:43 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:43.548223Z","level":"error","event":"25/05/12 02:57:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:43.567980Z","level":"error","event":"25/05/12 02:57:43 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:43.568654Z","level":"error","event":"25/05/12 02:57:43 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:43.569171Z","level":"error","event":"25/05/12 02:57:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-4b4cf035-49aa-449b-b004-e9660a0cccb3","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:43.569646Z","level":"error","event":"25/05/12 02:57:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-6cdbd0ba-3859-4f97-bedd-7484aaed80a0/pyspark-bf57b42c-af69-4101-8168-d69c4915c0f8","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:43.570033Z","level":"error","event":"25/05/12 02:57:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-6cdbd0ba-3859-4f97-bedd-7484aaed80a0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:13.995103","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/monthly.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:58:24.344516Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:58:24.345135Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:58:24.345663Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:58:24.346167Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:58:24.346609Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:58:24.347126Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:58:24.347697Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:58:24.348249Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:58:24.348844Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:58:24.349389Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:58:24.349956Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:58:28.417621Z","level":"error","event":"25/05/12 02:58:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:28.789991Z","level":"error","event":"25/05/12 02:58:28 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:28.799546Z","level":"error","event":"25/05/12 02:58:28 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:28.800495Z","level":"error","event":"25/05/12 02:58:28 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:28.801116Z","level":"error","event":"25/05/12 02:58:28 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:28.938635Z","level":"error","event":"25/05/12 02:58:28 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:28.948569Z","level":"error","event":"25/05/12 02:58:28 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:28.949239Z","level":"error","event":"25/05/12 02:58:28 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:28.996196Z","level":"error","event":"25/05/12 02:58:28 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:29.006997Z","level":"error","event":"25/05/12 02:58:28 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:29.007822Z","level":"error","event":"25/05/12 02:58:28 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:29.008809Z","level":"error","event":"25/05/12 02:58:28 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:29.044927Z","level":"error","event":"25/05/12 02:58:29 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:29.058456Z","level":"error","event":"25/05/12 02:58:29 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:29.068604Z","level":"error","event":"25/05/12 02:58:29 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:29.134632Z","level":"error","event":"25/05/12 02:58:29 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:29.155736Z","level":"error","event":"25/05/12 02:58:29 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:29.156483Z","level":"error","event":"25/05/12 02:58:29 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:29.157161Z","level":"error","event":"25/05/12 02:58:29 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:29.157863Z","level":"error","event":"25/05/12 02:58:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:29.573548Z","level":"error","event":"25/05/12 02:58:29 INFO Utils: Successfully started service 'sparkDriver' on port 44601.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:29.635099Z","level":"error","event":"25/05/12 02:58:29 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:29.708509Z","level":"error","event":"25/05/12 02:58:29 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:29.763568Z","level":"error","event":"25/05/12 02:58:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:29.779914Z","level":"error","event":"25/05/12 02:58:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:29.780752Z","level":"error","event":"25/05/12 02:58:29 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:29.832244Z","level":"error","event":"25/05/12 02:58:29 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7e7df1c4-b33e-45fd-ae34-9678ef192674","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:29.865227Z","level":"error","event":"25/05/12 02:58:29 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:29.903575Z","level":"error","event":"25/05/12 02:58:29 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.139579Z","level":"error","event":"25/05/12 02:58:30 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.251636Z","level":"error","event":"25/05/12 02:58:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.266661Z","level":"error","event":"25/05/12 02:58:30 INFO Utils: Successfully started service 'SparkUI' on port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.337145Z","level":"error","event":"25/05/12 02:58:30 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.337770Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.338419Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.338945Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.339542Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.340049Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.340576Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.341313Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.341977Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.342710Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.343430Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.344200Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.345002Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.345817Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.346494Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.347217Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.347843Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.348536Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.349156Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.349778Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.350376Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.350992Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.351536Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.352123Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.363708Z","level":"error","event":"25/05/12 02:58:30 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.364522Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.365156Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.365785Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.366476Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.367253Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.368028Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.368820Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.369689Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.370376Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.371094Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.371844Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.372432Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.374345Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.375133Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.375868Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.376594Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.377311Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.378160Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.378925Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.379625Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.380382Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.381108Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.381896Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.382680Z","level":"error","event":"25/05/12 02:58:30 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.383731Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.384551Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.385435Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.386788Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.387750Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.388655Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.389731Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.391411Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.392226Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.393000Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.393817Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.394718Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.395934Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.396872Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.398121Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.399072Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.400107Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.400822Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.401584Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.402285Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.402941Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.403559Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.404207Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.404715Z","level":"error","event":"25/05/12 02:58:30 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.405225Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.406014Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.416954Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.417781Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.418375Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.418913Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.419474Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.420051Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.420772Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.421511Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.422288Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.422940Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.423493Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.424005Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.424500Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.424961Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.425415Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.425911Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.426468Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.427089Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.427641Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.428181Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.428756Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.474263Z","level":"error","event":"25/05/12 02:58:30 INFO Executor: Starting executor ID driver on host 4ad261c1f1d7","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.485125Z","level":"error","event":"25/05/12 02:58:30 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.485874Z","level":"error","event":"25/05/12 02:58:30 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.498411Z","level":"error","event":"25/05/12 02:58:30 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.499185Z","level":"error","event":"25/05/12 02:58:30 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@20f8194d for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.581289Z","level":"error","event":"25/05/12 02:58:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45281.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.594704Z","level":"error","event":"25/05/12 02:58:30 INFO NettyBlockTransferService: Server created on 4ad261c1f1d7:45281","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.595506Z","level":"error","event":"25/05/12 02:58:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.606792Z","level":"error","event":"25/05/12 02:58:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4ad261c1f1d7, 45281, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.629871Z","level":"error","event":"25/05/12 02:58:30 INFO BlockManagerMasterEndpoint: Registering block manager 4ad261c1f1d7:45281 with 434.4 MiB RAM, BlockManagerId(driver, 4ad261c1f1d7, 45281, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.630650Z","level":"error","event":"25/05/12 02:58:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4ad261c1f1d7, 45281, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.631360Z","level":"error","event":"25/05/12 02:58:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4ad261c1f1d7, 45281, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:31.471787Z","level":"error","event":"25/05/12 02:58:31 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:31.525542Z","level":"error","event":"25/05/12 02:58:31 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:33.423431","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/monthly.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/monthly.py","lineno":27,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:58:33.473777Z","level":"error","event":"25/05/12 02:58:33 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:33.474291Z","level":"error","event":"25/05/12 02:58:33 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:33.474774Z","level":"error","event":"25/05/12 02:58:33 INFO SparkUI: Stopped Spark web UI at http://4ad261c1f1d7:4041","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:33.498580Z","level":"error","event":"25/05/12 02:58:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:33.516230Z","level":"error","event":"25/05/12 02:58:33 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:33.516773Z","level":"error","event":"25/05/12 02:58:33 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:33.538019Z","level":"error","event":"25/05/12 02:58:33 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:33.538595Z","level":"error","event":"25/05/12 02:58:33 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:33.558160Z","level":"error","event":"25/05/12 02:58:33 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:33.570333Z","level":"error","event":"25/05/12 02:58:33 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:33.570868Z","level":"error","event":"25/05/12 02:58:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-54d80065-a610-4aaa-aab6-40fcd22bf7c8","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:33.571355Z","level":"error","event":"25/05/12 02:58:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-54d80065-a610-4aaa-aab6-40fcd22bf7c8/pyspark-e29db7c5-4335-4d54-bf67-3cb139c65c9d","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:33.571891Z","level":"error","event":"25/05/12 02:58:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-08344a34-df57-49e0-ad72-53570df1442c","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:04.180755","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/monthly.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:59:16.257114Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:59:16.257913Z","level":"info","event":"ERROR: Gagal mengambil data AADI.JK: you are over your space quota, using 512 MB of 512 MB, full error: {'ok': 0, 'errmsg': 'you are over your space quota, using 512 MB of 512 MB', 'code': 8000, 'codeName': 'AtlasError'}","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:59:16.258827Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:59:16.259438Z","level":"info","event":"ERROR: Gagal mengambil data AALI.JK: you are over your space quota, using 512 MB of 512 MB, full error: {'ok': 0, 'errmsg': 'you are over your space quota, using 512 MB of 512 MB', 'code': 8000, 'codeName': 'AtlasError'}","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:59:16.259916Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:59:16.260314Z","level":"info","event":"ERROR: Gagal mengambil data ABBA.JK: you are over your space quota, using 512 MB of 512 MB, full error: {'ok': 0, 'errmsg': 'you are over your space quota, using 512 MB of 512 MB', 'code': 8000, 'codeName': 'AtlasError'}","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:59:16.260987Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:59:16.261507Z","level":"info","event":"ERROR: Gagal mengambil data ABDA.JK: you are over your space quota, using 512 MB of 512 MB, full error: {'ok': 0, 'errmsg': 'you are over your space quota, using 512 MB of 512 MB', 'code': 8000, 'codeName': 'AtlasError'}","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:59:16.261834Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:59:16.262121Z","level":"info","event":"ERROR: Gagal mengambil data ABMM.JK: you are over your space quota, using 512 MB of 512 MB, full error: {'ok': 0, 'errmsg': 'you are over your space quota, using 512 MB of 512 MB', 'code': 8000, 'codeName': 'AtlasError'}","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:59:16.262449Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:59:21.273669Z","level":"error","event":"25/05/12 02:59:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:21.782734Z","level":"error","event":"25/05/12 02:59:21 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:21.783380Z","level":"error","event":"25/05/12 02:59:21 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:21.784047Z","level":"error","event":"25/05/12 02:59:21 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:21.784581Z","level":"error","event":"25/05/12 02:59:21 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:22.100301Z","level":"error","event":"25/05/12 02:59:22 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:22.113684Z","level":"error","event":"25/05/12 02:59:22 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:22.114622Z","level":"error","event":"25/05/12 02:59:22 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:22.191467Z","level":"error","event":"25/05/12 02:59:22 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:22.206439Z","level":"error","event":"25/05/12 02:59:22 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:22.207425Z","level":"error","event":"25/05/12 02:59:22 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:22.208454Z","level":"error","event":"25/05/12 02:59:22 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:22.251086Z","level":"error","event":"25/05/12 02:59:22 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:22.273197Z","level":"error","event":"25/05/12 02:59:22 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:22.287633Z","level":"error","event":"25/05/12 02:59:22 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:22.404095Z","level":"error","event":"25/05/12 02:59:22 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:22.420214Z","level":"error","event":"25/05/12 02:59:22 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:22.420967Z","level":"error","event":"25/05/12 02:59:22 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:22.424811Z","level":"error","event":"25/05/12 02:59:22 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:22.425727Z","level":"error","event":"25/05/12 02:59:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.169990Z","level":"error","event":"25/05/12 02:59:23 INFO Utils: Successfully started service 'sparkDriver' on port 40493.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.267438Z","level":"error","event":"25/05/12 02:59:23 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.377070Z","level":"error","event":"25/05/12 02:59:23 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.453779Z","level":"error","event":"25/05/12 02:59:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.478737Z","level":"error","event":"25/05/12 02:59:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.484181Z","level":"error","event":"25/05/12 02:59:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.539012Z","level":"error","event":"25/05/12 02:59:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1d185a53-4bec-4c69-90a0-0a7a9f2b2f31","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.581157Z","level":"error","event":"25/05/12 02:59:23 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.733003Z","level":"error","event":"25/05/12 02:59:23 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.011685Z","level":"error","event":"25/05/12 02:59:24 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.205592Z","level":"error","event":"25/05/12 02:59:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.274470Z","level":"error","event":"25/05/12 02:59:24 INFO Utils: Successfully started service 'SparkUI' on port 4041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.378962Z","level":"error","event":"25/05/12 02:59:24 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.379785Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.380434Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.381212Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.381944Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.382642Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.383233Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.383973Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.384726Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.385503Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.386340Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.387169Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.388143Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.389324Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.390125Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.390814Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.391574Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.392285Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.392909Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.393621Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.394414Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.395264Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.396438Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.397481Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.415275Z","level":"error","event":"25/05/12 02:59:24 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.416237Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.417149Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.418028Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.424261Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.425090Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.426013Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.434803Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.435834Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.436569Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.437300Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.438786Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.441038Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.441843Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.444257Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.446664Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.447314Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.451694Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.456811Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.459270Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.463752Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.465048Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.465925Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.467947Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.469727Z","level":"error","event":"25/05/12 02:59:24 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.471951Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.472706Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.473409Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.474134Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.474832Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.475632Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.476247Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.476972Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.477688Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.479264Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.483206Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.484922Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.489823Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.490853Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.491660Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.492487Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.493352Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.494141Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.495178Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.496223Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.498571Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.499788Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.501728Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.503657Z","level":"error","event":"25/05/12 02:59:24 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.504878Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.506066Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.528834Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.530991Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.532093Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.532821Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.533756Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.534698Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.535396Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.536107Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.536869Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.537489Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.538190Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.538935Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.539635Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.540301Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.541116Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.544489Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.546312Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.547044Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.547760Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.548370Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.548941Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.628465Z","level":"error","event":"25/05/12 02:59:24 INFO Executor: Starting executor ID driver on host 4ad261c1f1d7","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.642951Z","level":"error","event":"25/05/12 02:59:24 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.643972Z","level":"error","event":"25/05/12 02:59:24 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.661456Z","level":"error","event":"25/05/12 02:59:24 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.675205Z","level":"error","event":"25/05/12 02:59:24 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@4c3682a6 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.714011Z","level":"error","event":"25/05/12 02:59:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37041.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.727351Z","level":"error","event":"25/05/12 02:59:24 INFO NettyBlockTransferService: Server created on 4ad261c1f1d7:37041","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.728079Z","level":"error","event":"25/05/12 02:59:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.743384Z","level":"error","event":"25/05/12 02:59:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4ad261c1f1d7, 37041, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.744237Z","level":"error","event":"25/05/12 02:59:24 INFO BlockManagerMasterEndpoint: Registering block manager 4ad261c1f1d7:37041 with 434.4 MiB RAM, BlockManagerId(driver, 4ad261c1f1d7, 37041, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.758049Z","level":"error","event":"25/05/12 02:59:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4ad261c1f1d7, 37041, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:24.759014Z","level":"error","event":"25/05/12 02:59:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4ad261c1f1d7, 37041, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:25.964787Z","level":"error","event":"25/05/12 02:59:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:26.003983Z","level":"error","event":"25/05/12 02:59:25 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:29.347777","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/monthly.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/monthly.py","lineno":27,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:59:29.397214Z","level":"error","event":"25/05/12 02:59:29 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:29.409794Z","level":"error","event":"25/05/12 02:59:29 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:29.424987Z","level":"error","event":"25/05/12 02:59:29 INFO SparkUI: Stopped Spark web UI at http://4ad261c1f1d7:4041","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:29.455609Z","level":"error","event":"25/05/12 02:59:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:29.546157Z","level":"error","event":"25/05/12 02:59:29 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:29.551837Z","level":"error","event":"25/05/12 02:59:29 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:29.554295Z","level":"error","event":"25/05/12 02:59:29 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:29.555040Z","level":"error","event":"25/05/12 02:59:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:29.606401Z","level":"error","event":"25/05/12 02:59:29 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:29.607075Z","level":"error","event":"25/05/12 02:59:29 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:29.607670Z","level":"error","event":"25/05/12 02:59:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-8f552525-c3ff-4ec0-8834-51dd508e782e","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:29.608272Z","level":"error","event":"25/05/12 02:59:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-d7606b08-d6f6-4c71-864c-f9f32c9b1e20","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:29.608956Z","level":"error","event":"25/05/12 02:59:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-8f552525-c3ff-4ec0-8834-51dd508e782e/pyspark-0bf1319d-7a07-446f-a804-9ad79fc69a5b","chan":"stderr","logger":"processor"}
