{"timestamp":"2025-05-12T11:07:22.910286","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-05-12T11:07:22.911380","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/yfinance_daily.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T11:07:46.630679Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:07:46.631891Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:07:46.644594Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:07:46.645080Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:07:46.645477Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:07:46.645868Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:07:46.646387Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:07:46.647069Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:07:46.647614Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:07:46.648025Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:07:46.648358Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"task"}
{"timestamp":"2025-05-12T11:07:50.458161Z","level":"error","event":"25/05/12 11:07:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:50.829701Z","level":"error","event":"25/05/12 11:07:50 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:50.830867Z","level":"error","event":"25/05/12 11:07:50 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:50.831807Z","level":"error","event":"25/05/12 11:07:50 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:50.832775Z","level":"error","event":"25/05/12 11:07:50 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:51.010790Z","level":"error","event":"25/05/12 11:07:51 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:51.011841Z","level":"error","event":"25/05/12 11:07:51 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:51.012904Z","level":"error","event":"25/05/12 11:07:51 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:51.062312Z","level":"error","event":"25/05/12 11:07:51 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:51.062971Z","level":"error","event":"25/05/12 11:07:51 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:51.063650Z","level":"error","event":"25/05/12 11:07:51 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:51.064614Z","level":"error","event":"25/05/12 11:07:51 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:51.095835Z","level":"error","event":"25/05/12 11:07:51 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:51.113584Z","level":"error","event":"25/05/12 11:07:51 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:51.114915Z","level":"error","event":"25/05/12 11:07:51 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:51.177717Z","level":"error","event":"25/05/12 11:07:51 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:51.209274Z","level":"error","event":"25/05/12 11:07:51 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:51.210009Z","level":"error","event":"25/05/12 11:07:51 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:51.210574Z","level":"error","event":"25/05/12 11:07:51 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:51.211143Z","level":"error","event":"25/05/12 11:07:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:51.534256Z","level":"error","event":"25/05/12 11:07:51 INFO Utils: Successfully started service 'sparkDriver' on port 34021.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:51.596761Z","level":"error","event":"25/05/12 11:07:51 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:51.656382Z","level":"error","event":"25/05/12 11:07:51 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:51.680691Z","level":"error","event":"25/05/12 11:07:51 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:51.682428Z","level":"error","event":"25/05/12 11:07:51 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:51.689683Z","level":"error","event":"25/05/12 11:07:51 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:51.753399Z","level":"error","event":"25/05/12 11:07:51 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-bf2b9d3b-f033-4574-bbcf-e0fe13013cc4","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:51.777179Z","level":"error","event":"25/05/12 11:07:51 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:51.800886Z","level":"error","event":"25/05/12 11:07:51 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:51.991343Z","level":"error","event":"25/05/12 11:07:51 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.075632Z","level":"error","event":"25/05/12 11:07:52 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.143014Z","level":"error","event":"25/05/12 11:07:52 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.143711Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.144306Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.144746Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.145247Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.145596Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.145972Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.146389Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.146735Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.147112Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.147440Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.147800Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.148303Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.148854Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.149453Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.150110Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.150617Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.151130Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.151573Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.152018Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.152359Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.152870Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.153334Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.153795Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.154378Z","level":"error","event":"25/05/12 11:07:52 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.154757Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.155157Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.155620Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.156219Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.156713Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.157126Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.157704Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.158559Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.159477Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.160039Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.160689Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.161549Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.162140Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.162648Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.163107Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.163549Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.164036Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.164563Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.165064Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.165569Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.166033Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.166380Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.166882Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.167234Z","level":"error","event":"25/05/12 11:07:52 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.167577Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.167950Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.168314Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.168645Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.169067Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.169428Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.169870Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.170418Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.170940Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.171608Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.172274Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.173204Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.174627Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.175495Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.176462Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.177331Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.177978Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.178531Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.179137Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.179662Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.180238Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.180943Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.181560Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.182159Z","level":"error","event":"25/05/12 11:07:52 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.182774Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.183296Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.184043Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.184827Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.185537Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.186208Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.186898Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.187443Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.188336Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.189752Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.190490Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.191333Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.191925Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.192586Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.193160Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.193745Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.194288Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.194966Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.195525Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.196021Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.196536Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.197144Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.197648Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.240002Z","level":"error","event":"25/05/12 11:07:52 INFO Executor: Starting executor ID driver on host bedd0e5c3c72","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.240644Z","level":"error","event":"25/05/12 11:07:52 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.241136Z","level":"error","event":"25/05/12 11:07:52 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.248959Z","level":"error","event":"25/05/12 11:07:52 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.250327Z","level":"error","event":"25/05/12 11:07:52 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@240f8e49 for default.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.276951Z","level":"error","event":"25/05/12 11:07:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39009.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.277595Z","level":"error","event":"25/05/12 11:07:52 INFO NettyBlockTransferService: Server created on bedd0e5c3c72:39009","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.280168Z","level":"error","event":"25/05/12 11:07:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.285985Z","level":"error","event":"25/05/12 11:07:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, bedd0e5c3c72, 39009, None)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.290249Z","level":"error","event":"25/05/12 11:07:52 INFO BlockManagerMasterEndpoint: Registering block manager bedd0e5c3c72:39009 with 434.4 MiB RAM, BlockManagerId(driver, bedd0e5c3c72, 39009, None)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.293292Z","level":"error","event":"25/05/12 11:07:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, bedd0e5c3c72, 39009, None)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.295066Z","level":"error","event":"25/05/12 11:07:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, bedd0e5c3c72, 39009, None)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.792901Z","level":"error","event":"25/05/12 11:07:52 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:52.796084Z","level":"error","event":"25/05/12 11:07:52 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.419790Z","level":"error","event":"Traceback (most recent call last):","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.420380Z","level":"error","event":"  File \"/opt/airflow/jobs/YFinance/daily.py\", line 27, in <module>","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.423913Z","level":"error","event":"    .load().withColumn(\"date\", to_date(col(\"Date\")))","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.424553Z","level":"error","event":"     ^^^^^^","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.425031Z","level":"error","event":"  File \"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py\", line 314, in load","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.425428Z","level":"error","event":"    return self._df(self._jreader.load())","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.425907Z","level":"error","event":"                    ^^^^^^^^^^^^^^^^^^^^","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.426444Z","level":"error","event":"  File \"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1322, in __call__","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.427032Z","level":"error","event":"    return_value = get_return_value(","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.427586Z","level":"error","event":"                   ^^^^^^^^^^^^^^^^^","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.428105Z","level":"error","event":"  File \"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.428590Z","level":"error","event":"    return f(*a, **kw)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.429072Z","level":"error","event":"           ^^^^^^^^^^^","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.429522Z","level":"error","event":"  File \"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py\", line 326, in get_return_value","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.429990Z","level":"error","event":"    raise Py4JJavaError(","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.430541Z","level":"error","event":"py4j.protocol.Py4JJavaError: An error occurred while calling o34.load.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.430982Z","level":"error","event":": org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.431443Z","level":"error","event":"\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.431840Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.432269Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.432703Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.433145Z","level":"error","event":"\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.433638Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.434161Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.434653Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.435182Z","level":"error","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.435822Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.436369Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.437001Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:282)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.437586Z","level":"error","event":"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.438272Z","level":"error","event":"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.438941Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.439589Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.440260Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.440884Z","level":"error","event":"Caused by: java.lang.ClassNotFoundException: mongo.DefaultSource","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.441580Z","level":"error","event":"\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.442278Z","level":"error","event":"\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.442857Z","level":"error","event":"\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.443453Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.444072Z","level":"error","event":"\tat scala.util.Try$.apply(Try.scala:213)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.444721Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.445804Z","level":"error","event":"\tat scala.util.Failure.orElse(Try.scala:224)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.446458Z","level":"error","event":"\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.446939Z","level":"error","event":"\t... 15 more","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.447361Z","level":"error","event":"","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.509375","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"CalledProcessError","exc_value":"Command '['python', 'jobs/YFinance/daily.py']' returned non-zero exit status 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":825,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1088,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":408,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":212,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/standard/operators/python.py","lineno":235,"name":"execute_callable"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/callback_runner.py","lineno":81,"name":"run"},{"filename":"/opt/airflow/dags/yfinance_daily.py","lineno":7,"name":"collect_data"},{"filename":"/usr/local/lib/python3.12/subprocess.py","lineno":571,"name":"run"}]}]}
{"timestamp":"2025-05-12T11:07:54.518138Z","level":"error","event":"25/05/12 11:07:54 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.519484Z","level":"error","event":"25/05/12 11:07:54 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.535943Z","level":"error","event":"25/05/12 11:07:54 INFO SparkUI: Stopped Spark web UI at http://bedd0e5c3c72:4040","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.553004Z","level":"error","event":"25/05/12 11:07:54 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.572423Z","level":"error","event":"25/05/12 11:07:54 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.573721Z","level":"error","event":"25/05/12 11:07:54 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.580584Z","level":"error","event":"25/05/12 11:07:54 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.585031Z","level":"error","event":"25/05/12 11:07:54 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.601444Z","level":"error","event":"25/05/12 11:07:54 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.602690Z","level":"error","event":"25/05/12 11:07:54 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.604660Z","level":"error","event":"25/05/12 11:07:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-d2729ce7-fd0e-44af-95db-74585c78518e","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.613552Z","level":"error","event":"25/05/12 11:07:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-1cdf1875-ce1b-4431-ae16-b6b3b19165f8","chan":"stderr","logger":"task"}
{"timestamp":"2025-05-12T11:07:54.620829Z","level":"error","event":"25/05/12 11:07:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-d2729ce7-fd0e-44af-95db-74585c78518e/pyspark-f56f3a89-819e-4d13-971d-ffa3d2de4a23","chan":"stderr","logger":"task"}
