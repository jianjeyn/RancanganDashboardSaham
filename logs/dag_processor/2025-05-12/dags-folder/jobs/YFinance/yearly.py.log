{"timestamp":"2025-05-12T02:54:17.105080","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/yearly.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:54:48.792879Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:54:48.793858Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:54:48.798318Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:54:48.800417Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:54:48.801119Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:54:48.808791Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:54:48.809560Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:54:48.810173Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:54:48.810815Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:54:48.818855Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:54:48.819579Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:54:49.260397","level":"error","event":"Process timed out, PID: 37","logger":"airflow.utils.timeout.TimeoutPosix"}
{"timestamp":"2025-05-12T02:54:49.281577","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/yearly.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"AirflowTaskTimeout","exc_value":"DagBag import timeout for /opt/airflow/dags/jobs/YFinance/yearly.py after 30.0s.\nPlease take a look at these docs to improve your DAG import time:\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#top-level-python-code\n* https://airflow.apache.org/docs/apache-airflow/3.0.0/best-practices.html#reducing-dag-complexity, PID: 37","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/yearly.py","lineno":12,"name":"<module>"},{"filename":"/usr/local/lib/python3.12/subprocess.py","lineno":550,"name":"run"},{"filename":"/usr/local/lib/python3.12/subprocess.py","lineno":1201,"name":"communicate"},{"filename":"/usr/local/lib/python3.12/subprocess.py","lineno":1264,"name":"wait"},{"filename":"/usr/local/lib/python3.12/subprocess.py","lineno":2053,"name":"_wait"},{"filename":"/usr/local/lib/python3.12/subprocess.py","lineno":2011,"name":"_try_wait"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py","lineno":69,"name":"handle_timeout"}]}]}
{"timestamp":"2025-05-12T02:55:19.758492","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/yearly.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:55:37.756788Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:55:37.757544Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:55:37.758284Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:55:37.759091Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:55:37.759939Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:55:37.760512Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:55:37.761107Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:55:37.761577Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:55:37.761988Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:55:37.765222Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:55:37.767818Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:55:43.273294Z","level":"error","event":"25/05/12 02:55:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:43.569976Z","level":"error","event":"25/05/12 02:55:43 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:43.580946Z","level":"error","event":"25/05/12 02:55:43 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:43.581749Z","level":"error","event":"25/05/12 02:55:43 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:43.582570Z","level":"error","event":"25/05/12 02:55:43 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:43.821723Z","level":"error","event":"25/05/12 02:55:43 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:43.832352Z","level":"error","event":"25/05/12 02:55:43 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:43.833113Z","level":"error","event":"25/05/12 02:55:43 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:43.891255Z","level":"error","event":"25/05/12 02:55:43 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:43.904711Z","level":"error","event":"25/05/12 02:55:43 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:43.907109Z","level":"error","event":"25/05/12 02:55:43 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:43.908585Z","level":"error","event":"25/05/12 02:55:43 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:43.961352Z","level":"error","event":"25/05/12 02:55:43 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:43.980915Z","level":"error","event":"25/05/12 02:55:43 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:43.992109Z","level":"error","event":"25/05/12 02:55:43 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:44.074579Z","level":"error","event":"25/05/12 02:55:44 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:44.085333Z","level":"error","event":"25/05/12 02:55:44 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:44.087169Z","level":"error","event":"25/05/12 02:55:44 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:44.087998Z","level":"error","event":"25/05/12 02:55:44 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:44.088645Z","level":"error","event":"25/05/12 02:55:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:44.538238Z","level":"error","event":"25/05/12 02:55:44 INFO Utils: Successfully started service 'sparkDriver' on port 36855.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:44.588476Z","level":"error","event":"25/05/12 02:55:44 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:44.646014Z","level":"error","event":"25/05/12 02:55:44 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:44.681786Z","level":"error","event":"25/05/12 02:55:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:44.695503Z","level":"error","event":"25/05/12 02:55:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:44.696609Z","level":"error","event":"25/05/12 02:55:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:44.741797Z","level":"error","event":"25/05/12 02:55:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1419efd8-c8bb-4276-b54b-7613eb1e3698","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:44.793034Z","level":"error","event":"25/05/12 02:55:44 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:44.815227Z","level":"error","event":"25/05/12 02:55:44 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:44.978155Z","level":"error","event":"25/05/12 02:55:44 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.075718Z","level":"error","event":"25/05/12 02:55:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.160350Z","level":"error","event":"25/05/12 02:55:45 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.161951Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.163168Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.165730Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.167367Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.169237Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.169986Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.170646Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.171135Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.172470Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.174940Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.176698Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.177456Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.178165Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.178913Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.179644Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.181472Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.182618Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.183270Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.183918Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.184509Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.185169Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.186667Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.187490Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.208654Z","level":"error","event":"25/05/12 02:55:45 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.209694Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.210680Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.212532Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.214028Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.215733Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.216639Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.217520Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.219928Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.222888Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.231058Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.237688Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.239521Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.240469Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.241498Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.242917Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.244312Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.245146Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.246032Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.246733Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.247682Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.248712Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.249611Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.250556Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.251605Z","level":"error","event":"25/05/12 02:55:45 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.252406Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.253013Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.253595Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.254227Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.254872Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.255738Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.256559Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.257737Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.258748Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.259888Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.260962Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.261709Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.262327Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.262968Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.263650Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.264372Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.265033Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.265759Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.266968Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.267856Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.268659Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.269414Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.270114Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.270880Z","level":"error","event":"25/05/12 02:55:45 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.271591Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.272253Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.287970Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.289213Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.290349Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.291646Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.294697Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.296208Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.300138Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.301429Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.302189Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.302888Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.303577Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.304644Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.305326Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.306706Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.308344Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.309271Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.310195Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.310984Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.311700Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.312467Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.314011Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.347128Z","level":"error","event":"25/05/12 02:55:45 INFO Executor: Starting executor ID driver on host 4ad261c1f1d7","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.356716Z","level":"error","event":"25/05/12 02:55:45 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.357300Z","level":"error","event":"25/05/12 02:55:45 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.369902Z","level":"error","event":"25/05/12 02:55:45 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.370591Z","level":"error","event":"25/05/12 02:55:45 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@73663445 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.402514Z","level":"error","event":"25/05/12 02:55:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34683.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.410520Z","level":"error","event":"25/05/12 02:55:45 INFO NettyBlockTransferService: Server created on 4ad261c1f1d7:34683","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.411035Z","level":"error","event":"25/05/12 02:55:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.418077Z","level":"error","event":"25/05/12 02:55:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4ad261c1f1d7, 34683, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.427031Z","level":"error","event":"25/05/12 02:55:45 INFO BlockManagerMasterEndpoint: Registering block manager 4ad261c1f1d7:34683 with 434.4 MiB RAM, BlockManagerId(driver, 4ad261c1f1d7, 34683, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.427669Z","level":"error","event":"25/05/12 02:55:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4ad261c1f1d7, 34683, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.428189Z","level":"error","event":"25/05/12 02:55:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4ad261c1f1d7, 34683, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.950208Z","level":"error","event":"25/05/12 02:55:45 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:45.980695Z","level":"error","event":"25/05/12 02:55:45 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:47.468146","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/yearly.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/yearly.py","lineno":27,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:55:47.493620Z","level":"error","event":"25/05/12 02:55:47 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:47.494153Z","level":"error","event":"25/05/12 02:55:47 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:47.506763Z","level":"error","event":"25/05/12 02:55:47 INFO SparkUI: Stopped Spark web UI at http://4ad261c1f1d7:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:47.522620Z","level":"error","event":"25/05/12 02:55:47 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:47.543573Z","level":"error","event":"25/05/12 02:55:47 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:47.552169Z","level":"error","event":"25/05/12 02:55:47 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:47.561629Z","level":"error","event":"25/05/12 02:55:47 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:47.562367Z","level":"error","event":"25/05/12 02:55:47 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:47.577006Z","level":"error","event":"25/05/12 02:55:47 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:47.588083Z","level":"error","event":"25/05/12 02:55:47 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:47.588540Z","level":"error","event":"25/05/12 02:55:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-8a074554-2c32-4f31-879d-c3fedf874e08","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:47.588970Z","level":"error","event":"25/05/12 02:55:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-6df25c45-f5f9-48d1-86a4-6c5981fa3cb6","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:55:47.595648Z","level":"error","event":"25/05/12 02:55:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-8a074554-2c32-4f31-879d-c3fedf874e08/pyspark-ddf4b7b7-c1ac-4d7d-a71a-eec91e0b59f7","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:18.705424","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/yearly.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:56:34.519822Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:56:34.520393Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:56:34.520880Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:56:34.521300Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:56:34.521700Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:56:34.521960Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:56:34.522230Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:56:34.522480Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:56:34.523049Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:56:34.523335Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:56:34.523588Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:56:38.075252Z","level":"error","event":"25/05/12 02:56:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:38.438078Z","level":"error","event":"25/05/12 02:56:38 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:38.449316Z","level":"error","event":"25/05/12 02:56:38 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:38.450139Z","level":"error","event":"25/05/12 02:56:38 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:38.450799Z","level":"error","event":"25/05/12 02:56:38 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:38.657487Z","level":"error","event":"25/05/12 02:56:38 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:38.678670Z","level":"error","event":"25/05/12 02:56:38 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:38.679380Z","level":"error","event":"25/05/12 02:56:38 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:38.707606Z","level":"error","event":"25/05/12 02:56:38 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:38.715466Z","level":"error","event":"25/05/12 02:56:38 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:38.716075Z","level":"error","event":"25/05/12 02:56:38 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:38.716609Z","level":"error","event":"25/05/12 02:56:38 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:38.739291Z","level":"error","event":"25/05/12 02:56:38 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:38.752082Z","level":"error","event":"25/05/12 02:56:38 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:38.760770Z","level":"error","event":"25/05/12 02:56:38 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:38.818346Z","level":"error","event":"25/05/12 02:56:38 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:38.827821Z","level":"error","event":"25/05/12 02:56:38 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:38.828368Z","level":"error","event":"25/05/12 02:56:38 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:38.828807Z","level":"error","event":"25/05/12 02:56:38 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:38.829236Z","level":"error","event":"25/05/12 02:56:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.153460Z","level":"error","event":"25/05/12 02:56:39 INFO Utils: Successfully started service 'sparkDriver' on port 36827.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.188381Z","level":"error","event":"25/05/12 02:56:39 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.234571Z","level":"error","event":"25/05/12 02:56:39 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.258251Z","level":"error","event":"25/05/12 02:56:39 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.268648Z","level":"error","event":"25/05/12 02:56:39 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.269298Z","level":"error","event":"25/05/12 02:56:39 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.299179Z","level":"error","event":"25/05/12 02:56:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c5a93cc7-3be8-49a5-a922-0795e4f7f481","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.317609Z","level":"error","event":"25/05/12 02:56:39 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.344479Z","level":"error","event":"25/05/12 02:56:39 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.583850Z","level":"error","event":"25/05/12 02:56:39 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.694532Z","level":"error","event":"25/05/12 02:56:39 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.770394Z","level":"error","event":"25/05/12 02:56:39 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.771046Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.771561Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.772049Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.772531Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.772985Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.773460Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.774028Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.774694Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.775236Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.775870Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.776521Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.777203Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.777868Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.778474Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.779039Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.779694Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.780229Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.780704Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.781118Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.781502Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.781976Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.782421Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.782974Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.793971Z","level":"error","event":"25/05/12 02:56:39 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.794623Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.795489Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.796093Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.796724Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.797295Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.797939Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.798495Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.799354Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.800234Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.801214Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.802037Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.802748Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.803207Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.803556Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.803881Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.804308Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.804914Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.805366Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.805889Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.806458Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.807130Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.807955Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.808661Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.809453Z","level":"error","event":"25/05/12 02:56:39 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.810273Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.811095Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.811953Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.812491Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.813069Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.813581Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.814028Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.814696Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.815617Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.816515Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.817679Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.818477Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.819077Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.819795Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.820429Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.820918Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.821431Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.822080Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.823134Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.823864Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.829771Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.830706Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.831634Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.832749Z","level":"error","event":"25/05/12 02:56:39 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.833630Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.835071Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.843971Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.844625Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.845159Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.845789Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.846557Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.847287Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.848085Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.848890Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.849413Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.849875Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.850358Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.850912Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.851648Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.852321Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.852876Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.853429Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.854001Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.854583Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.855437Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.856248Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.856969Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.888908Z","level":"error","event":"25/05/12 02:56:39 INFO Executor: Starting executor ID driver on host 4ad261c1f1d7","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.897532Z","level":"error","event":"25/05/12 02:56:39 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.898183Z","level":"error","event":"25/05/12 02:56:39 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.905380Z","level":"error","event":"25/05/12 02:56:39 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.905826Z","level":"error","event":"25/05/12 02:56:39 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6ff21454 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.932360Z","level":"error","event":"25/05/12 02:56:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42769.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.940960Z","level":"error","event":"25/05/12 02:56:39 INFO NettyBlockTransferService: Server created on 4ad261c1f1d7:42769","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.941494Z","level":"error","event":"25/05/12 02:56:39 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.954346Z","level":"error","event":"25/05/12 02:56:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4ad261c1f1d7, 42769, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.955165Z","level":"error","event":"25/05/12 02:56:39 INFO BlockManagerMasterEndpoint: Registering block manager 4ad261c1f1d7:42769 with 434.4 MiB RAM, BlockManagerId(driver, 4ad261c1f1d7, 42769, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.955941Z","level":"error","event":"25/05/12 02:56:39 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4ad261c1f1d7, 42769, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:39.956679Z","level":"error","event":"25/05/12 02:56:39 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4ad261c1f1d7, 42769, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:40.440465Z","level":"error","event":"25/05/12 02:56:40 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:40.451221Z","level":"error","event":"25/05/12 02:56:40 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:42.817510","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/yearly.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/yearly.py","lineno":27,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:56:42.883806Z","level":"error","event":"25/05/12 02:56:42 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:42.897579Z","level":"error","event":"25/05/12 02:56:42 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:42.930314Z","level":"error","event":"25/05/12 02:56:42 INFO SparkUI: Stopped Spark web UI at http://4ad261c1f1d7:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:43.014220Z","level":"error","event":"25/05/12 02:56:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:43.077700Z","level":"error","event":"25/05/12 02:56:43 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:43.125976Z","level":"error","event":"25/05/12 02:56:43 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:43.137032Z","level":"error","event":"25/05/12 02:56:43 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:43.156162Z","level":"error","event":"25/05/12 02:56:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:43.182110Z","level":"error","event":"25/05/12 02:56:43 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:43.195060Z","level":"error","event":"25/05/12 02:56:43 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:43.195813Z","level":"error","event":"25/05/12 02:56:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-fd23a16e-da3c-41a2-bdef-caa1f60f5773","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:43.196446Z","level":"error","event":"25/05/12 02:56:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-1da4ad67-bbe5-4332-bb02-6d5360e6dc35/pyspark-1196bf75-b7e8-4944-93df-642e1b1e2bff","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:56:43.214516Z","level":"error","event":"25/05/12 02:56:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-1da4ad67-bbe5-4332-bb02-6d5360e6dc35","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:14.015851","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/yearly.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:57:26.957212Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:57:26.957904Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:57:26.958375Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:57:26.958829Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:57:26.959243Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:57:26.959666Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:57:26.960109Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:57:26.960563Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:57:26.960953Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:57:26.961352Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:57:26.961706Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:57:30.794227Z","level":"error","event":"25/05/12 02:57:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.065938Z","level":"error","event":"25/05/12 02:57:31 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.078986Z","level":"error","event":"25/05/12 02:57:31 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.079740Z","level":"error","event":"25/05/12 02:57:31 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.080415Z","level":"error","event":"25/05/12 02:57:31 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.258883Z","level":"error","event":"25/05/12 02:57:31 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.276466Z","level":"error","event":"25/05/12 02:57:31 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.277086Z","level":"error","event":"25/05/12 02:57:31 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.315174Z","level":"error","event":"25/05/12 02:57:31 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.327806Z","level":"error","event":"25/05/12 02:57:31 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.328593Z","level":"error","event":"25/05/12 02:57:31 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.329245Z","level":"error","event":"25/05/12 02:57:31 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.384276Z","level":"error","event":"25/05/12 02:57:31 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.398650Z","level":"error","event":"25/05/12 02:57:31 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.406849Z","level":"error","event":"25/05/12 02:57:31 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.468234Z","level":"error","event":"25/05/12 02:57:31 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.477638Z","level":"error","event":"25/05/12 02:57:31 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.478321Z","level":"error","event":"25/05/12 02:57:31 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.478860Z","level":"error","event":"25/05/12 02:57:31 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.479422Z","level":"error","event":"25/05/12 02:57:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.767790Z","level":"error","event":"25/05/12 02:57:31 INFO Utils: Successfully started service 'sparkDriver' on port 40107.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.805912Z","level":"error","event":"25/05/12 02:57:31 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.861569Z","level":"error","event":"25/05/12 02:57:31 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.893462Z","level":"error","event":"25/05/12 02:57:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.905826Z","level":"error","event":"25/05/12 02:57:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.906798Z","level":"error","event":"25/05/12 02:57:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.944333Z","level":"error","event":"25/05/12 02:57:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-12963da2-a380-4b93-b4f5-6df7fd23c625","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.963632Z","level":"error","event":"25/05/12 02:57:31 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:31.986874Z","level":"error","event":"25/05/12 02:57:31 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.139917Z","level":"error","event":"25/05/12 02:57:32 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.226719Z","level":"error","event":"25/05/12 02:57:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.273055Z","level":"error","event":"25/05/12 02:57:32 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.273979Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.274525Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.275069Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.275684Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.276188Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.276795Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.277405Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.277874Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.278381Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.278991Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.279584Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.280132Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.280688Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.281322Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.281931Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.282596Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.283133Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.283761Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.284403Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.284966Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.285434Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.285939Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.286447Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.297147Z","level":"error","event":"25/05/12 02:57:32 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.297753Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.298240Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.298747Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.299201Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.299724Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.300273Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.300895Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.301421Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.301956Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.303006Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.303772Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.304815Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.305668Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.306435Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.307321Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.308128Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.309107Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.309793Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.310424Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.311096Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.311648Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.312208Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.312698Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.313243Z","level":"error","event":"25/05/12 02:57:32 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.313769Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.314347Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.314998Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.315555Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.316118Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.316711Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.317361Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.317904Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.318397Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.318887Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.319316Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.320007Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.320734Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.321467Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.322131Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.322637Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.323104Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.323625Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.324086Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.324508Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.324932Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.325358Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.325776Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.326197Z","level":"error","event":"25/05/12 02:57:32 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.326631Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.327046Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.335941Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.336667Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.337212Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.337672Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.338099Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.338539Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.338949Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.339345Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.339722Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.340160Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.340575Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.340966Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.341347Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.341728Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.342110Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.342496Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.342899Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.343262Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.343627Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.343982Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.344324Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.371561Z","level":"error","event":"25/05/12 02:57:32 INFO Executor: Starting executor ID driver on host 4ad261c1f1d7","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.379309Z","level":"error","event":"25/05/12 02:57:32 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.379968Z","level":"error","event":"25/05/12 02:57:32 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.389754Z","level":"error","event":"25/05/12 02:57:32 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.390509Z","level":"error","event":"25/05/12 02:57:32 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6ff21454 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.416299Z","level":"error","event":"25/05/12 02:57:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36595.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.426167Z","level":"error","event":"25/05/12 02:57:32 INFO NettyBlockTransferService: Server created on 4ad261c1f1d7:36595","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.426925Z","level":"error","event":"25/05/12 02:57:32 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.435335Z","level":"error","event":"25/05/12 02:57:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4ad261c1f1d7, 36595, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.435963Z","level":"error","event":"25/05/12 02:57:32 INFO BlockManagerMasterEndpoint: Registering block manager 4ad261c1f1d7:36595 with 434.4 MiB RAM, BlockManagerId(driver, 4ad261c1f1d7, 36595, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.436490Z","level":"error","event":"25/05/12 02:57:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4ad261c1f1d7, 36595, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:32.444756Z","level":"error","event":"25/05/12 02:57:32 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4ad261c1f1d7, 36595, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:33.076586Z","level":"error","event":"25/05/12 02:57:33 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:33.085667Z","level":"error","event":"25/05/12 02:57:33 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:37.447504","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/yearly.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/yearly.py","lineno":27,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:57:37.488653Z","level":"error","event":"25/05/12 02:57:37 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:37.489371Z","level":"error","event":"25/05/12 02:57:37 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:37.512177Z","level":"error","event":"25/05/12 02:57:37 INFO SparkUI: Stopped Spark web UI at http://4ad261c1f1d7:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:37.538110Z","level":"error","event":"25/05/12 02:57:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:37.570880Z","level":"error","event":"25/05/12 02:57:37 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:37.586302Z","level":"error","event":"25/05/12 02:57:37 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:37.603404Z","level":"error","event":"25/05/12 02:57:37 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:37.604587Z","level":"error","event":"25/05/12 02:57:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:37.658754Z","level":"error","event":"25/05/12 02:57:37 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:37.669495Z","level":"error","event":"25/05/12 02:57:37 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:37.671417Z","level":"error","event":"25/05/12 02:57:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-e97ad3c6-03b3-47f8-96e1-9e97593dedcd","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:37.683549Z","level":"error","event":"25/05/12 02:57:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-e6cabd45-fe48-4c2b-ba85-1cf16fc58a11","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:57:37.695350Z","level":"error","event":"25/05/12 02:57:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-e6cabd45-fe48-4c2b-ba85-1cf16fc58a11/pyspark-d7cfa51b-0c3c-4a06-ac22-4259694a7626","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:08.370235","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/yearly.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:58:22.351583Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:58:22.352223Z","level":"info","event":"Data saham AADI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:58:22.352682Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:58:22.353186Z","level":"info","event":"Data saham AALI.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:58:22.353619Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:58:22.354035Z","level":"info","event":"Data saham ABBA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:58:22.354451Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:58:22.354921Z","level":"info","event":"Data saham ABDA.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:58:22.355326Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:58:22.355707Z","level":"info","event":"Data saham ABMM.JK berhasil disimpan ke MongoDB!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:58:22.356070Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:58:25.206401Z","level":"error","event":"25/05/12 02:58:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:25.605002Z","level":"error","event":"25/05/12 02:58:25 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:25.616768Z","level":"error","event":"25/05/12 02:58:25 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:25.617618Z","level":"error","event":"25/05/12 02:58:25 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:25.618477Z","level":"error","event":"25/05/12 02:58:25 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:25.866715Z","level":"error","event":"25/05/12 02:58:25 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:25.875367Z","level":"error","event":"25/05/12 02:58:25 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:25.875995Z","level":"error","event":"25/05/12 02:58:25 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:25.923375Z","level":"error","event":"25/05/12 02:58:25 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:25.932556Z","level":"error","event":"25/05/12 02:58:25 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:25.933229Z","level":"error","event":"25/05/12 02:58:25 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:25.933917Z","level":"error","event":"25/05/12 02:58:25 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:25.963124Z","level":"error","event":"25/05/12 02:58:25 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:25.981438Z","level":"error","event":"25/05/12 02:58:25 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:25.992182Z","level":"error","event":"25/05/12 02:58:25 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:26.075031Z","level":"error","event":"25/05/12 02:58:26 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:26.084993Z","level":"error","event":"25/05/12 02:58:26 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:26.085884Z","level":"error","event":"25/05/12 02:58:26 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:26.086899Z","level":"error","event":"25/05/12 02:58:26 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:26.087649Z","level":"error","event":"25/05/12 02:58:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:26.473686Z","level":"error","event":"25/05/12 02:58:26 INFO Utils: Successfully started service 'sparkDriver' on port 36437.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:26.532182Z","level":"error","event":"25/05/12 02:58:26 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:26.592291Z","level":"error","event":"25/05/12 02:58:26 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:26.625312Z","level":"error","event":"25/05/12 02:58:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:26.659404Z","level":"error","event":"25/05/12 02:58:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:26.660113Z","level":"error","event":"25/05/12 02:58:26 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:26.684156Z","level":"error","event":"25/05/12 02:58:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4ffa1c9d-f06a-4be1-b6df-510d768d1a42","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:26.706669Z","level":"error","event":"25/05/12 02:58:26 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:26.738632Z","level":"error","event":"25/05/12 02:58:26 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:26.988561Z","level":"error","event":"25/05/12 02:58:26 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.122768Z","level":"error","event":"25/05/12 02:58:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.200030Z","level":"error","event":"25/05/12 02:58:27 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.200921Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.201629Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.202473Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.203400Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.204276Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.205119Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.205893Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.206685Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.207310Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.208056Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.208835Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.209547Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.210247Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.211194Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.211972Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.212665Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.213307Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.213918Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.214541Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.215237Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.215949Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.216760Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.217532Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.230071Z","level":"error","event":"25/05/12 02:58:27 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.231058Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.231936Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.232734Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.233545Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.234394Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.235125Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.236234Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.236973Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.237960Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.238654Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.239358Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.240177Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.240944Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.241664Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.242354Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.243098Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.243867Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.244584Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.253673Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.254534Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.255344Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.256192Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.256993Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.257803Z","level":"error","event":"25/05/12 02:58:27 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.258542Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.259422Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.261886Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.262839Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.263673Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.264547Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.265425Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.266200Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.266881Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.267533Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.268222Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.268823Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.269527Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.270253Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.271057Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.271772Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.272502Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.273277Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.273958Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.274674Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.275374Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.276132Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.277121Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.277964Z","level":"error","event":"25/05/12 02:58:27 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.278682Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.279556Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.291783Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.292623Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.293350Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.296047Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.296826Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.297570Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.298305Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.298934Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.299626Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.300220Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.300753Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.301350Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.301985Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.302735Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.303511Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.304223Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.304865Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.305500Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.306137Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.306759Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.307325Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.356048Z","level":"error","event":"25/05/12 02:58:27 INFO Executor: Starting executor ID driver on host 4ad261c1f1d7","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.366220Z","level":"error","event":"25/05/12 02:58:27 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.367192Z","level":"error","event":"25/05/12 02:58:27 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.379185Z","level":"error","event":"25/05/12 02:58:27 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.379899Z","level":"error","event":"25/05/12 02:58:27 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2c9036a8 for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.416832Z","level":"error","event":"25/05/12 02:58:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36569.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.425985Z","level":"error","event":"25/05/12 02:58:27 INFO NettyBlockTransferService: Server created on 4ad261c1f1d7:36569","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.426653Z","level":"error","event":"25/05/12 02:58:27 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.437444Z","level":"error","event":"25/05/12 02:58:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4ad261c1f1d7, 36569, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.447514Z","level":"error","event":"25/05/12 02:58:27 INFO BlockManagerMasterEndpoint: Registering block manager 4ad261c1f1d7:36569 with 434.4 MiB RAM, BlockManagerId(driver, 4ad261c1f1d7, 36569, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.448521Z","level":"error","event":"25/05/12 02:58:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4ad261c1f1d7, 36569, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:27.449298Z","level":"error","event":"25/05/12 02:58:27 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4ad261c1f1d7, 36569, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:28.078698Z","level":"error","event":"25/05/12 02:58:28 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:28.086785Z","level":"error","event":"25/05/12 02:58:28 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.270708","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/yearly.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/yearly.py","lineno":27,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:58:30.299936Z","level":"error","event":"25/05/12 02:58:30 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.300510Z","level":"error","event":"25/05/12 02:58:30 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.316468Z","level":"error","event":"25/05/12 02:58:30 INFO SparkUI: Stopped Spark web UI at http://4ad261c1f1d7:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.406749Z","level":"error","event":"25/05/12 02:58:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.407463Z","level":"error","event":"25/05/12 02:58:30 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.408033Z","level":"error","event":"25/05/12 02:58:30 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.408548Z","level":"error","event":"25/05/12 02:58:30 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.409257Z","level":"error","event":"25/05/12 02:58:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.437980Z","level":"error","event":"25/05/12 02:58:30 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.438736Z","level":"error","event":"25/05/12 02:58:30 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.439457Z","level":"error","event":"25/05/12 02:58:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-5cf53c99-3959-468d-9d74-6b3de89cbcf2","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.440064Z","level":"error","event":"25/05/12 02:58:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-593eac87-fb7c-4f5c-8d58-6fcf6aed6547/pyspark-14de9377-63db-40f9-a546-374d25ea45b5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:58:30.450430Z","level":"error","event":"25/05/12 02:58:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-593eac87-fb7c-4f5c-8d58-6fcf6aed6547","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:00.993263","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/jobs/YFinance/yearly.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-12T02:59:16.059722Z","level":"info","event":"Mengambil data saham AADI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:59:16.060434Z","level":"info","event":"ERROR: Gagal mengambil data AADI.JK: you are over your space quota, using 512 MB of 512 MB, full error: {'ok': 0, 'errmsg': 'you are over your space quota, using 512 MB of 512 MB', 'code': 8000, 'codeName': 'AtlasError'}","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:59:16.060994Z","level":"info","event":"Mengambil data saham AALI.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:59:16.061437Z","level":"info","event":"ERROR: Gagal mengambil data AALI.JK: you are over your space quota, using 512 MB of 512 MB, full error: {'ok': 0, 'errmsg': 'you are over your space quota, using 512 MB of 512 MB', 'code': 8000, 'codeName': 'AtlasError'}","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:59:16.061893Z","level":"info","event":"Mengambil data saham ABBA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:59:16.062350Z","level":"info","event":"ERROR: Gagal mengambil data ABBA.JK: you are over your space quota, using 512 MB of 512 MB, full error: {'ok': 0, 'errmsg': 'you are over your space quota, using 512 MB of 512 MB', 'code': 8000, 'codeName': 'AtlasError'}","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:59:16.062728Z","level":"info","event":"Mengambil data saham ABDA.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:59:16.063111Z","level":"info","event":"ERROR: Gagal mengambil data ABDA.JK: you are over your space quota, using 512 MB of 512 MB, full error: {'ok': 0, 'errmsg': 'you are over your space quota, using 512 MB of 512 MB', 'code': 8000, 'codeName': 'AtlasError'}","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:59:16.063505Z","level":"info","event":"Mengambil data saham ABMM.JK dari yfinance...","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:59:16.063894Z","level":"info","event":"ERROR: Gagal mengambil data ABMM.JK: you are over your space quota, using 512 MB of 512 MB, full error: {'ok': 0, 'errmsg': 'you are over your space quota, using 512 MB of 512 MB', 'code': 8000, 'codeName': 'AtlasError'}","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:59:16.064262Z","level":"info","event":"Semua data saham selesai diproses!","chan":"stdout","logger":"processor"}
{"timestamp":"2025-05-12T02:59:20.397856Z","level":"error","event":"25/05/12 02:59:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:21.033660Z","level":"error","event":"25/05/12 02:59:21 WARN DependencyUtils: Local jar /opt/airflow/jars/mongo-spark-connector.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:21.047109Z","level":"error","event":"25/05/12 02:59:21 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-sync.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:21.047749Z","level":"error","event":"25/05/12 02:59:21 WARN DependencyUtils: Local jar /opt/airflow/jars/mongodb-driver-core.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:21.048461Z","level":"error","event":"25/05/12 02:59:21 WARN DependencyUtils: Local jar /opt/airflow/jars/bson.jar does not exist, skipping.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:21.397291Z","level":"error","event":"25/05/12 02:59:21 INFO SparkContext: Running Spark version 3.5.5","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:21.423489Z","level":"error","event":"25/05/12 02:59:21 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:21.425381Z","level":"error","event":"25/05/12 02:59:21 INFO SparkContext: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:21.508489Z","level":"error","event":"25/05/12 02:59:21 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:21.524823Z","level":"error","event":"25/05/12 02:59:21 INFO ResourceUtils: No custom resources configured for spark.driver.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:21.525907Z","level":"error","event":"25/05/12 02:59:21 INFO ResourceUtils: ==============================================================","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:21.526799Z","level":"error","event":"25/05/12 02:59:21 INFO SparkContext: Submitted application: ReadMongo","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:21.569157Z","level":"error","event":"25/05/12 02:59:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:21.590177Z","level":"error","event":"25/05/12 02:59:21 INFO ResourceProfile: Limiting resource is cpu","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:21.605259Z","level":"error","event":"25/05/12 02:59:21 INFO ResourceProfileManager: Added ResourceProfile id: 0","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:21.751701Z","level":"error","event":"25/05/12 02:59:21 INFO SecurityManager: Changing view acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:21.765546Z","level":"error","event":"25/05/12 02:59:21 INFO SecurityManager: Changing modify acls to: airflow","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:21.769186Z","level":"error","event":"25/05/12 02:59:21 INFO SecurityManager: Changing view acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:21.781347Z","level":"error","event":"25/05/12 02:59:21 INFO SecurityManager: Changing modify acls groups to:","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:21.782036Z","level":"error","event":"25/05/12 02:59:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:22.361243Z","level":"error","event":"25/05/12 02:59:22 INFO Utils: Successfully started service 'sparkDriver' on port 39047.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:22.436683Z","level":"error","event":"25/05/12 02:59:22 INFO SparkEnv: Registering MapOutputTracker","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:22.512109Z","level":"error","event":"25/05/12 02:59:22 INFO SparkEnv: Registering BlockManagerMaster","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:22.557088Z","level":"error","event":"25/05/12 02:59:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:22.573577Z","level":"error","event":"25/05/12 02:59:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:22.586141Z","level":"error","event":"25/05/12 02:59:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:22.634344Z","level":"error","event":"25/05/12 02:59:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c71a7d6a-ff9b-455b-ac02-4e6efa5e63f4","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:22.681909Z","level":"error","event":"25/05/12 02:59:22 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:22.754837Z","level":"error","event":"25/05/12 02:59:22 INFO SparkEnv: Registering OutputCommitCoordinator","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.216229Z","level":"error","event":"25/05/12 02:59:23 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.514091Z","level":"error","event":"25/05/12 02:59:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.607537Z","level":"error","event":"25/05/12 02:59:23 ERROR SparkContext: Failed to add /opt/airflow/jars/mongo-spark-connector.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.608319Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongo-spark-connector.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.609080Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.609779Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.610823Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.611479Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.612355Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.613208Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.614413Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.620529Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.621426Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.622279Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.623331Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.624149Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.624938Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.625715Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.626522Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.627418Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.628485Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.629315Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.630329Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.631249Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.632033Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.632839Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.648792Z","level":"error","event":"25/05/12 02:59:23 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-sync.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.650221Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-sync.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.652586Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.653796Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.657064Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.658375Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.661842Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.662892Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.663719Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.665161Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.667401Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.669309Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.670234Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.671465Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.675763Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.677182Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.678300Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.679121Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.689966Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.691582Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.693928Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.695532Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.696469Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.697499Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.698511Z","level":"error","event":"25/05/12 02:59:23 ERROR SparkContext: Failed to add /opt/airflow/jars/mongodb-driver-core.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.699337Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/mongodb-driver-core.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.708807Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.709846Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.710729Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.711647Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.712644Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.713530Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.714634Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.715456Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.716481Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.717391Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.718271Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.719099Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.720005Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.720875Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.721723Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.722438Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.723431Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.724276Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.725048Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.726015Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.727226Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.728547Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.729613Z","level":"error","event":"25/05/12 02:59:23 ERROR SparkContext: Failed to add /opt/airflow/jars/bson.jar to Spark environment","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.730850Z","level":"error","event":"java.io.FileNotFoundException: Jar /opt/airflow/jars/bson.jar not found","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.731965Z","level":"error","event":"\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2095)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.749282Z","level":"error","event":"\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2151)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.750158Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.751028Z","level":"error","event":"\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.751928Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.752720Z","level":"error","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.753490Z","level":"error","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.754468Z","level":"error","event":"\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:521)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.755248Z","level":"error","event":"\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.755946Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.761444Z","level":"error","event":"\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.762311Z","level":"error","event":"\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.763865Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.764773Z","level":"error","event":"\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.765736Z","level":"error","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.766730Z","level":"error","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.767546Z","level":"error","event":"\tat py4j.Gateway.invoke(Gateway.java:238)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.768298Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.769111Z","level":"error","event":"\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.769862Z","level":"error","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.771074Z","level":"error","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.771878Z","level":"error","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.814162Z","level":"error","event":"25/05/12 02:59:23 INFO Executor: Starting executor ID driver on host 4ad261c1f1d7","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.828730Z","level":"error","event":"25/05/12 02:59:23 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.829684Z","level":"error","event":"25/05/12 02:59:23 INFO Executor: Java version 17.0.15","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.846182Z","level":"error","event":"25/05/12 02:59:23 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.846929Z","level":"error","event":"25/05/12 02:59:23 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@5c48057c for default.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.891245Z","level":"error","event":"25/05/12 02:59:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41255.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.908677Z","level":"error","event":"25/05/12 02:59:23 INFO NettyBlockTransferService: Server created on 4ad261c1f1d7:41255","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.909482Z","level":"error","event":"25/05/12 02:59:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.930091Z","level":"error","event":"25/05/12 02:59:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4ad261c1f1d7, 41255, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.931569Z","level":"error","event":"25/05/12 02:59:23 INFO BlockManagerMasterEndpoint: Registering block manager 4ad261c1f1d7:41255 with 434.4 MiB RAM, BlockManagerId(driver, 4ad261c1f1d7, 41255, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.950445Z","level":"error","event":"25/05/12 02:59:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4ad261c1f1d7, 41255, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:23.952350Z","level":"error","event":"25/05/12 02:59:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4ad261c1f1d7, 41255, None)","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:25.124338Z","level":"error","event":"25/05/12 02:59:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:25.153045Z","level":"error","event":"25/05/12 02:59:25 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:28.761222","level":"error","event":"Failed to import: /opt/airflow/dags/jobs/YFinance/yearly.py","logger":"airflow.models.dagbag.DagBag","error_detail":[{"exc_type":"Py4JJavaError","exc_value":"An error occurred while calling o34.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongo. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: mongo.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py","lineno":384,"name":"parse"},{"filename":"<frozen importlib._bootstrap_external>","lineno":999,"name":"exec_module"},{"filename":"<frozen importlib._bootstrap>","lineno":488,"name":"_call_with_frames_removed"},{"filename":"/opt/airflow/dags/jobs/YFinance/yearly.py","lineno":27,"name":"<module>"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/sql/readwriter.py","lineno":314,"name":"load"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/java_gateway.py","lineno":1322,"name":"__call__"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py","lineno":179,"name":"deco"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/py4j/protocol.py","lineno":326,"name":"get_return_value"}]}]}
{"timestamp":"2025-05-12T02:59:28.813955Z","level":"error","event":"25/05/12 02:59:28 INFO SparkContext: Invoking stop() from shutdown hook","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:28.815253Z","level":"error","event":"25/05/12 02:59:28 INFO SparkContext: SparkContext is stopping with exitCode 0.","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:28.848960Z","level":"error","event":"25/05/12 02:59:28 INFO SparkUI: Stopped Spark web UI at http://4ad261c1f1d7:4040","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:28.885772Z","level":"error","event":"25/05/12 02:59:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:28.929331Z","level":"error","event":"25/05/12 02:59:28 INFO MemoryStore: MemoryStore cleared","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:28.944491Z","level":"error","event":"25/05/12 02:59:28 INFO BlockManager: BlockManager stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:28.959098Z","level":"error","event":"25/05/12 02:59:28 INFO BlockManagerMaster: BlockManagerMaster stopped","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:28.976691Z","level":"error","event":"25/05/12 02:59:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:29.044440Z","level":"error","event":"25/05/12 02:59:29 INFO SparkContext: Successfully stopped SparkContext","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:29.045673Z","level":"error","event":"25/05/12 02:59:29 INFO ShutdownHookManager: Shutdown hook called","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:29.046567Z","level":"error","event":"25/05/12 02:59:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-622054da-708e-4bf5-b30b-804deb3cc9ce","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:29.074852Z","level":"error","event":"25/05/12 02:59:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-9e853ab0-3eb6-4871-a87c-980654d9c2ba","chan":"stderr","logger":"processor"}
{"timestamp":"2025-05-12T02:59:29.108761Z","level":"error","event":"25/05/12 02:59:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-622054da-708e-4bf5-b30b-804deb3cc9ce/pyspark-7a32ff31-629f-424d-a823-260732204eec","chan":"stderr","logger":"processor"}
